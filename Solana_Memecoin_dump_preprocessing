{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb150f3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T11:59:14.626622Z",
     "iopub.status.busy": "2025-07-23T11:59:14.626266Z",
     "iopub.status.idle": "2025-07-23T11:59:15.724497Z",
     "shell.execute_reply": "2025-07-23T11:59:15.723446Z"
    },
    "papermill": {
     "duration": 1.11877,
     "end_time": "2025-07-23T11:59:15.726498",
     "exception": false,
     "start_time": "2025-07-23T11:59:14.607728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "# import cudf as cd\n",
    "# import dask_cudf as pd\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1cc0c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T11:59:15.759336Z",
     "iopub.status.busy": "2025-07-23T11:59:15.758722Z",
     "iopub.status.idle": "2025-07-23T11:59:25.767264Z",
     "shell.execute_reply": "2025-07-23T11:59:25.766021Z"
    },
    "papermill": {
     "duration": 10.026952,
     "end_time": "2025-07-23T11:59:25.769390",
     "exception": false,
     "start_time": "2025-07-23T11:59:15.742438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (1.9.0)\r\n",
      "Collecting polars\r\n",
      "  Downloading polars-1.31.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\r\n",
      "Downloading polars-1.31.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: polars\r\n",
      "  Attempting uninstall: polars\r\n",
      "    Found existing installation: polars 1.9.0\r\n",
      "    Uninstalling polars-1.9.0:\r\n",
      "      Successfully uninstalled polars-1.9.0\r\n",
      "Successfully installed polars-1.31.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadc8399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T11:59:25.803506Z",
     "iopub.status.busy": "2025-07-23T11:59:25.803155Z",
     "iopub.status.idle": "2025-07-23T11:59:25.925618Z",
     "shell.execute_reply": "2025-07-23T11:59:25.924463Z"
    },
    "papermill": {
     "duration": 0.142388,
     "end_time": "2025-07-23T11:59:25.927531",
     "exception": false,
     "start_time": "2025-07-23T11:59:25.785143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.31.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars\n",
    "polars.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c04c74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T11:59:25.961170Z",
     "iopub.status.busy": "2025-07-23T11:59:25.960438Z",
     "iopub.status.idle": "2025-07-23T11:59:25.965270Z",
     "shell.execute_reply": "2025-07-23T11:59:25.964180Z"
    },
    "papermill": {
     "duration": 0.023382,
     "end_time": "2025-07-23T11:59:25.967068",
     "exception": false,
     "start_time": "2025-07-23T11:59:25.943686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a177e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T11:59:26.000081Z",
     "iopub.status.busy": "2025-07-23T11:59:25.999682Z",
     "iopub.status.idle": "2025-07-23T12:03:01.193180Z",
     "shell.execute_reply": "2025-07-23T12:03:01.191310Z"
    },
    "papermill": {
     "duration": 215.213071,
     "end_time": "2025-07-23T12:03:01.195873",
     "exception": false,
     "start_time": "2025-07-23T11:59:25.982802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-9b4bcdabe7bf>:5: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  onchain_df_1 = pd.read_csv('/kaggle/input/pump-fun-graduation-february-2025/token_info_onchain_divers.csv')\n",
      "Loading Chunks: 100%|██████████| 41/41 [02:28<00:00,  3.62s/it]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/solana-skill-sprint-memcoin-graduation/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/solana-skill-sprint-memcoin-graduation/test_unlabeled.csv')\n",
    "dune_token_df_1 = pd.read_csv('/kaggle/input/pump-fun-graduation-february-2025/dune_token_info.csv')\n",
    "dune_token_df_2 = pd.read_csv('/kaggle/input/pump-fun-graduation-february-2025/dune_token_info_v2.csv')\n",
    "onchain_df_1 = pd.read_csv('/kaggle/input/pump-fun-graduation-february-2025/token_info_onchain_divers.csv')\n",
    "onchain_df_2 = pd.read_csv('/kaggle/input/pump-fun-graduation-february-2025/token_info_onchain_divers_v2.csv')\n",
    "samp_sub = pd.read_csv('/kaggle/input/solana-skill-sprint-memcoin-graduation/sample_submission.csv')\n",
    "\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "combined_df = pd.concat([train, test], axis=0)\n",
    "\n",
    "dune_token_df_combined = pd.concat([dune_token_df_1, dune_token_df_2], axis=0)\n",
    "\n",
    "data_path = '/kaggle/input/pump-fun-graduation-february-2025'\n",
    "chunk_pattern = os.path.join(data_path, 'chunk_*.csv')\n",
    "\n",
    "chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "\n",
    "chunks = []\n",
    "for f in tqdm(chunk_files, desc='Loading Chunks'):\n",
    "    try:\n",
    "        chunks.append(pd.read_csv(f, low_memory=False))\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {os.path.basename(f)}: {str(e)}')\n",
    "\n",
    "transactions = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83530121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:01.246359Z",
     "iopub.status.busy": "2025-07-23T12:03:01.245845Z",
     "iopub.status.idle": "2025-07-23T12:03:01.252370Z",
     "shell.execute_reply": "2025-07-23T12:03:01.250261Z"
    },
    "papermill": {
     "duration": 0.035848,
     "end_time": "2025-07-23T12:03:01.254700",
     "exception": false,
     "start_time": "2025-07-23T12:03:01.218852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'\\nmissing values :')\n",
    "# print(f'Train set : \\n{train.isnull().sum()}')\n",
    "# print('--------------')\n",
    "# print(f'Test set : \\n{test.isnull().sum()}')\n",
    "# print('--------------')\n",
    "# print(f'Dune set : \\n{dune_token_df_combined.isnull().sum()}')\n",
    "# print('--------------')\n",
    "# print(f'Onchain_df_2 set : \\n{onchain_df_2.isnull().sum()}')\n",
    "# print('--------------')\n",
    "# print(f'Transactions set : \\n{transactions.isnull().sum()}')\n",
    "# print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7adc2915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:01.300090Z",
     "iopub.status.busy": "2025-07-23T12:03:01.299633Z",
     "iopub.status.idle": "2025-07-23T12:03:01.305548Z",
     "shell.execute_reply": "2025-07-23T12:03:01.304007Z"
    },
    "papermill": {
     "duration": 0.032611,
     "end_time": "2025-07-23T12:03:01.308032",
     "exception": false,
     "start_time": "2025-07-23T12:03:01.275421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'\\nmissing values :')\n",
    "# print(f'Train set : \\n{train.describe()}')\n",
    "# print('--------------')\n",
    "# print(f'Test set : \\n{test.describe()}')\n",
    "# print('--------------')\n",
    "# print(f'Dune set : \\n{dune_token_df_combined.describe()}')\n",
    "# print('--------------')\n",
    "# print(f'Onchain_df_2 set : \\n{onchain_df_2.describe()}')\n",
    "# print('--------------')\n",
    "# print(f'Transactions set : \\n{transactions.describe()}')\n",
    "# print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18221339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:01.349770Z",
     "iopub.status.busy": "2025-07-23T12:03:01.349439Z",
     "iopub.status.idle": "2025-07-23T12:03:01.353961Z",
     "shell.execute_reply": "2025-07-23T12:03:01.352709Z"
    },
    "papermill": {
     "duration": 0.02828,
     "end_time": "2025-07-23T12:03:01.356057",
     "exception": false,
     "start_time": "2025-07-23T12:03:01.327777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns \n",
    "\n",
    "# sns.countplot(x='has_graduated', data=train)\n",
    "# plt.title('Distribution of target label(train)')\n",
    "# plt.xlabel('has_graduated')\n",
    "# plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abfdb30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:01.399701Z",
     "iopub.status.busy": "2025-07-23T12:03:01.399344Z",
     "iopub.status.idle": "2025-07-23T12:03:01.407376Z",
     "shell.execute_reply": "2025-07-23T12:03:01.405733Z"
    },
    "papermill": {
     "duration": 0.034605,
     "end_time": "2025-07-23T12:03:01.409795",
     "exception": false,
     "start_time": "2025-07-23T12:03:01.375190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(data=combined_df[combined_df['is_train']==1], x='slot_min', label='Train', alpha=0.5, kde=True)\n",
    "# sns.histplot(data=combined_df[combined_df['is_train']==0], x='slot_min', label='Test', alpha=0.5, kde=True)\n",
    "# plt.title('Distribution of slot min in train vs test')\n",
    "# plt.xlabel('slot_min')\n",
    "# plt.ylabel('count')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0740e492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:01.452938Z",
     "iopub.status.busy": "2025-07-23T12:03:01.452572Z",
     "iopub.status.idle": "2025-07-23T12:03:01.457786Z",
     "shell.execute_reply": "2025-07-23T12:03:01.456272Z"
    },
    "papermill": {
     "duration": 0.029321,
     "end_time": "2025-07-23T12:03:01.460233",
     "exception": false,
     "start_time": "2025-07-23T12:03:01.430912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# pd.to_datetime(transactions['block_time']).dt.hour.hist(bins=24)\n",
    "# plt.title('Transaction Activity by hour of day')\n",
    "# plt.xlabel('Hour of day')\n",
    "# plt.ylabel('Number of transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b509d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:01.508816Z",
     "iopub.status.busy": "2025-07-23T12:03:01.508249Z",
     "iopub.status.idle": "2025-07-23T12:03:19.280762Z",
     "shell.execute_reply": "2025-07-23T12:03:19.279747Z"
    },
    "papermill": {
     "duration": 17.800839,
     "end_time": "2025-07-23T12:03:19.282682",
     "exception": false,
     "start_time": "2025-07-23T12:03:01.481843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df = pd.merge(\n",
    "    transactions,\n",
    "    combined_df[['mint', 'slot_min']],\n",
    "    left_on='base_coin',\n",
    "    right_on='mint',\n",
    "    how='left'\n",
    ")\n",
    "transactions_df = transactions_df[transactions_df['slot'] <= transactions_df['slot_min'] + 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56421596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:19.322113Z",
     "iopub.status.busy": "2025-07-23T12:03:19.321687Z",
     "iopub.status.idle": "2025-07-23T12:03:19.769635Z",
     "shell.execute_reply": "2025-07-23T12:03:19.768409Z"
    },
    "papermill": {
     "duration": 0.470343,
     "end_time": "2025-07-23T12:03:19.771689",
     "exception": false,
     "start_time": "2025-07-23T12:03:19.301346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e8e786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:19.810779Z",
     "iopub.status.busy": "2025-07-23T12:03:19.810429Z",
     "iopub.status.idle": "2025-07-23T12:03:19.820676Z",
     "shell.execute_reply": "2025-07-23T12:03:19.819677Z"
    },
    "papermill": {
     "duration": 0.031949,
     "end_time": "2025-07-23T12:03:19.822498",
     "exception": false,
     "start_time": "2025-07-23T12:03:19.790549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import concurrent.futures\n",
    "\n",
    "# def fast_polars_process(transactions_df, n_jobs=4):\n",
    "#     \"\"\"Optimized version of mint processing using Polars operations\"\"\"\n",
    "#     print(f\"Starting Polars transaction processing at {datetime.now()}\")\n",
    "    \n",
    "#     # Convert pandas dataframe to polars if needed\n",
    "#     if not isinstance(transactions_df, pl.DataFrame):\n",
    "#         transactions_df = pl.from_pandas(transactions_df)\n",
    "    \n",
    "#     # Ensure datetime conversion\n",
    "#     if transactions_df[\"block_time\"].dtype != pl.Datetime:\n",
    "#         transactions_df = transactions_df.with_columns(\n",
    "#             pl.col(\"block_time\").str.to_datetime()\n",
    "#         )\n",
    "    \n",
    "#     print(\"Calculating aggregated features...\")\n",
    "    \n",
    "#     # Basic counts (vectorized)\n",
    "#     counts = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.count().alias(\"total_txs\"),\n",
    "#             pl.n_unique(\"tx_idx\").alias(\"unique_tx_indices\"),\n",
    "#             pl.n_unique(\"signing_wallet\").alias(\"unique_wallets\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Direction-based metrics\n",
    "#     direction_counts = (\n",
    "#         transactions_df\n",
    "#         .group_by([\"base_coin\", \"direction\"])\n",
    "#         .agg(pl.count().alias(\"count\"))\n",
    "#         .pivot(index=\"base_coin\", columns=\"direction\", values=\"count\")\n",
    "#         .fill_null(0)\n",
    "#     )\n",
    "    \n",
    "#     # Ensure buy and sell columns exist\n",
    "#     if \"buy\" not in direction_counts.columns:\n",
    "#         direction_counts = direction_counts.with_columns(pl.lit(0).alias(\"buy\"))\n",
    "#     if \"sell\" not in direction_counts.columns:\n",
    "#         direction_counts = direction_counts.with_columns(pl.lit(0).alias(\"sell\"))\n",
    "    \n",
    "#     # Rename columns\n",
    "#     direction_counts = direction_counts.rename({\"buy\": \"buy_count\", \"sell\": \"sell_count\"})\n",
    "    \n",
    "#     # Wallet counts by direction\n",
    "#     buy_wallets = (\n",
    "#         transactions_df\n",
    "#         .filter(pl.col(\"direction\") == \"buy\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_buy_wallets\"))\n",
    "#     )\n",
    "    \n",
    "#     sell_wallets = (\n",
    "#         transactions_df\n",
    "#         .filter(pl.col(\"direction\") == \"sell\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_sell_wallets\"))\n",
    "#     )\n",
    "    \n",
    "#     # Calculate base and quote statistics\n",
    "#     base_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"base_coin_amount\").sum().alias(\"base_sum\"),\n",
    "#             pl.col(\"base_coin_amount\").mean().alias(\"base_mean\"),\n",
    "#             pl.col(\"base_coin_amount\").max().alias(\"base_max\"),\n",
    "#             pl.col(\"base_coin_amount\").min().alias(\"base_min\"),\n",
    "#             pl.col(\"base_coin_amount\").std().alias(\"base_std\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     quote_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"quote_coin_amount\").sum().alias(\"quote_sum\"),\n",
    "#             pl.col(\"quote_coin_amount\").mean().alias(\"quote_mean\"),\n",
    "#             pl.col(\"quote_coin_amount\").max().alias(\"quote_max\"),\n",
    "#             pl.col(\"quote_coin_amount\").min().alias(\"quote_min\"),\n",
    "#             pl.col(\"quote_coin_amount\").std().alias(\"quote_std\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Time-based features\n",
    "#     time_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"block_time\").min().alias(\"first_time\"),\n",
    "#             pl.col(\"block_time\").max().alias(\"last_time\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_time\") - pl.col(\"first_time\")).dt.total_seconds().alias(\"activity_duration_sec\")\n",
    "#         )\n",
    "#     )\n",
    "#     # Transaction velocity calculation (NEW FEATURE)\n",
    "#     time_stats = time_stats.join(counts.select(\"base_coin\", \"total_txs\"), on=\"base_coin\").with_columns(\n",
    "#         (pl.col(\"total_txs\") / (pl.col(\"activity_duration_sec\") + 1e-6)).alias(\"tx_per_sec\")\n",
    "#     )\n",
    "    \n",
    "#     # Slot features\n",
    "#     slot_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"slot\").min().alias(\"first_slot\"),\n",
    "#             pl.col(\"slot\").max().alias(\"last_slot\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_slot\") - pl.col(\"first_slot\")).alias(\"slot_span\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Early transactions calculation (within first 100 slots)\n",
    "#     early_txs = transactions_df.group_by(\"base_coin\").agg(\n",
    "#         pl.col(\"tx_idx\")\n",
    "#         .filter(pl.col(\"slot\") <= pl.col(\"slot\").min() + 100)\n",
    "#         .count()\n",
    "#         .alias(\"early_txs\")\n",
    "#     )\n",
    "    \n",
    "#     # Balance features - get first and last records efficiently\n",
    "#     sorted_df = transactions_df.sort([\"base_coin\", \"slot\"])\n",
    "    \n",
    "#     # First values\n",
    "#     first_values = (\n",
    "#         sorted_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").first().alias(\"first_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").first().alias(\"first_sol\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Last values\n",
    "#     last_values = (\n",
    "#         sorted_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").last().alias(\"last_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").last().alias(\"last_sol\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Combine balance features\n",
    "#     balance_features = first_values.join(last_values, on=\"base_coin\").with_columns(\n",
    "#         (pl.col(\"last_sol\") - pl.col(\"first_sol\")).alias(\"sol_change\")\n",
    "#     )\n",
    "    \n",
    "#     # Balance volatility\n",
    "#     balance_vol = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").std().alias(\"balance_volatility\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").max().alias(\"max_balance\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").min().alias(\"min_balance\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Gas features\n",
    "#     gas_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"provided_gas_fee\").sum().alias(\"total_gas_fees\"),\n",
    "#             pl.col(\"provided_gas_fee\").mean().alias(\"avg_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").std().alias(\"std_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").max().alias(\"max_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").min().alias(\"min_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_limit\").mean().alias(\"avg_gas_limit\"),\n",
    "#             pl.col(\"consumed_gas\").mean().alias(\"avg_consumed_gas\"),\n",
    "#             pl.col(\"consumed_gas\").sum().alias(\"total_consumed_gas\"),\n",
    "#             pl.col(\"fee\").sum().alias(\"total_fee\"),\n",
    "#             (pl.col(\"provided_gas_limit\") - pl.col(\"consumed_gas\")).mean().alias(\"limit_utilization\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     liq_stats = (\n",
    "#     transactions_df\n",
    "#       .group_by(\"base_coin\")\n",
    "#       .agg([\n",
    "#         # average SOL‐balance after each transaction (pool depth proxy)\n",
    "#         pl.col(\"virtual_sol_balance_after\")\n",
    "#           .mean()\n",
    "#           .alias(\"liq_virtual_sol_balance_after_mean\"),\n",
    "#         # total quote‐volume (another liquidity proxy)\n",
    "#         pl.col(\"quote_coin_amount\")\n",
    "#           .sum()\n",
    "#           .alias(\"liq_quote_coin_amount\"),\n",
    "#       ])\n",
    "#     )\n",
    "#     # Gas metrics with quote\n",
    "#     gas_metrics = gas_stats.join(quote_stats.select([\"base_coin\", \"quote_sum\"]), on=\"base_coin\").with_columns([\n",
    "#         (pl.col(\"total_fee\") / (pl.col(\"quote_sum\") + 1e-6)).alias(\"fee_per_sol\"),\n",
    "#         pl.col(\"avg_consumed_gas\").alias(\"gas_per_tx\"),\n",
    "#         (pl.col(\"quote_sum\") / (pl.col(\"total_consumed_gas\") + 1e-6)).alias(\"gas_efficiency\")\n",
    "#     ])\n",
    "    \n",
    "#     # Wallet concentration (Gini and top wallet share) - using chunks to avoid memory issues\n",
    "#     def calculate_concentration_metrics(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "#         wallet_volumes = mint_df.group_by(\"signing_wallet\").agg(pl.col(\"base_coin_amount\").sum())\n",
    "        \n",
    "#         if wallet_volumes.height <= 1:\n",
    "#             return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "        \n",
    "#         sorted_values = wallet_volumes.sort(\"base_coin_amount\").to_pandas()[\"base_coin_amount\"].values\n",
    "#         n = len(sorted_values)\n",
    "        \n",
    "#         if n > 1:\n",
    "#             gini_coeff = np.sum((2 * np.arange(1, n+1) - n - 1) * sorted_values) / (n * np.sum(sorted_values) + 1e-6)\n",
    "#         else:\n",
    "#             gini_coeff = 0.0\n",
    "        \n",
    "#         total_volume = sorted_values.sum()\n",
    "#         if total_volume > 0:\n",
    "#             top_k = min(5, n)\n",
    "#             top_k_volume = np.sum(sorted_values[-top_k:])\n",
    "#             top5_wallet_share = top_k_volume / total_volume\n",
    "#         else:\n",
    "#             top5_wallet_share = 0.0\n",
    "            \n",
    "#         return {\"base_coin\": mint, \"gini_coeff\": gini_coeff, \"top5_wallet_share\": top5_wallet_share}\n",
    "    \n",
    "#     print(\"Calculating wallet concentration metrics...\")\n",
    "#     unique_mints = counts[\"base_coin\"].to_list()\n",
    "    \n",
    "#     # Use ThreadPoolExecutor for parallel processing\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         concentration_results = list(executor.map(calculate_concentration_metrics, unique_mints))\n",
    "    \n",
    "#     concentration_df = pl.DataFrame(concentration_results)\n",
    "    \n",
    "#     # Hour distribution entropy\n",
    "#     def calculate_hour_entropy(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "#         hours = mint_df[\"block_time\"].dt.hour()\n",
    "#         hour_counts = hours.value_counts().sort(\"count\", descending=True)\n",
    "        \n",
    "#         if hour_counts.height > 0:\n",
    "#             total = hour_counts[\"count\"].sum()\n",
    "#             probs = hour_counts[\"count\"] / total\n",
    "#             non_zero_probs = probs.filter(probs > 0)\n",
    "            \n",
    "#             if non_zero_probs.len() > 0:\n",
    "#                 entropy = -np.sum(non_zero_probs.to_numpy() * np.log2(non_zero_probs.to_numpy()))\n",
    "#                 return {\"base_coin\": mint, \"hour_entropy\": entropy}\n",
    "        \n",
    "#         return {\"base_coin\": mint, \"hour_entropy\": 0.0}\n",
    "    \n",
    "#     print(\"Calculating hour entropy...\")\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         hour_entropy_results = list(executor.map(calculate_hour_entropy, unique_mints))\n",
    "    \n",
    "#     hour_entropy_df = pl.DataFrame(hour_entropy_results)\n",
    "    \n",
    "#     # Flow imbalance calculation\n",
    "#     flow_imbalance = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg([\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"buy\").sum().alias(\"buy_quote_amount\"),\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"sell\").sum().alias(\"sell_quote_amount\")\n",
    "#         ])\n",
    "#         .with_columns([\n",
    "#             (pl.col(\"buy_quote_amount\") + pl.col(\"sell_quote_amount\")).alias(\"total_quote_amount\"),\n",
    "#             ((pl.col(\"buy_quote_amount\") - pl.col(\"sell_quote_amount\")) / \n",
    "#              (pl.col(\"buy_quote_amount\") + pl.col(\"sell_quote_amount\") + 1e-6)).alias(\"flow_imbalance\")\n",
    "#         ])\n",
    "#         .select([\"base_coin\", \"flow_imbalance\"])\n",
    "#     )\n",
    "    \n",
    "#     # Price and volatility calculations\n",
    "#     def calculate_price_metrics(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "        \n",
    "#         # Calculate price\n",
    "#         mint_df = mint_df.with_columns(\n",
    "#             (pl.col(\"quote_coin_amount\") / pl.col(\"base_coin_amount\")\n",
    "#              .replace(0, None))\n",
    "#             .alias(\"price\")\n",
    "#         )\n",
    "        \n",
    "#         # Replace infinite values with nulls\n",
    "#         mint_df = mint_df.with_columns(\n",
    "#             pl.when(pl.col(\"price\").is_infinite())\n",
    "#             .then(None)\n",
    "#             .otherwise(pl.col(\"price\"))\n",
    "#             .alias(\"price\")\n",
    "#         )\n",
    "        \n",
    "#         avg_price = mint_df[\"price\"].mean()\n",
    "        \n",
    "#         if mint_df.height >= 5:\n",
    "#             # For rolling window calculations, we need to sort and convert to pandas temporarily\n",
    "#             # This is a performance bottleneck, but necessary for the rolling calculation\n",
    "#             sorted_df = mint_df.sort(\"slot\").to_pandas()\n",
    "#             sorted_df['rolling_price_std'] = sorted_df['price'].rolling(window=5, min_periods=1).std().fillna(0)\n",
    "#             price_vol_mean = sorted_df['rolling_price_std'].mean()\n",
    "#             price_vol_max = sorted_df['rolling_price_std'].max()\n",
    "#         else:\n",
    "#             price_vol_mean = mint_df[\"price\"].std() if mint_df.height > 1 else 0\n",
    "#             price_vol_max = price_vol_mean\n",
    "        \n",
    "#         price_min = mint_df[\"price\"].min()\n",
    "#         price_max = mint_df[\"price\"].max()\n",
    "#         price_mean = mint_df[\"price\"].mean()\n",
    "        \n",
    "#         price_slippage = (price_max - price_min) / (price_mean + 1e-6) if price_mean is not None else 0\n",
    "        \n",
    "#         return {\n",
    "#             \"base_coin\": mint,\n",
    "#             \"avg_price\": float(avg_price) if avg_price is not None else 0.0,\n",
    "#             \"price_vol_mean\": float(price_vol_mean) if price_vol_mean is not None else 0.0,\n",
    "#             \"price_vol_max\": float(price_vol_max) if price_vol_max is not None else 0.0,\n",
    "#             \"price_slippage\": float(price_slippage)\n",
    "#         }\n",
    "    \n",
    "#     print(\"Calculating price and volatility metrics...\")\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         price_results = list(executor.map(calculate_price_metrics, unique_mints))\n",
    "    \n",
    "#     price_df = pl.DataFrame(price_results)\n",
    "    \n",
    "#     # Time-weighted features\n",
    "#     def calculate_tw_features(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "#         max_slot = mint_df[\"slot\"].max()\n",
    "        \n",
    "#         # Calculate time weights using numpy instead of pl.exp\n",
    "#         mint_df = mint_df.with_columns(\n",
    "#             pl.col(\"slot\").map_elements(lambda s: np.exp(-0.1 * (max_slot - s))).alias(\"time_weight\")\n",
    "#         )\n",
    "        \n",
    "#         tw_base = (mint_df[\"base_coin_amount\"] * mint_df[\"time_weight\"]).sum()\n",
    "#         tw_quote = (mint_df[\"quote_coin_amount\"] * mint_df[\"time_weight\"]).sum()\n",
    "        \n",
    "#         return {\n",
    "#             \"base_coin\": mint,\n",
    "#             \"tw_base_coin_amount\": float(tw_base),\n",
    "#             \"tw_quote_coin_amount\": float(tw_quote)\n",
    "#         }\n",
    "    \n",
    "#     print(\"Calculating time-weighted features...\")\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         tw_results = list(executor.map(calculate_tw_features, unique_mints))\n",
    "    \n",
    "#     tw_df = pl.DataFrame(tw_results)\n",
    "    \n",
    "#     # Now merge all feature DataFrames\n",
    "#     print(\"Merging all features...\")\n",
    "    \n",
    "#     # Start with counts as the base\n",
    "#     result = counts\n",
    "    \n",
    "#     # Add buy ratio\n",
    "#     result = result.join(direction_counts, on=\"base_coin\", how=\"left\")\n",
    "#     result = result.with_columns(\n",
    "#         (pl.col(\"buy_count\") / pl.col(\"total_txs\")).alias(\"buy_ratio\")\n",
    "#     )\n",
    "    \n",
    "#     # Join remaining dataframes\n",
    "#     dfs_to_join = [\n",
    "#         buy_wallets, sell_wallets, base_stats, quote_stats, \n",
    "#         time_stats.select([\"base_coin\", \"activity_duration_sec\", \"first_time\", \"last_time\", \"tx_per_sec\"]), \n",
    "#         slot_stats, early_txs, balance_features, balance_vol, \n",
    "#         gas_metrics, concentration_df, hour_entropy_df, flow_imbalance, price_df, tw_df, liq_stats\n",
    "#     ]\n",
    "    \n",
    "#     for df in dfs_to_join:\n",
    "#         result = result.join(df, on=\"base_coin\", how=\"left\")\n",
    "    \n",
    "#     # Rename base_coin to mint for consistency with original\n",
    "#     result = result.rename({\"base_coin\": \"mint\"})\n",
    "    \n",
    "#     # Fill null values\n",
    "#     result = result.fill_null(0)\n",
    "    \n",
    "#     print(f\"Polars processing complete with {result.height} rows and {len(result.columns)} columns\")\n",
    "    \n",
    "#     # Convert back to pandas if needed\n",
    "#     return result.to_pandas()\n",
    "\n",
    "# start_time = datetime.now()\n",
    "# mint_transactions = fast_polars_process(transactions_df.iloc[:100000], n_jobs=4)\n",
    "# end_time = datetime.now()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f'Time it took to process: {elapsed_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8603bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:19.862058Z",
     "iopub.status.busy": "2025-07-23T12:03:19.861587Z",
     "iopub.status.idle": "2025-07-23T12:03:19.877796Z",
     "shell.execute_reply": "2025-07-23T12:03:19.876313Z"
    },
    "papermill": {
     "duration": 0.039043,
     "end_time": "2025-07-23T12:03:19.880139",
     "exception": false,
     "start_time": "2025-07-23T12:03:19.841096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import concurrent.futures\n",
    "# import time\n",
    "# import cudf\n",
    "# import cupy as cp\n",
    "\n",
    "# def fast_polars_process(transactions_df, n_jobs=4):\n",
    "#     \"\"\"Fully optimized version using Polars LazyFrames and GPU acceleration\"\"\"\n",
    "#     total_start_time = time.time()\n",
    "#     print(f\"Starting GPU-accelerated Polars transaction processing at {datetime.now()}\")\n",
    "    \n",
    "#     # Convert to LazyFrame\n",
    "#     convert_start = time.time()\n",
    "#     if not isinstance(transactions_df, pl.LazyFrame):\n",
    "#         if not isinstance(transactions_df, pl.DataFrame):\n",
    "#             transactions_df = pl.from_pandas(transactions_df)\n",
    "#         lf = transactions_df.lazy()\n",
    "#     else:\n",
    "#         lf = transactions_df\n",
    "#     convert_end = time.time()\n",
    "#     print(f\"Time to convert to LazyFrame: {convert_end - convert_start:.4f} seconds\")\n",
    "    \n",
    "#     # Ensure datetime conversion\n",
    "#     datetime_start = time.time()\n",
    "#     lf = lf.with_columns(\n",
    "#         pl.col(\"block_time\").str.to_datetime(strict=False)\n",
    "#     )\n",
    "#     datetime_end = time.time()\n",
    "#     print(f\"Time to convert block_time to datetime: {datetime_end - datetime_start:.4f} seconds\")\n",
    "    \n",
    "#     print(\"Calculating aggregated features...\")\n",
    "    \n",
    "#     # Basic counts\n",
    "#     counts_start = time.time()\n",
    "#     counts = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.len().alias(\"total_txs\"),\n",
    "#             pl.n_unique(\"tx_idx\").alias(\"unique_tx_indices\"),\n",
    "#             pl.n_unique(\"signing_wallet\").alias(\"unique_wallets\")\n",
    "#         )\n",
    "#     )\n",
    "#     counts_end = time.time()\n",
    "#     print(f\"Time to compute basic counts: {counts_end - counts_start:.4f} seconds\")\n",
    "    \n",
    "#     # Direction-based metrics\n",
    "#     dir_start = time.time()\n",
    "#     direction_counts = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.when(pl.col(\"direction\") == \"buy\")\n",
    "#               .then(1)\n",
    "#               .otherwise(0)\n",
    "#               .sum()\n",
    "#               .alias(\"buy_count\"),\n",
    "#             pl.when(pl.col(\"direction\") == \"sell\")\n",
    "#               .then(1)\n",
    "#               .otherwise(0)\n",
    "#               .sum()\n",
    "#               .alias(\"sell_count\")\n",
    "#         )\n",
    "#     )\n",
    "#     dir_end = time.time()\n",
    "#     print(f\"Time to compute direction-based metrics: {dir_end - dir_start:.4f} seconds\")\n",
    "    \n",
    "#     # Wallet counts - collect with GPU\n",
    "#     wallet_start = time.time()\n",
    "#     buy_wallets = (\n",
    "#         lf.filter(pl.col(\"direction\") == \"buy\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_buy_wallets\"))\n",
    "#     )\n",
    "    \n",
    "#     sell_wallets = (\n",
    "#         lf.filter(pl.col(\"direction\") == \"sell\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_sell_wallets\"))\n",
    "#     )\n",
    "#     wallet_end = time.time()\n",
    "#     print(f\"Time to compute wallet counts: {wallet_end - wallet_start:.4f} seconds\")\n",
    "    \n",
    "#     # Base/quote statistics - collect with GPU\n",
    "#     stats_start = time.time()\n",
    "#     base_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"base_coin_amount\").sum().alias(\"base_sum\"),\n",
    "#             pl.col(\"base_coin_amount\").mean().alias(\"base_mean\"),\n",
    "#             pl.col(\"base_coin_amount\").max().alias(\"base_max\"),\n",
    "#             pl.col(\"base_coin_amount\").min().alias(\"base_min\"),\n",
    "#             pl.col(\"base_coin_amount\").std().alias(\"base_std\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     quote_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"quote_coin_amount\").sum().alias(\"quote_sum\"),\n",
    "#             pl.col(\"quote_coin_amount\").mean().alias(\"quote_mean\"),\n",
    "#             pl.col(\"quote_coin_amount\").max().alias(\"quote_max\"),\n",
    "#             pl.col(\"quote_coin_amount\").min().alias(\"quote_min\"),\n",
    "#             pl.col(\"quote_coin_amount\").std().alias(\"quote_std\")\n",
    "#         )\n",
    "#     )\n",
    "#     stats_end = time.time()\n",
    "#     print(f\"Time to compute base/quote statistics: {stats_end - stats_start:.4f} seconds\")\n",
    "    \n",
    "#     # Time-based features - collect with GPU\n",
    "#     time_feat_start = time.time()\n",
    "#     time_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"block_time\").min().alias(\"first_time\"),\n",
    "#             pl.col(\"block_time\").max().alias(\"last_time\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_time\") - pl.col(\"first_time\"))\n",
    "#             .dt.total_seconds()\n",
    "#             .alias(\"activity_duration_sec\")\n",
    "#         )\n",
    "#         .join(counts.select(\"base_coin\", \"total_txs\"), on=\"base_coin\")\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"total_txs\") / (pl.col(\"activity_duration_sec\") + 1e-6))\n",
    "#             .alias(\"tx_per_sec\")\n",
    "#         )\n",
    "#     )\n",
    "#     time_feat_end = time.time()\n",
    "#     print(f\"Time to compute time-based features: {time_feat_end - time_feat_start:.4f} seconds\")\n",
    "    \n",
    "#     # Slot features - collect with GPU\n",
    "#     slot_start = time.time()\n",
    "#     slot_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"slot\").min().alias(\"first_slot\"),\n",
    "#             pl.col(\"slot\").max().alias(\"last_slot\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_slot\") - pl.col(\"first_slot\")).alias(\"slot_span\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Early transactions - collect with GPU\n",
    "#     early_txs = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"tx_idx\")\n",
    "#             .filter(pl.col(\"slot\") <= pl.col(\"slot\").min() + 100)\n",
    "#             .count()\n",
    "#             .alias(\"early_txs\")\n",
    "#         )\n",
    "#     )\n",
    "#     slot_end = time.time()\n",
    "#     print(f\"Time to compute slot features: {slot_end - slot_start:.4f} seconds\")\n",
    "    \n",
    "#     # Balance features - collect with GPU\n",
    "#     balance_start = time.time()\n",
    "#     sorted_lf = lf.sort([\"base_coin\", \"slot\"])\n",
    "#     first_values = (\n",
    "#         sorted_lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").first().alias(\"first_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").first().alias(\"first_sol\")\n",
    "#         )\n",
    "#     )\n",
    "#     last_values = (\n",
    "#         sorted_lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").last().alias(\"last_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").last().alias(\"last_sol\")\n",
    "#         )\n",
    "#     )\n",
    "#     balance_features = (\n",
    "#         first_values.join(last_values, on=\"base_coin\")\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_sol\") - pl.col(\"first_sol\")).alias(\"sol_change\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Balance volatility - collect with GPU\n",
    "#     balance_vol = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").std().alias(\"balance_volatility\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").max().alias(\"max_balance\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").min().alias(\"min_balance\")\n",
    "#         )\n",
    "#     )\n",
    "#     balance_end = time.time()\n",
    "#     print(f\"Time to compute balance features: {balance_end - balance_start:.4f} seconds\")\n",
    "    \n",
    "#     # Gas features - collect with GPU\n",
    "#     gas_start = time.time()\n",
    "#     gas_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"provided_gas_fee\").sum().alias(\"total_gas_fees\"),\n",
    "#             pl.col(\"provided_gas_fee\").mean().alias(\"avg_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").std().alias(\"std_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").max().alias(\"max_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").min().alias(\"min_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_limit\").mean().alias(\"avg_gas_limit\"),\n",
    "#             pl.col(\"consumed_gas\").mean().alias(\"avg_consumed_gas\"),\n",
    "#             pl.col(\"consumed_gas\").sum().alias(\"total_consumed_gas\"),\n",
    "#             pl.col(\"fee\").sum().alias(\"total_fee\"),\n",
    "#             (pl.col(\"provided_gas_limit\") - pl.col(\"consumed_gas\")).mean().alias(\"limit_utilization\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Liquidity stats - collect with GPU\n",
    "#     liq_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_sol_balance_after\").mean().alias(\"liq_virtual_sol_balance_after_mean\"),\n",
    "#             pl.col(\"quote_coin_amount\").sum().alias(\"liq_quote_coin_amount\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Gas metrics - collect with GPU\n",
    "#     gas_metrics = (\n",
    "#         gas_stats.join(quote_stats.select([\"base_coin\", \"quote_sum\"]), on=\"base_coin\")\n",
    "#         .with_columns([\n",
    "#             (pl.col(\"total_fee\") / (pl.col(\"quote_sum\") + 1e-6)).alias(\"fee_per_sol\"),\n",
    "#             pl.col(\"avg_consumed_gas\").alias(\"gas_per_tx\"),\n",
    "#             (pl.col(\"quote_sum\") / (pl.col(\"total_consumed_gas\") + 1e-6)).alias(\"gas_efficiency\")\n",
    "#         ])\n",
    "#     )\n",
    "#     gas_end = time.time()\n",
    "#     print(f\"Time to compute gas and liquidity features: {gas_end - gas_start:.4f} seconds\")\n",
    "    \n",
    "#     # Flow imbalance - collect with GPU\n",
    "#     flow_start = time.time()\n",
    "#     flow_imbalance = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"buy\").sum().alias(\"buy_quote_amount\"),\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"sell\").sum().alias(\"sell_quote_amount\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             ((pl.col(\"buy_quote_amount\") - pl.col(\"sell_quote_amount\")) / \n",
    "#              (pl.col(\"buy_quote_amount\") + pl.col(\"sell_quote_amount\") + 1e-6)).alias(\"flow_imbalance\")\n",
    "#         )\n",
    "#         .select([\"base_coin\", \"flow_imbalance\"])\n",
    "#     )\n",
    "#     flow_end = time.time()\n",
    "#     print(f\"Time to compute flow imbalance features: {flow_end - flow_start:.4f} seconds\")\n",
    "    \n",
    "#     # Collect necessary data for custom functions\n",
    "#     print(\"Collecting data for custom functions...\")\n",
    "#     collect_start = time.time()\n",
    "#     unique_mints = counts.collect(engine='gpu')[\"base_coin\"].to_list()\n",
    "#     collect_end = time.time()\n",
    "#     print(f\"Time to collect unique mints: {collect_end - collect_start:.4f} seconds\")\n",
    "    \n",
    "#     # Wallet concentration metrics - GPU-optimized version\n",
    "#     wallet_conc_start = time.time()\n",
    "#     print(\"Calculating wallet concentration metrics using GPU...\")\n",
    "\n",
    "#     # Pre-calculate wallet volumes for all mints at once\n",
    "#     wallet_volumes = (\n",
    "#         lf.group_by([\"base_coin\", \"signing_wallet\"])\n",
    "#         .agg(pl.col(\"base_coin_amount\").sum().alias(\"volume\"))\n",
    "#         .collect(engine='gpu')\n",
    "#     )\n",
    "\n",
    "#     # Convert to CUDF for faster GPU-based calculations\n",
    "#     try:\n",
    "#         cudf_wallet_volumes = cudf.DataFrame.from_pandas(wallet_volumes.to_pandas())\n",
    "        \n",
    "#         # Function to calculate Gini and top5 share for a single mint using CUDA\n",
    "#         def calculate_concentration_metrics_gpu(mint):\n",
    "#             mint_volumes = cudf_wallet_volumes[cudf_wallet_volumes['base_coin'] == mint]\n",
    "            \n",
    "#             if len(mint_volumes) <= 1:\n",
    "#                 return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "            \n",
    "#             sorted_values = mint_volumes.sort_values('volume')['volume'].values\n",
    "#             sorted_values_cp = cp.array(sorted_values)\n",
    "#             n = len(sorted_values)\n",
    "            \n",
    "#             # Gini coefficient calculation using CuPy\n",
    "#             cumulative_values = cp.cumsum(sorted_values_cp)\n",
    "#             total = cumulative_values[-1]\n",
    "#             if total <= 0:\n",
    "#                 return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "            \n",
    "#             # Faster Gini calculation using cumulative sum\n",
    "#             gini_coeff = (n + 1 - 2 * cp.sum(cumulative_values) / total) / n\n",
    "            \n",
    "#             # Top wallet share\n",
    "#             top_k = min(5, n)\n",
    "#             top5_wallet_share = cp.sum(sorted_values_cp[-top_k:]) / total\n",
    "            \n",
    "#             return {\"base_coin\": mint, \"gini_coeff\": float(gini_coeff), \"top5_wallet_share\": float(top5_wallet_share)}\n",
    "        \n",
    "#         concentration_results = []\n",
    "#         for mint in unique_mints:\n",
    "#             concentration_results.append(calculate_concentration_metrics_gpu(mint))\n",
    "        \n",
    "#     except (ImportError, ModuleNotFoundError):\n",
    "#         print(\"CUDF not available, falling back to CPU calculation\")\n",
    "#         # Function to calculate Gini and top5 share for a single mint using pre-calculated data\n",
    "#         def calculate_concentration_metrics_optimized(mint):\n",
    "#             mint_volumes = wallet_volumes.filter(pl.col(\"base_coin\") == mint)\n",
    "            \n",
    "#             if mint_volumes.height <= 1:\n",
    "#                 return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "            \n",
    "#             sorted_values = mint_volumes.sort(\"volume\")[\"volume\"].to_numpy()\n",
    "#             n = len(sorted_values)\n",
    "            \n",
    "#             # Gini coefficient calculation\n",
    "#             cumulative_values = np.cumsum(sorted_values)\n",
    "#             total = cumulative_values[-1]\n",
    "#             if total <= 0:\n",
    "#                 return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "            \n",
    "#             # Faster Gini calculation using cumulative sum\n",
    "#             gini_coeff = (n + 1 - 2 * np.sum(cumulative_values) / total) / n\n",
    "            \n",
    "#             # Top wallet share\n",
    "#             top_k = min(5, n)\n",
    "#             top5_wallet_share = np.sum(sorted_values[-top_k:]) / total\n",
    "            \n",
    "#             return {\"base_coin\": mint, \"gini_coeff\": float(gini_coeff), \"top5_wallet_share\": float(top5_wallet_share)}\n",
    "\n",
    "#         # Process in parallel using the pre-calculated data\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#             concentration_results = list(executor.map(calculate_concentration_metrics_optimized, unique_mints))\n",
    "            \n",
    "#     concentration_df = pl.DataFrame(concentration_results)\n",
    "\n",
    "#     wallet_conc_end = time.time()\n",
    "#     print(f\"Time to compute wallet concentration metrics: {wallet_conc_end - wallet_conc_start:.4f} seconds\")\n",
    "\n",
    "#     # Hour entropy - GPU-optimized version\n",
    "#     hour_start = time.time()\n",
    "#     print(\"Calculating hour entropy using GPU...\")\n",
    "\n",
    "#     # Calculate hours for all records at once\n",
    "#     hours_df = (\n",
    "#         lf.select([\n",
    "#             pl.col(\"base_coin\"),\n",
    "#             pl.col(\"block_time\").dt.hour().alias(\"hour\")\n",
    "#         ])\n",
    "#         .collect(engine='gpu')\n",
    "#     )\n",
    "\n",
    "#     # Calculate hour counts for all mints at once\n",
    "#     hour_counts = (\n",
    "#         hours_df.group_by([\"base_coin\", \"hour\"])\n",
    "#         .agg(pl.len().alias(\"count\"))\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         # Convert to CUDF for faster GPU-based calculations\n",
    "#         cudf_hour_counts = cudf.DataFrame.from_pandas(hour_counts.to_pandas())\n",
    "#         cudf_hour_counts_grouped = cudf_hour_counts.groupby('base_coin')\n",
    "        \n",
    "#         mint_entropy = []\n",
    "#         for mint in unique_mints:\n",
    "#             try:\n",
    "#                 mint_hours = cudf_hour_counts[cudf_hour_counts['base_coin'] == mint]\n",
    "#                 if len(mint_hours) == 0:\n",
    "#                     mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#                     continue\n",
    "                \n",
    "#                 total = mint_hours['count'].sum()\n",
    "#                 if total == 0:\n",
    "#                     mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#                     continue\n",
    "                \n",
    "#                 probs = mint_hours['count'] / total\n",
    "#                 probs_cp = cp.array(probs.values)\n",
    "#                 non_zero_probs = probs_cp[probs_cp > 0]\n",
    "                \n",
    "#                 if len(non_zero_probs) == 0:\n",
    "#                     mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#                     continue\n",
    "                \n",
    "#                 entropy = -cp.sum(non_zero_probs * cp.log2(non_zero_probs))\n",
    "#                 mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": float(entropy)})\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing mint {mint} for hour entropy: {e}\")\n",
    "#                 mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "                \n",
    "#     except (ImportError, ModuleNotFoundError):\n",
    "#         print(\"CUDF not available, falling back to CPU calculation\")\n",
    "#         # Calculate entropy for each mint\n",
    "#         mint_entropy = []\n",
    "#         for mint in unique_mints:\n",
    "#             mint_hours = hour_counts.filter(pl.col(\"base_coin\") == mint)\n",
    "#             if mint_hours.height == 0:\n",
    "#                 mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#                 continue\n",
    "            \n",
    "#             total = mint_hours[\"count\"].sum()\n",
    "#             if total == 0:\n",
    "#                 mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#                 continue\n",
    "            \n",
    "#             probs = mint_hours[\"count\"] / total\n",
    "#             non_zero_probs = probs.filter(probs > 0).to_numpy()\n",
    "            \n",
    "#             if len(non_zero_probs) == 0:\n",
    "#                 mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#                 continue\n",
    "            \n",
    "#             entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "#             mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": float(entropy)})\n",
    "        \n",
    "#     hour_entropy_df = pl.DataFrame(mint_entropy)\n",
    "#     hour_end = time.time()\n",
    "#     print(f\"Time to compute hour entropy: {hour_end - hour_start:.4f} seconds\")\n",
    "\n",
    "#     # Price and volatility - GPU-optimized version\n",
    "#     price_start = time.time()\n",
    "#     print(\"Calculating price and volatility metrics using GPU...\")\n",
    "\n",
    "#     # Calculate prices for all records at once\n",
    "#     price_df_all = (\n",
    "#         lf.with_columns(\n",
    "#             (pl.col(\"quote_coin_amount\") / pl.col(\"base_coin_amount\"))\n",
    "#             .replace(0, None)\n",
    "#             .alias(\"price\")\n",
    "#         )\n",
    "#         .select([\"base_coin\", \"slot\", \"price\"])\n",
    "#         .collect(engine='gpu')\n",
    "#         .filter(\n",
    "#             (pl.col(\"price\").is_not_nan()) & \n",
    "#             (pl.col(\"price\").is_not_null()) & \n",
    "#             (pl.col(\"price\").is_finite())\n",
    "#         )\n",
    "#         .sort([\"base_coin\", \"slot\"])\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         # Convert to CUDF for faster GPU-based calculations\n",
    "#         cudf_price_df = cudf.DataFrame.from_pandas(price_df_all.to_pandas())\n",
    "        \n",
    "#         # Calculate price metrics for each mint\n",
    "#         price_metrics = []\n",
    "#         for mint in unique_mints:\n",
    "#             try:\n",
    "#                 mint_prices = cudf_price_df[cudf_price_df['base_coin'] == mint]\n",
    "                \n",
    "#                 if len(mint_prices) == 0:\n",
    "#                     price_metrics.append({\n",
    "#                         \"base_coin\": mint,\n",
    "#                         \"avg_price\": 0.0,\n",
    "#                         \"price_vol_mean\": 0.0,\n",
    "#                         \"price_vol_max\": 0.0,\n",
    "#                         \"price_slippage\": 0.0\n",
    "#                     })\n",
    "#                     continue\n",
    "                \n",
    "#                 prices_cp = cp.array(mint_prices['price'].values)\n",
    "#                 avg_price = float(cp.mean(prices_cp))\n",
    "#                 price_min = float(cp.min(prices_cp))\n",
    "#                 price_max = float(cp.max(prices_cp))\n",
    "                \n",
    "#                 # Calculate rolling standard deviation with CuPy\n",
    "#                 if len(prices_cp) >= 5:\n",
    "#                     window_size = 5\n",
    "#                     rolling_stds = []\n",
    "#                     for i in range(len(prices_cp) - window_size + 1):\n",
    "#                         window = prices_cp[i:i+window_size]\n",
    "#                         rolling_stds.append(float(cp.std(window)))\n",
    "#                     if rolling_stds:\n",
    "#                         price_vol_mean = float(cp.mean(cp.array(rolling_stds)))\n",
    "#                         price_vol_max = float(cp.max(cp.array(rolling_stds)))\n",
    "#                     else:\n",
    "#                         price_vol_mean = float(cp.std(prices_cp))\n",
    "#                         price_vol_max = price_vol_mean\n",
    "#                 else:\n",
    "#                     price_vol_mean = float(cp.std(prices_cp))\n",
    "#                     price_vol_max = price_vol_mean\n",
    "                \n",
    "#                 price_slippage = (price_max - price_min) / (avg_price + 1e-6) if avg_price != 0 else 0.0\n",
    "                \n",
    "#                 price_metrics.append({\n",
    "#                     \"base_coin\": mint,\n",
    "#                     \"avg_price\": float(avg_price),\n",
    "#                     \"price_vol_mean\": float(price_vol_mean),\n",
    "#                     \"price_vol_max\": float(price_vol_max),\n",
    "#                     \"price_slippage\": float(price_slippage)\n",
    "#                 })\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing mint {mint} for price metrics: {e}\")\n",
    "#                 price_metrics.append({\n",
    "#                     \"base_coin\": mint,\n",
    "#                     \"avg_price\": 0.0,\n",
    "#                     \"price_vol_mean\": 0.0,\n",
    "#                     \"price_vol_max\": 0.0,\n",
    "#                     \"price_slippage\": 0.0\n",
    "#                 })\n",
    "                \n",
    "#     except (ImportError, ModuleNotFoundError):\n",
    "#         print(\"CUDF/CuPy not available, falling back to CPU calculation\")\n",
    "#         # Calculate price metrics for each mint\n",
    "#         price_metrics = []\n",
    "#         for mint in unique_mints:\n",
    "#             mint_prices = price_df_all.filter(pl.col(\"base_coin\") == mint)\n",
    "            \n",
    "#             if mint_prices.height == 0:\n",
    "#                 price_metrics.append({\n",
    "#                     \"base_coin\": mint,\n",
    "#                     \"avg_price\": 0.0,\n",
    "#                     \"price_vol_mean\": 0.0,\n",
    "#                     \"price_vol_max\": 0.0,\n",
    "#                     \"price_slippage\": 0.0\n",
    "#                 })\n",
    "#                 continue\n",
    "            \n",
    "#             prices = mint_prices[\"price\"].to_numpy()\n",
    "#             avg_price = np.mean(prices)\n",
    "#             price_min = np.min(prices)\n",
    "#             price_max = np.max(prices)\n",
    "            \n",
    "#             # Calculate rolling standard deviation without pandas\n",
    "#             if len(prices) >= 5:\n",
    "#                 # Calculate rolling window standard deviations with numpy\n",
    "#                 window_size = 5\n",
    "#                 rolling_stds = []\n",
    "#                 for i in range(len(prices) - window_size + 1):\n",
    "#                     window = prices[i:i+window_size]\n",
    "#                     rolling_stds.append(np.std(window))\n",
    "#                 if rolling_stds:\n",
    "#                     price_vol_mean = np.mean(rolling_stds)\n",
    "#                     price_vol_max = np.max(rolling_stds)\n",
    "#                 else:\n",
    "#                     price_vol_mean = np.std(prices)\n",
    "#                     price_vol_max = price_vol_mean\n",
    "#             else:\n",
    "#                 price_vol_mean = np.std(prices)\n",
    "#                 price_vol_max = price_vol_mean\n",
    "            \n",
    "#             price_slippage = (price_max - price_min) / (avg_price + 1e-6) if avg_price != 0 else 0.0\n",
    "            \n",
    "#             price_metrics.append({\n",
    "#                 \"base_coin\": mint,\n",
    "#                 \"avg_price\": float(avg_price),\n",
    "#                 \"price_vol_mean\": float(price_vol_mean),\n",
    "#                 \"price_vol_max\": float(price_vol_max),\n",
    "#                 \"price_slippage\": float(price_slippage)\n",
    "#             })\n",
    "\n",
    "#     price_df = pl.DataFrame(price_metrics)\n",
    "#     price_end = time.time()\n",
    "#     print(f\"Time to compute price and volatility metrics: {price_end - price_start:.4f} seconds\")\n",
    "\n",
    "#     # Time-weighted features - GPU-optimized version\n",
    "#     tw_start = time.time()\n",
    "#     print(\"Calculating time-weighted features using GPU...\")\n",
    "\n",
    "#     # Get max slot for each base_coin in one operation\n",
    "#     max_slots = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(pl.col(\"slot\").max().alias(\"max_slot\"))\n",
    "#         .collect(engine='gpu')\n",
    "#     )\n",
    "\n",
    "#     # Create expanded dataframe with time weights\n",
    "#     tw_expanded = (\n",
    "#         lf.select([\n",
    "#             \"base_coin\", \"slot\", \"base_coin_amount\", \"quote_coin_amount\"\n",
    "#         ])\n",
    "#         .collect(engine='gpu')\n",
    "#         .join(max_slots, on=\"base_coin\", how=\"left\")\n",
    "#         .with_columns(\n",
    "#             (0.1 * (pl.col(\"slot\") - pl.col(\"max_slot\")))\n",
    "#             .exp()\n",
    "#             .alias(\"time_weight\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         # Convert to CUDF for faster GPU-based calculations\n",
    "#         cudf_tw_expanded = cudf.DataFrame.from_pandas(tw_expanded.to_pandas())\n",
    "        \n",
    "#         # Calculate time-weighted sums for all mints at once using GPU\n",
    "#         tw_results_gpu = []\n",
    "#         for mint in unique_mints:\n",
    "#             mint_data = cudf_tw_expanded[cudf_tw_expanded['base_coin'] == mint]\n",
    "#             if len(mint_data) == 0:\n",
    "#                 tw_results_gpu.append({\n",
    "#                     \"base_coin\": mint,\n",
    "#                     \"tw_base_coin_amount\": 0.0,\n",
    "#                     \"tw_quote_coin_amount\": 0.0\n",
    "#                 })\n",
    "#                 continue\n",
    "            \n",
    "#             tw_base = float((mint_data['base_coin_amount'] * mint_data['time_weight']).sum())\n",
    "#             tw_quote = float((mint_data['quote_coin_amount'] * mint_data['time_weight']).sum())\n",
    "            \n",
    "#             tw_results_gpu.append({\n",
    "#                 \"base_coin\": mint,\n",
    "#                 \"tw_base_coin_amount\": tw_base if not np.isnan(tw_base) else 0.0,\n",
    "#                 \"tw_quote_coin_amount\": tw_quote if not np.isnan(tw_quote) else 0.0\n",
    "#             })\n",
    "            \n",
    "#         tw_df = pl.DataFrame(tw_results_gpu)\n",
    "        \n",
    "#     except (ImportError, ModuleNotFoundError):\n",
    "#         print(\"CUDF not available, falling back to CPU calculation\")\n",
    "#         # Calculate time-weighted sums for all mints at once\n",
    "#         tw_results = (\n",
    "#             tw_expanded.group_by(\"base_coin\")\n",
    "#             .agg([\n",
    "#                 (pl.col(\"base_coin_amount\") * pl.col(\"time_weight\")).sum().alias(\"tw_base_coin_amount\"),\n",
    "#                 (pl.col(\"quote_coin_amount\") * pl.col(\"time_weight\")).sum().alias(\"tw_quote_coin_amount\")\n",
    "#             ])\n",
    "#             .with_columns([\n",
    "#                 pl.col(\"tw_base_coin_amount\").fill_null(0.0).cast(pl.Float64),\n",
    "#                 pl.col(\"tw_quote_coin_amount\").fill_null(0.0).cast(pl.Float64)\n",
    "#             ])\n",
    "#         )\n",
    "#         tw_df = tw_results\n",
    "\n",
    "#     tw_end = time.time()\n",
    "#     print(f\"Time to compute time-weighted features: {tw_end - tw_start:.4f} seconds\")\n",
    "    \n",
    "#     # Collect all lazy results with GPU engine\n",
    "#     print(\"Collecting lazy results with GPU engine...\")\n",
    "#     collect_start = time.time()\n",
    "#     collected = {\n",
    "#         \"counts\": counts.collect(engine='gpu'),\n",
    "#         \"direction_counts\": direction_counts.collect(engine='gpu'),\n",
    "#         \"buy_wallets\": buy_wallets.collect(engine='gpu'),\n",
    "#         \"sell_wallets\": sell_wallets.collect(engine='gpu'),\n",
    "#         \"base_stats\": base_stats.collect(engine='gpu'),\n",
    "#         \"quote_stats\": quote_stats.collect(engine='gpu'),\n",
    "#         \"time_stats\": time_stats.collect(engine='gpu'),\n",
    "#         \"slot_stats\": slot_stats.collect(engine='gpu'),\n",
    "#         \"early_txs\": early_txs.collect(engine='gpu'),\n",
    "#         \"balance_features\": balance_features.collect(engine='gpu'),\n",
    "#         \"balance_vol\": balance_vol.collect(engine='gpu'),\n",
    "#         \"gas_metrics\": gas_metrics.collect(engine='gpu'),\n",
    "#         \"liq_stats\": liq_stats.collect(engine='gpu'),\n",
    "#         \"flow_imbalance\": flow_imbalance.collect(engine='gpu')\n",
    "#     }\n",
    "#     collect_end = time.time()\n",
    "#     print(f\"Time to collect all lazy results: {collect_end - collect_start:.4f} seconds\")\n",
    "    \n",
    "#     # Merge all features\n",
    "#     print(\"Merging all features...\")\n",
    "#     merge_start = time.time()\n",
    "#     result = collected[\"counts\"].join(collected[\"direction_counts\"], on=\"base_coin\", how=\"left\")\n",
    "#     result = result.with_columns(\n",
    "#         (pl.col(\"buy_count\") / pl.col(\"total_txs\")).alias(\"buy_ratio\")\n",
    "#     )\n",
    "    \n",
    "#     dfs_to_join = [\n",
    "#         collected[\"buy_wallets\"],\n",
    "#         collected[\"sell_wallets\"],\n",
    "#         collected[\"base_stats\"],\n",
    "#         collected[\"quote_stats\"],\n",
    "#         collected[\"time_stats\"].select([\"base_coin\", \"activity_duration_sec\", \"first_time\", \"last_time\", \"tx_per_sec\"]),\n",
    "#         collected[\"slot_stats\"],\n",
    "#         collected[\"early_txs\"],\n",
    "#         collected[\"balance_features\"],\n",
    "#         collected[\"balance_vol\"],\n",
    "#         collected[\"gas_metrics\"],\n",
    "#         concentration_df,\n",
    "#         hour_entropy_df,\n",
    "#         collected[\"flow_imbalance\"],\n",
    "#         price_df,\n",
    "#         tw_df,\n",
    "#         collected[\"liq_stats\"]\n",
    "#     ]\n",
    "    \n",
    "#     for df in dfs_to_join:\n",
    "#         result = result.join(df, on=\"base_coin\", how=\"left\")\n",
    "#     merge_end = time.time()\n",
    "#     print(f\"Time to join all features: {merge_end - merge_start:.4f} seconds\")\n",
    "    \n",
    "#     # Final processing\n",
    "#     final_start = time.time()\n",
    "#     result = result.rename({\"base_coin\": \"mint\"}).fill_null(0)\n",
    "#     final_end = time.time()\n",
    "#     print(f\"Time for final processing: {final_end - final_start:.4f} seconds\")\n",
    "    \n",
    "#     total_end_time = time.time()\n",
    "#     print(f\"Total processing time: {total_end_time - total_start_time:.4f} seconds\")\n",
    "#     print(f\"Polars processing complete with {result.height} rows and {len(result.columns)} columns\")\n",
    "    \n",
    "#     # Summary of time profiling\n",
    "#     print(\"\\n=== TIMING SUMMARY ===\")\n",
    "#     timing_data = [\n",
    "#         (\"Convert to LazyFrame\", convert_end - convert_start),\n",
    "#         (\"Datetime conversion\", datetime_end - datetime_start),\n",
    "#         (\"Basic counts\", counts_end - counts_start),\n",
    "#         (\"Direction-based metrics\", dir_end - dir_start),\n",
    "#         (\"Wallet counts\", wallet_end - wallet_start),\n",
    "#         (\"Base/quote statistics\", stats_end - stats_start),\n",
    "#         (\"Time-based features\", time_feat_end - time_feat_start),\n",
    "#         (\"Slot features\", slot_end - slot_start),\n",
    "#         (\"Balance features\", balance_end - balance_start),\n",
    "#         (\"Gas and liquidity features\", gas_end - gas_start),\n",
    "#         (\"Flow imbalance features\", flow_end - flow_start),\n",
    "#         (\"Wallet concentration metrics\", wallet_conc_end - wallet_conc_start),\n",
    "#         (\"Hour entropy\", hour_end - hour_start),\n",
    "#         (\"Price and volatility metrics\", price_end - price_start),\n",
    "#         (\"Time-weighted features\", tw_end - tw_start),\n",
    "#         (\"Collect lazy results\", collect_end - collect_start),\n",
    "#         (\"Join all features\", merge_end - merge_start),\n",
    "#         (\"Final processing\", final_end - final_start),\n",
    "#         (\"Total processing time\", total_end_time - total_start_time)\n",
    "#     ]\n",
    "    \n",
    "#     timing_data.sort(key=lambda x: x[1], reverse=True)\n",
    "#     for operation, duration in timing_data:\n",
    "#         print(f\"{operation}: {duration:.4f} seconds ({(duration/(total_end_time - total_start_time))*100:.2f}%)\")\n",
    "    \n",
    "#     return result.to_pandas()\n",
    "\n",
    "# start_time = datetime.now()\n",
    "# mint_transactions = fast_polars_process(transactions_df.iloc[:1000000], n_jobs=4)\n",
    "# end_time = datetime.now()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f'Total execution time: {elapsed_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24962819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:19.925694Z",
     "iopub.status.busy": "2025-07-23T12:03:19.925305Z",
     "iopub.status.idle": "2025-07-23T12:03:19.938689Z",
     "shell.execute_reply": "2025-07-23T12:03:19.937283Z"
    },
    "papermill": {
     "duration": 0.040732,
     "end_time": "2025-07-23T12:03:19.940952",
     "exception": false,
     "start_time": "2025-07-23T12:03:19.900220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import concurrent.futures\n",
    "# import time\n",
    "\n",
    "# def fast_polars_process(transactions_df, n_jobs=4):\n",
    "#     \"\"\"Fully optimized version using Polars LazyFrames for all operations\"\"\"\n",
    "#     total_start_time = time.time()\n",
    "#     print(f\"Starting Polars transaction processing at {datetime.now()}\")\n",
    "    \n",
    "#     # Convert to LazyFrame\n",
    "#     convert_start = time.time()\n",
    "#     if not isinstance(transactions_df, pl.LazyFrame):\n",
    "#         if not isinstance(transactions_df, pl.DataFrame):\n",
    "#             transactions_df = pl.from_pandas(transactions_df)\n",
    "#         lf = transactions_df.lazy()\n",
    "#     else:\n",
    "#         lf = transactions_df\n",
    "#     convert_end = time.time()\n",
    "#     print(f\"Time to convert to LazyFrame: {convert_end - convert_start:.4f} seconds\")\n",
    "    \n",
    "#     # Ensure datetime conversion\n",
    "#     datetime_start = time.time()\n",
    "#     lf = lf.with_columns(\n",
    "#         pl.col(\"block_time\").str.to_datetime(strict=False)\n",
    "#     )\n",
    "#     datetime_end = time.time()\n",
    "#     print(f\"Time to convert block_time to datetime: {datetime_end - datetime_start:.4f} seconds\")\n",
    "    \n",
    "#     print(\"Calculating aggregated features...\")\n",
    "    \n",
    "#     # Basic counts\n",
    "#     counts_start = time.time()\n",
    "#     counts = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.len().alias(\"total_txs\"),\n",
    "#             pl.n_unique(\"tx_idx\").alias(\"unique_tx_indices\"),\n",
    "#             pl.n_unique(\"signing_wallet\").alias(\"unique_wallets\")\n",
    "#         )\n",
    "#     )\n",
    "#     counts_end = time.time()\n",
    "#     print(f\"Time to compute basic counts: {counts_end - counts_start:.4f} seconds\")\n",
    "    \n",
    "#     # Direction-based metrics\n",
    "#     dir_start = time.time()\n",
    "#     direction_counts = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.when(pl.col(\"direction\") == \"buy\")\n",
    "#               .then(1)\n",
    "#               .otherwise(0)\n",
    "#               .sum()\n",
    "#               .alias(\"buy_count\"),\n",
    "#             pl.when(pl.col(\"direction\") == \"sell\")\n",
    "#               .then(1)\n",
    "#               .otherwise(0)\n",
    "#               .sum()\n",
    "#               .alias(\"sell_count\")\n",
    "#         )\n",
    "#     )\n",
    "#     dir_end = time.time()\n",
    "#     print(f\"Time to compute direction-based metrics: {dir_end - dir_start:.4f} seconds\")\n",
    "    \n",
    "#     # Wallet counts (lazy)\n",
    "#     wallet_start = time.time()\n",
    "#     buy_wallets = (\n",
    "#         lf.filter(pl.col(\"direction\") == \"buy\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_buy_wallets\"))\n",
    "#     )\n",
    "    \n",
    "#     sell_wallets = (\n",
    "#         lf.filter(pl.col(\"direction\") == \"sell\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_sell_wallets\"))\n",
    "#     )\n",
    "#     wallet_end = time.time()\n",
    "#     print(f\"Time to compute wallet counts: {wallet_end - wallet_start:.4f} seconds\")\n",
    "    \n",
    "#     # Base/quote statistics (lazy)\n",
    "#     stats_start = time.time()\n",
    "#     base_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"base_coin_amount\").sum().alias(\"base_sum\"),\n",
    "#             pl.col(\"base_coin_amount\").mean().alias(\"base_mean\"),\n",
    "#             pl.col(\"base_coin_amount\").max().alias(\"base_max\"),\n",
    "#             pl.col(\"base_coin_amount\").min().alias(\"base_min\"),\n",
    "#             pl.col(\"base_coin_amount\").std().alias(\"base_std\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     quote_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"quote_coin_amount\").sum().alias(\"quote_sum\"),\n",
    "#             pl.col(\"quote_coin_amount\").mean().alias(\"quote_mean\"),\n",
    "#             pl.col(\"quote_coin_amount\").max().alias(\"quote_max\"),\n",
    "#             pl.col(\"quote_coin_amount\").min().alias(\"quote_min\"),\n",
    "#             pl.col(\"quote_coin_amount\").std().alias(\"quote_std\")\n",
    "#         )\n",
    "#     )\n",
    "#     stats_end = time.time()\n",
    "#     print(f\"Time to compute base/quote statistics: {stats_end - stats_start:.4f} seconds\")\n",
    "    \n",
    "#     # Time-based features (lazy)\n",
    "#     time_feat_start = time.time()\n",
    "#     time_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"block_time\").min().alias(\"first_time\"),\n",
    "#             pl.col(\"block_time\").max().alias(\"last_time\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_time\") - pl.col(\"first_time\"))\n",
    "#             .dt.total_seconds()\n",
    "#             .alias(\"activity_duration_sec\")\n",
    "#         )\n",
    "#         .join(counts.select(\"base_coin\", \"total_txs\"), on=\"base_coin\")\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"total_txs\") / (pl.col(\"activity_duration_sec\") + 1e-6))\n",
    "#             .alias(\"tx_per_sec\")\n",
    "#         )\n",
    "#     )\n",
    "#     time_feat_end = time.time()\n",
    "#     print(f\"Time to compute time-based features: {time_feat_end - time_feat_start:.4f} seconds\")\n",
    "    \n",
    "#     # Slot features (lazy)\n",
    "#     slot_start = time.time()\n",
    "#     slot_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"slot\").min().alias(\"first_slot\"),\n",
    "#             pl.col(\"slot\").max().alias(\"last_slot\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_slot\") - pl.col(\"first_slot\")).alias(\"slot_span\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Early transactions (lazy)\n",
    "#     early_txs = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"tx_idx\")\n",
    "#             .filter(pl.col(\"slot\") <= pl.col(\"slot\").min() + 100)\n",
    "#             .count()\n",
    "#             .alias(\"early_txs\")\n",
    "#         )\n",
    "#     )\n",
    "#     slot_end = time.time()\n",
    "#     print(f\"Time to compute slot features: {slot_end - slot_start:.4f} seconds\")\n",
    "    \n",
    "#     # Balance features (lazy)\n",
    "#     balance_start = time.time()\n",
    "#     sorted_lf = lf.sort([\"base_coin\", \"slot\"])\n",
    "#     first_values = (\n",
    "#         sorted_lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").first().alias(\"first_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").first().alias(\"first_sol\")\n",
    "#         )\n",
    "#     )\n",
    "#     last_values = (\n",
    "#         sorted_lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").last().alias(\"last_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").last().alias(\"last_sol\")\n",
    "#         )\n",
    "#     )\n",
    "#     balance_features = (\n",
    "#         first_values.join(last_values, on=\"base_coin\")\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_sol\") - pl.col(\"first_sol\")).alias(\"sol_change\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Balance volatility (lazy)\n",
    "#     balance_vol = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").std().alias(\"balance_volatility\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").max().alias(\"max_balance\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").min().alias(\"min_balance\")\n",
    "#         )\n",
    "#     )\n",
    "#     balance_end = time.time()\n",
    "#     print(f\"Time to compute balance features: {balance_end - balance_start:.4f} seconds\")\n",
    "    \n",
    "#     # Gas features (lazy)\n",
    "#     gas_start = time.time()\n",
    "#     gas_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"provided_gas_fee\").sum().alias(\"total_gas_fees\"),\n",
    "#             pl.col(\"provided_gas_fee\").mean().alias(\"avg_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").std().alias(\"std_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").max().alias(\"max_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").min().alias(\"min_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_limit\").mean().alias(\"avg_gas_limit\"),\n",
    "#             pl.col(\"consumed_gas\").mean().alias(\"avg_consumed_gas\"),\n",
    "#             pl.col(\"consumed_gas\").sum().alias(\"total_consumed_gas\"),\n",
    "#             pl.col(\"fee\").sum().alias(\"total_fee\"),\n",
    "#             (pl.col(\"provided_gas_limit\") - pl.col(\"consumed_gas\")).mean().alias(\"limit_utilization\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Liquidity stats (lazy)\n",
    "#     liq_stats = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_sol_balance_after\").mean().alias(\"liq_virtual_sol_balance_after_mean\"),\n",
    "#             pl.col(\"quote_coin_amount\").sum().alias(\"liq_quote_coin_amount\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Gas metrics (lazy)\n",
    "#     gas_metrics = (\n",
    "#         gas_stats.join(quote_stats.select([\"base_coin\", \"quote_sum\"]), on=\"base_coin\")\n",
    "#         .with_columns([\n",
    "#             (pl.col(\"total_fee\") / (pl.col(\"quote_sum\") + 1e-6)).alias(\"fee_per_sol\"),\n",
    "#             pl.col(\"avg_consumed_gas\").alias(\"gas_per_tx\"),\n",
    "#             (pl.col(\"quote_sum\") / (pl.col(\"total_consumed_gas\") + 1e-6)).alias(\"gas_efficiency\")\n",
    "#         ])\n",
    "#     )\n",
    "#     gas_end = time.time()\n",
    "#     print(f\"Time to compute gas and liquidity features: {gas_end - gas_start:.4f} seconds\")\n",
    "    \n",
    "#     # Flow imbalance (lazy)\n",
    "#     flow_start = time.time()\n",
    "#     flow_imbalance = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"buy\").sum().alias(\"buy_quote_amount\"),\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"sell\").sum().alias(\"sell_quote_amount\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             ((pl.col(\"buy_quote_amount\") - pl.col(\"sell_quote_amount\")) / \n",
    "#              (pl.col(\"buy_quote_amount\") + pl.col(\"sell_quote_amount\") + 1e-6)).alias(\"flow_imbalance\")\n",
    "#         )\n",
    "#         .select([\"base_coin\", \"flow_imbalance\"])\n",
    "#     )\n",
    "#     flow_end = time.time()\n",
    "#     print(f\"Time to compute flow imbalance features: {flow_end - flow_start:.4f} seconds\")\n",
    "    \n",
    "#     # Collect necessary data for custom functions\n",
    "#     print(\"Collecting data for custom functions...\")\n",
    "#     collect_start = time.time()\n",
    "#     unique_mints = counts.collect()[\"base_coin\"].to_list()\n",
    "#     collect_end = time.time()\n",
    "#     print(f\"Time to collect unique mints: {collect_end - collect_start:.4f} seconds\")\n",
    "    \n",
    "#     # Wallet concentration metrics - optimized version\n",
    "#     wallet_conc_start = time.time()\n",
    "#     print(\"Calculating wallet concentration metrics...\")\n",
    "\n",
    "#     # Pre-calculate wallet volumes for all mints at once\n",
    "#     wallet_volumes = (\n",
    "#         lf.group_by([\"base_coin\", \"signing_wallet\"])\n",
    "#         .agg(pl.col(\"base_coin_amount\").sum().alias(\"volume\"))\n",
    "#         .collect()\n",
    "#     )\n",
    "\n",
    "#     # Function to calculate Gini and top5 share for a single mint using pre-calculated data\n",
    "#     def calculate_concentration_metrics_optimized(mint):\n",
    "#         mint_volumes = wallet_volumes.filter(pl.col(\"base_coin\") == mint)\n",
    "        \n",
    "#         if mint_volumes.height <= 1:\n",
    "#             return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "        \n",
    "#         sorted_values = mint_volumes.sort(\"volume\")[\"volume\"].to_numpy()\n",
    "#         n = len(sorted_values)\n",
    "        \n",
    "#         # Gini coefficient calculation\n",
    "#         cumulative_values = np.cumsum(sorted_values)\n",
    "#         total = cumulative_values[-1]\n",
    "#         if total <= 0:\n",
    "#             return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "        \n",
    "#         # Faster Gini calculation using cumulative sum\n",
    "#         gini_coeff = (n + 1 - 2 * np.sum(cumulative_values) / total) / n\n",
    "        \n",
    "#         # Top wallet share\n",
    "#         top_k = min(5, n)\n",
    "#         top5_wallet_share = np.sum(sorted_values[-top_k:]) / total\n",
    "        \n",
    "#         return {\"base_coin\": mint, \"gini_coeff\": float(gini_coeff), \"top5_wallet_share\": float(top5_wallet_share)}\n",
    "\n",
    "#     # Process in parallel using the pre-calculated data\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         concentration_results = list(executor.map(calculate_concentration_metrics_optimized, unique_mints))\n",
    "#     concentration_df = pl.DataFrame(concentration_results)\n",
    "\n",
    "#     wallet_conc_end = time.time()\n",
    "#     print(f\"Time to compute wallet concentration metrics: {wallet_conc_end - wallet_conc_start:.4f} seconds\")\n",
    "\n",
    "#     # Hour entropy - optimized version\n",
    "#     hour_start = time.time()\n",
    "#     print(\"Calculating hour entropy...\")\n",
    "\n",
    "#     # Calculate hours for all records at once\n",
    "#     hours_df = (\n",
    "#         lf.select([\n",
    "#             pl.col(\"base_coin\"),\n",
    "#             pl.col(\"block_time\").dt.hour().alias(\"hour\")\n",
    "#         ])\n",
    "#         .collect()\n",
    "#     )\n",
    "\n",
    "#     # Calculate hour counts for all mints at once\n",
    "#     hour_counts = (\n",
    "#         hours_df.group_by([\"base_coin\", \"hour\"])\n",
    "#         .agg(pl.len().alias(\"count\"))\n",
    "#     )\n",
    "\n",
    "#     # Calculate entropy for each mint\n",
    "#     mint_entropy = []\n",
    "#     for mint in unique_mints:\n",
    "#         mint_hours = hour_counts.filter(pl.col(\"base_coin\") == mint)\n",
    "#         if mint_hours.height == 0:\n",
    "#             mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#             continue\n",
    "        \n",
    "#         total = mint_hours[\"count\"].sum()\n",
    "#         if total == 0:\n",
    "#             mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#             continue\n",
    "        \n",
    "#         probs = mint_hours[\"count\"] / total\n",
    "#         non_zero_probs = probs.filter(probs > 0).to_numpy()\n",
    "        \n",
    "#         if len(non_zero_probs) == 0:\n",
    "#             mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": 0.0})\n",
    "#             continue\n",
    "        \n",
    "#         entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "#         mint_entropy.append({\"base_coin\": mint, \"hour_entropy\": float(entropy)})\n",
    "        \n",
    "#     hour_entropy_df = pl.DataFrame(mint_entropy)\n",
    "#     hour_end = time.time()\n",
    "#     print(f\"Time to compute hour entropy: {hour_end - hour_start:.4f} seconds\")\n",
    "\n",
    "#     # Price and volatility - optimized version\n",
    "#     price_start = time.time()\n",
    "#     print(\"Calculating price and volatility metrics...\")\n",
    "\n",
    "#     # Calculate prices for all records at once\n",
    "#     price_df_all = (\n",
    "#         lf.with_columns(\n",
    "#             (pl.col(\"quote_coin_amount\") / pl.col(\"base_coin_amount\"))\n",
    "#             .replace(0, None)\n",
    "#             .alias(\"price\")\n",
    "#         )\n",
    "#         .select([\"base_coin\", \"slot\", \"price\"])\n",
    "#         .collect()\n",
    "#         .filter(\n",
    "#             (pl.col(\"price\").is_not_nan()) & \n",
    "#             (pl.col(\"price\").is_not_null()) & \n",
    "#             (pl.col(\"price\").is_finite())\n",
    "#         )\n",
    "#         .sort([\"base_coin\", \"slot\"])\n",
    "#     )\n",
    "\n",
    "#     # Calculate price metrics for each mint\n",
    "#     price_metrics = []\n",
    "#     for mint in unique_mints:\n",
    "#         mint_prices = price_df_all.filter(pl.col(\"base_coin\") == mint)\n",
    "        \n",
    "#         if mint_prices.height == 0:\n",
    "#             price_metrics.append({\n",
    "#                 \"base_coin\": mint,\n",
    "#                 \"avg_price\": 0.0,\n",
    "#                 \"price_vol_mean\": 0.0,\n",
    "#                 \"price_vol_max\": 0.0,\n",
    "#                 \"price_slippage\": 0.0\n",
    "#             })\n",
    "#             continue\n",
    "        \n",
    "#         prices = mint_prices[\"price\"].to_numpy()\n",
    "#         avg_price = np.mean(prices)\n",
    "#         price_min = np.min(prices)\n",
    "#         price_max = np.max(prices)\n",
    "        \n",
    "#         # Calculate rolling standard deviation without pandas\n",
    "#         if len(prices) >= 5:\n",
    "#             # Calculate rolling window standard deviations with numpy\n",
    "#             window_size = 5\n",
    "#             rolling_stds = []\n",
    "#             for i in range(len(prices) - window_size + 1):\n",
    "#                 window = prices[i:i+window_size]\n",
    "#                 rolling_stds.append(np.std(window))\n",
    "#             if rolling_stds:\n",
    "#                 price_vol_mean = np.mean(rolling_stds)\n",
    "#                 price_vol_max = np.max(rolling_stds)\n",
    "#             else:\n",
    "#                 price_vol_mean = np.std(prices)\n",
    "#                 price_vol_max = price_vol_mean\n",
    "#         else:\n",
    "#             price_vol_mean = np.std(prices)\n",
    "#             price_vol_max = price_vol_mean\n",
    "        \n",
    "#         price_slippage = (price_max - price_min) / (avg_price + 1e-6) if avg_price != 0 else 0.0\n",
    "        \n",
    "#         price_metrics.append({\n",
    "#             \"base_coin\": mint,\n",
    "#             \"avg_price\": float(avg_price),\n",
    "#             \"price_vol_mean\": float(price_vol_mean),\n",
    "#             \"price_vol_max\": float(price_vol_max),\n",
    "#             \"price_slippage\": float(price_slippage)\n",
    "#         })\n",
    "\n",
    "#     price_df = pl.DataFrame(price_metrics)\n",
    "#     price_end = time.time()\n",
    "#     print(f\"Time to compute price and volatility metrics: {price_end - price_start:.4f} seconds\")\n",
    "\n",
    "#     # Time-weighted features - optimized version\n",
    "#     tw_start = time.time()\n",
    "#     print(\"Calculating time-weighted features...\")\n",
    "\n",
    "#     # Get max slot for each base_coin in one operation\n",
    "#     max_slots = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(pl.col(\"slot\").max().alias(\"max_slot\"))\n",
    "#         .collect()\n",
    "#     )\n",
    "\n",
    "#     # Create expanded dataframe with time weights\n",
    "#     tw_expanded = (\n",
    "#         lf.select([\n",
    "#             \"base_coin\", \"slot\", \"base_coin_amount\", \"quote_coin_amount\"\n",
    "#         ])\n",
    "#         .collect()\n",
    "#         .join(max_slots, on=\"base_coin\", how=\"left\")\n",
    "#         .with_columns(\n",
    "#             (0.1 * (pl.col(\"slot\") - pl.col(\"max_slot\")))\n",
    "#             .exp()\n",
    "#             .alias(\"time_weight\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # Calculate time-weighted sums for all mints at once\n",
    "#     tw_results = (\n",
    "#         tw_expanded.group_by(\"base_coin\")\n",
    "#         .agg([\n",
    "#             (pl.col(\"base_coin_amount\") * pl.col(\"time_weight\")).sum().alias(\"tw_base_coin_amount\"),\n",
    "#             (pl.col(\"quote_coin_amount\") * pl.col(\"time_weight\")).sum().alias(\"tw_quote_coin_amount\")\n",
    "#         ])\n",
    "#         .with_columns([\n",
    "#             pl.col(\"tw_base_coin_amount\").fill_null(0.0).cast(pl.Float64),\n",
    "#             pl.col(\"tw_quote_coin_amount\").fill_null(0.0).cast(pl.Float64)\n",
    "#         ])\n",
    "#     )\n",
    "\n",
    "#     tw_df = tw_results\n",
    "#     tw_end = time.time()\n",
    "#     print(f\"Time to compute time-weighted features: {tw_end - tw_start:.4f} seconds\")\n",
    "    \n",
    "#     # Collect all lazy results\n",
    "#     print(\"Collecting lazy results...\")\n",
    "#     collect_start = time.time()\n",
    "#     collected = {\n",
    "#         \"counts\": counts.collect(),\n",
    "#         \"direction_counts\": direction_counts.collect(),\n",
    "#         \"buy_wallets\": buy_wallets.collect(),\n",
    "#         \"sell_wallets\": sell_wallets.collect(),\n",
    "#         \"base_stats\": base_stats.collect(),\n",
    "#         \"quote_stats\": quote_stats.collect(),\n",
    "#         \"time_stats\": time_stats.collect(),\n",
    "#         \"slot_stats\": slot_stats.collect(),\n",
    "#         \"early_txs\": early_txs.collect(),\n",
    "#         \"balance_features\": balance_features.collect(),\n",
    "#         \"balance_vol\": balance_vol.collect(),\n",
    "#         \"gas_metrics\": gas_metrics.collect(),\n",
    "#         \"liq_stats\": liq_stats.collect(),\n",
    "#         \"flow_imbalance\": flow_imbalance.collect()\n",
    "#     }\n",
    "#     collect_end = time.time()\n",
    "#     print(f\"Time to collect all lazy results: {collect_end - collect_start:.4f} seconds\")\n",
    "    \n",
    "#     # Merge all features\n",
    "#     print(\"Merging all features...\")\n",
    "#     merge_start = time.time()\n",
    "#     result = collected[\"counts\"].join(collected[\"direction_counts\"], on=\"base_coin\", how=\"left\")\n",
    "#     result = result.with_columns(\n",
    "#         (pl.col(\"buy_count\") / pl.col(\"total_txs\")).alias(\"buy_ratio\")\n",
    "#     )\n",
    "    \n",
    "#     dfs_to_join = [\n",
    "#         collected[\"buy_wallets\"],\n",
    "#         collected[\"sell_wallets\"],\n",
    "#         collected[\"base_stats\"],\n",
    "#         collected[\"quote_stats\"],\n",
    "#         collected[\"time_stats\"].select([\"base_coin\", \"activity_duration_sec\", \"first_time\", \"last_time\", \"tx_per_sec\"]),\n",
    "#         collected[\"slot_stats\"],\n",
    "#         collected[\"early_txs\"],\n",
    "#         collected[\"balance_features\"],\n",
    "#         collected[\"balance_vol\"],\n",
    "#         collected[\"gas_metrics\"],\n",
    "#         concentration_df,\n",
    "#         hour_entropy_df,\n",
    "#         collected[\"flow_imbalance\"],\n",
    "#         price_df,\n",
    "#         tw_df,\n",
    "#         collected[\"liq_stats\"]\n",
    "#     ]\n",
    "    \n",
    "#     for df in dfs_to_join:\n",
    "#         result = result.join(df, on=\"base_coin\", how=\"left\")\n",
    "#     merge_end = time.time()\n",
    "#     print(f\"Time to join all features: {merge_end - merge_start:.4f} seconds\")\n",
    "    \n",
    "#     # Final processing\n",
    "#     final_start = time.time()\n",
    "#     result = result.rename({\"base_coin\": \"mint\"}).fill_null(0)\n",
    "#     final_end = time.time()\n",
    "#     print(f\"Time for final processing: {final_end - final_start:.4f} seconds\")\n",
    "    \n",
    "#     total_end_time = time.time()\n",
    "#     print(f\"Total processing time: {total_end_time - total_start_time:.4f} seconds\")\n",
    "#     print(f\"Polars processing complete with {result.height} rows and {len(result.columns)} columns\")\n",
    "    \n",
    "#     # Summary of time profiling\n",
    "#     print(\"\\n=== TIMING SUMMARY ===\")\n",
    "#     timing_data = [\n",
    "#         (\"Convert to LazyFrame\", convert_end - convert_start),\n",
    "#         (\"Datetime conversion\", datetime_end - datetime_start),\n",
    "#         (\"Basic counts\", counts_end - counts_start),\n",
    "#         (\"Direction-based metrics\", dir_end - dir_start),\n",
    "#         (\"Wallet counts\", wallet_end - wallet_start),\n",
    "#         (\"Base/quote statistics\", stats_end - stats_start),\n",
    "#         (\"Time-based features\", time_feat_end - time_feat_start),\n",
    "#         (\"Slot features\", slot_end - slot_start),\n",
    "#         (\"Balance features\", balance_end - balance_start),\n",
    "#         (\"Gas and liquidity features\", gas_end - gas_start),\n",
    "#         (\"Flow imbalance features\", flow_end - flow_start),\n",
    "#         (\"Wallet concentration metrics\", wallet_conc_end - wallet_conc_start),\n",
    "#         (\"Hour entropy\", hour_end - hour_start),\n",
    "#         (\"Price and volatility metrics\", price_end - price_start),\n",
    "#         (\"Time-weighted features\", tw_end - tw_start),\n",
    "#         (\"Collect lazy results\", collect_end - collect_start),\n",
    "#         (\"Join all features\", merge_end - merge_start),\n",
    "#         (\"Final processing\", final_end - final_start),\n",
    "#         (\"Total processing time\", total_end_time - total_start_time)\n",
    "#     ]\n",
    "    \n",
    "#     timing_data.sort(key=lambda x: x[1], reverse=True)\n",
    "#     for operation, duration in timing_data:\n",
    "#         print(f\"{operation}: {duration:.4f} seconds ({(duration/(total_end_time - total_start_time))*100:.2f}%)\")\n",
    "    \n",
    "#     return result.to_pandas()\n",
    "\n",
    "# start_time = datetime.now()\n",
    "# mint_transactions = fast_polars_process(transactions_df.iloc[:100000], n_jobs=4)\n",
    "# end_time = datetime.now()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f'Total execution time: {elapsed_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85311129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:19.982712Z",
     "iopub.status.busy": "2025-07-23T12:03:19.982311Z",
     "iopub.status.idle": "2025-07-23T12:03:19.988379Z",
     "shell.execute_reply": "2025-07-23T12:03:19.987234Z"
    },
    "papermill": {
     "duration": 0.029609,
     "end_time": "2025-07-23T12:03:19.990569",
     "exception": false,
     "start_time": "2025-07-23T12:03:19.960960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "\n",
    "# def fast_polars_process(transactions_df, n_jobs=4):\n",
    "#     total_start_time = time.time()\n",
    "#     print(f\"Starting Polars transaction processing at {datetime.now()}\")\n",
    "\n",
    "#     # Convert to LazyFrame\n",
    "#     if not isinstance(transactions_df, pl.LazyFrame):\n",
    "#         if not isinstance(transactions_df, pl.DataFrame):\n",
    "#             transactions_df = pl.from_pandas(transactions_df)\n",
    "#         lf = transactions_df.lazy()\n",
    "#     else:\n",
    "#         lf = transactions_df\n",
    "\n",
    "#     # Ensure datetime conversion\n",
    "#     lf = lf.with_columns(\n",
    "#         pl.col(\"block_time\").str.to_datetime(strict=False)\n",
    "#     )\n",
    "\n",
    "#     # Precompute common group_by operations\n",
    "#     base_group = lf.group_by(\"base_coin\")\n",
    "\n",
    "#     # Vectorized wallet concentration metrics\n",
    "# # Vectorized wallet concentration metrics (FIXED HERE)\n",
    "#     wallet_concentration = (\n",
    "#         lf.group_by([\"base_coin\", \"signing_wallet\"])\n",
    "#         .agg(pl.col(\"base_coin_amount\").sum().alias(\"volume\"))\n",
    "#         .sort([\"base_coin\", \"volume\"])  # Sort within each base_coin group\n",
    "#         .with_columns([\n",
    "#             pl.col(\"volume\").rank(\"ordinal\", descending=False).over(\"base_coin\").alias(\"rank\"),\n",
    "#             pl.col(\"volume\").sum().over(\"base_coin\").alias(\"total_volume\")\n",
    "#         ])\n",
    "#         .filter(pl.col(\"total_volume\") > 0)\n",
    "#         .with_columns([\n",
    "#         # FIXED: Use cumsum() instead of cumulative_sum()\n",
    "#             (pl.col(\"volume\").cum_sum().over(\"base_coin\").last() / pl.col(\"total_volume\")).alias(\"cumsum_ratio\"),\n",
    "#             (pl.col(\"volume\") * pl.col(\"rank\")).sum().over(\"base_coin\").alias(\"rank_volume_sum\")\n",
    "#         ])\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg([\n",
    "#             ((pl.count() + 1 - 2 * pl.col(\"rank_volume_sum\") / pl.col(\"total_volume\")) \n",
    "#              / pl.count()).alias(\"gini_coeff\"),\n",
    "#             (pl.col(\"volume\").top_k(5).sum() / pl.col(\"total_volume\")).alias(\"top5_wallet_share\")\n",
    "#         ])\n",
    "#         .fill_null(0)\n",
    "#     )\n",
    "#     # Vectorized hour entropy\n",
    "#     hour_entropy = (\n",
    "#         lf\n",
    "#         .select([\n",
    "#             \"base_coin\",\n",
    "#             pl.col(\"block_time\").dt.hour().alias(\"hour\")\n",
    "#         ])\n",
    "#         .group_by([\"base_coin\", \"hour\"])\n",
    "#         .agg(\n",
    "#             pl.len().alias(\"count\")                           # use pl.len() not pl.count()\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"count\") / pl.sum(\"count\").over(\"base_coin\"))\n",
    "#               .alias(\"prob\")\n",
    "#         )\n",
    "#         .filter(pl.col(\"prob\") > 0)\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#         # compute -∑ p·log2(p) via .log(2) + .sum()\n",
    "#             (- (pl.col(\"prob\") * pl.col(\"prob\").log(2))).sum()\n",
    "#              .alias(\"hour_entropy\")\n",
    "#         )\n",
    "#         .fill_null(0)\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # Vectorized price and volatility\n",
    "#     price_volatility = (\n",
    "#         lf.with_columns(\n",
    "#             (pl.col(\"quote_coin_amount\") / pl.col(\"base_coin_amount\"))\n",
    "#             .replace(0, None)\n",
    "#             .alias(\"price\")\n",
    "#         )\n",
    "#         .select([\"base_coin\", \"slot\", \"price\"])\n",
    "#         .filter(pl.col(\"price\").is_finite())\n",
    "#         .sort([\"base_coin\", \"slot\"])\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg([\n",
    "#             pl.col(\"price\").mean().alias(\"avg_price\"),\n",
    "#             pl.col(\"price\").rolling_std(window_size=5, min_periods=1).mean().alias(\"price_vol_mean\"),\n",
    "#             pl.col(\"price\").rolling_std(window_size=5, min_periods=1).max().alias(\"price_vol_max\"),\n",
    "#             ((pl.col(\"price\").max() - pl.col(\"price\").min()) \n",
    "#              / (pl.col(\"price\").mean() + 1e-6)).alias(\"price_slippage\")\n",
    "#         ])\n",
    "#         .fill_null(0)\n",
    "#     )\n",
    "\n",
    "#     # Time-weighted features with JIT\n",
    "#     time_weighted = (\n",
    "#         lf.group_by(\"base_coin\")\n",
    "#         .agg(pl.col(\"slot\").max().alias(\"max_slot\"))\n",
    "#         .join(lf.select([\"base_coin\", \"slot\", \"base_coin_amount\", \"quote_coin_amount\"]), \n",
    "#               on=\"base_coin\", how=\"left\")\n",
    "#         .with_columns(\n",
    "#             (0.1 * (pl.col(\"slot\") - pl.col(\"max_slot\")))\n",
    "#             .exp()\n",
    "#             .alias(\"time_weight\")\n",
    "#         )\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg([\n",
    "#             (pl.col(\"base_coin_amount\") * pl.col(\"time_weight\")).sum().alias(\"tw_base_coin_amount\"),\n",
    "#             (pl.col(\"quote_coin_amount\") * pl.col(\"time_weight\")).sum().alias(\"tw_quote_coin_amount\")\n",
    "#         ])\n",
    "#         .collect()  # Use JIT here for large aggregations\n",
    "#         .fill_null(0)\n",
    "#     )\n",
    "\n",
    "#     # Combine all features\n",
    "#     all_features = [\n",
    "#         wallet_concentration,\n",
    "#         hour_entropy,\n",
    "#         price_volatility,\n",
    "#         time_weighted.lazy()\n",
    "#     ]\n",
    "\n",
    "#     # Final join (simplified for brevity)\n",
    "#     result = (\n",
    "#         base_group.agg(\n",
    "#             pl.len().alias(\"total_txs\"),\n",
    "#             pl.n_unique(\"tx_idx\").alias(\"unique_tx_indices\"),\n",
    "#             pl.n_unique(\"signing_wallet\").alias(\"unique_wallets\")\n",
    "#         )\n",
    "#         .join(wallet_concentration, on=\"base_coin\", how=\"left\")\n",
    "#         .join(hour_entropy, on=\"base_coin\", how=\"left\")\n",
    "#         .join(price_volatility, on=\"base_coin\", how=\"left\")\n",
    "#         .join(time_weighted.lazy(), on=\"base_coin\", how=\"left\")\n",
    "#     )\n",
    "\n",
    "#     # Final collection with JIT where beneficial\n",
    "#     final_df = result.collect()\n",
    "#     return final_df.to_pandas()\n",
    "\n",
    "# # Ensure Polars is updated to >=0.18.0\n",
    "# mint_transactions_2 = fast_polars_process(transactions_df.iloc[:100000], n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ea2800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:03:20.032075Z",
     "iopub.status.busy": "2025-07-23T12:03:20.031666Z",
     "iopub.status.idle": "2025-07-23T12:04:43.021557Z",
     "shell.execute_reply": "2025-07-23T12:04:43.019626Z"
    },
    "papermill": {
     "duration": 83.037514,
     "end_time": "2025-07-23T12:04:43.047377",
     "exception": false,
     "start_time": "2025-07-23T12:03:20.009863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Overall Start] 2025-07-23 12:03:20.194857\n",
      "[Prepare LazyFrame] 21.900s\n",
      "[Datetime Conversion] 0.006s\n",
      "[Group By base_coin] 0.001s\n",
      "[Counts & Direction] 0.001s\n",
      "[Wallet Counts] 0.000s\n",
      "[Base/Quote Stats] 0.000s\n",
      "[Time Features] 0.001s\n",
      "[Slot Features] 0.000s\n",
      "[Balance Features] 0.000s\n",
      "[Gas & Liquidity] 0.000s\n",
      "[Flow Imbalance] 0.000s\n",
      "[Wallet Concentration] 0.000s\n",
      "[Hour Entropy] 0.000s\n",
      "[Price Volatility] 0.001s\n",
      "[Time-Weighted] 0.000s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-4a8fe06f882b>:221: DeprecationWarning: the argument `min_periods` for `Expr.rolling_std` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_std(window_size=5, min_periods=1).mean().alias(\"price_vol_mean\"),\n",
      "<ipython-input-17-4a8fe06f882b>:222: DeprecationWarning: the argument `min_periods` for `Expr.rolling_std` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_std(window_size=5, min_periods=1).max().alias(\"price_vol_max\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Join & Collect] 60.903s\n",
      "[Overall Complete] 82.819s, rows=1118389, cols=71\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def fast_polars_process(transactions_df, n_jobs=4):\n",
    "    \"\"\"Fully vectorized Polars pipeline computing ~61 mint-level features with detailed timing.\"\"\"\n",
    "    t0 = time.time()\n",
    "    print(f\"[Overall Start] {datetime.now()}\")\n",
    "    \n",
    "    # Prepare LazyFrame\n",
    "    t1 = time.time()\n",
    "    if isinstance(transactions_df, pl.DataFrame):\n",
    "        lf = transactions_df.lazy()\n",
    "    else:\n",
    "        lf = pl.from_pandas(transactions_df).lazy()\n",
    "    print(f\"[Prepare LazyFrame] {time.time() - t1:.3f}s\")\n",
    "    \n",
    "    # Ensure datetime conversion\n",
    "    t2 = time.time()\n",
    "    lf = lf.with_columns(\n",
    "        pl.col(\"block_time\").str.to_datetime(strict=False)\n",
    "    )\n",
    "    print(f\"[Datetime Conversion] {time.time() - t2:.3f}s\")\n",
    "    \n",
    "    # Base grouping\n",
    "    t3 = time.time()\n",
    "    base = lf.group_by(\"base_coin\")\n",
    "    print(f\"[Group By base_coin] {time.time() - t3:.3f}s\")\n",
    "    \n",
    "    # 1. Basic counts & direction\n",
    "    t4 = time.time()\n",
    "    counts = base.agg(\n",
    "        pl.len().alias(\"total_txs\"),\n",
    "        pl.n_unique(\"tx_idx\").alias(\"unique_tx_indices\"),\n",
    "        pl.n_unique(\"signing_wallet\").alias(\"unique_wallets\")\n",
    "    )\n",
    "    \n",
    "    raw_dir = base.agg(\n",
    "        pl.when(pl.col(\"direction\") == \"buy\").then(1).otherwise(0).sum().alias(\"buy_count\"),\n",
    "        pl.when(pl.col(\"direction\") == \"sell\").then(1).otherwise(0).sum().alias(\"sell_count\")\n",
    "    )\n",
    "    \n",
    "    dir_stats = (\n",
    "        raw_dir\n",
    "        .join(counts.select([\"base_coin\", \"total_txs\"]), on=\"base_coin\")\n",
    "        .with_columns(\n",
    "            (pl.col(\"buy_count\") / pl.col(\"total_txs\")).alias(\"buy_ratio\")\n",
    "        )\n",
    "        .rename({\"total_txs\": \"total_txs_dir\"})  # Rename to avoid conflict\n",
    "    )\n",
    "    print(f\"[Counts & Direction] {time.time() - t4:.3f}s\")\n",
    "    \n",
    "    # 2. Wallet counts\n",
    "    t5 = time.time()\n",
    "    buy_wallets = (\n",
    "        lf.filter(pl.col(\"direction\") == \"buy\")\n",
    "          .group_by(\"base_coin\").agg(pl.n_unique(\"signing_wallet\").alias(\"unique_buy_wallets\"))\n",
    "    )\n",
    "    sell_wallets = (\n",
    "        lf.filter(pl.col(\"direction\") == \"sell\")\n",
    "          .group_by(\"base_coin\").agg(pl.n_unique(\"signing_wallet\").alias(\"unique_sell_wallets\"))\n",
    "    )\n",
    "    print(f\"[Wallet Counts] {time.time() - t5:.3f}s\")\n",
    "    \n",
    "    # 3. Base/Quote stats\n",
    "    t6 = time.time()\n",
    "    base_stats = base.agg(\n",
    "        pl.col(\"base_coin_amount\").sum().alias(\"base_sum\"),\n",
    "        pl.col(\"base_coin_amount\").mean().alias(\"base_mean\"),\n",
    "        pl.col(\"base_coin_amount\").max().alias(\"base_max\"),\n",
    "        pl.col(\"base_coin_amount\").min().alias(\"base_min\"),\n",
    "        pl.col(\"base_coin_amount\").std().alias(\"base_std\")\n",
    "    )\n",
    "    quote_stats = base.agg(\n",
    "        pl.col(\"quote_coin_amount\").sum().alias(\"quote_sum\"),\n",
    "        pl.col(\"quote_coin_amount\").mean().alias(\"quote_mean\"),\n",
    "        pl.col(\"quote_coin_amount\").max().alias(\"quote_max\"),\n",
    "        pl.col(\"quote_coin_amount\").min().alias(\"quote_min\"),\n",
    "        pl.col(\"quote_coin_amount\").std().alias(\"quote_std\")\n",
    "    )\n",
    "    print(f\"[Base/Quote Stats] {time.time() - t6:.3f}s\")\n",
    "    \n",
    "    # 4. Time-based features\n",
    "    t7 = time.time()\n",
    "    time_stats = (\n",
    "        base.agg(\n",
    "            pl.col(\"block_time\").min().alias(\"first_time\"),\n",
    "            pl.col(\"block_time\").max().alias(\"last_time\")\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"last_time\") - pl.col(\"first_time\")).dt.total_seconds().alias(\"activity_duration_sec\")\n",
    "        )\n",
    "        .join(counts, on=\"base_coin\")\n",
    "        .with_columns(\n",
    "            (pl.col(\"total_txs\") / (pl.col(\"activity_duration_sec\") + 1e-6)).alias(\"tx_per_sec\")\n",
    "        )\n",
    "        .rename({\"total_txs\": \"total_txs_time\"})  # Rename to avoid conflict\n",
    "    )\n",
    "    print(f\"[Time Features] {time.time() - t7:.3f}s\")\n",
    "    \n",
    "    # 5. Slot features\n",
    "    t8 = time.time()\n",
    "    slot_stats = (\n",
    "        base.agg(\n",
    "            pl.col(\"slot\").min().alias(\"first_slot\"),\n",
    "            pl.col(\"slot\").max().alias(\"last_slot\")\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"last_slot\") - pl.col(\"first_slot\")).alias(\"slot_span\")\n",
    "        )\n",
    "    )\n",
    "    early_txs = lf.with_columns(\n",
    "        pl.when(pl.col(\"slot\") <= pl.col(\"slot\").min() + 100)\n",
    "          .then(1).otherwise(0).alias(\"is_early\")\n",
    "    ).group_by(\"base_coin\").agg(pl.col(\"is_early\").sum().alias(\"early_txs\"))\n",
    "    print(f\"[Slot Features] {time.time() - t8:.3f}s\")\n",
    "    \n",
    "    # 6. Balance features\n",
    "    t9 = time.time()\n",
    "    sorted_lf = lf.sort([\"base_coin\", \"slot\"])\n",
    "    balance_feats = (\n",
    "        sorted_lf.group_by(\"base_coin\").agg(\n",
    "            pl.col(\"virtual_token_balance_after\").first().alias(\"first_balance\"),\n",
    "            pl.col(\"virtual_token_balance_after\").last().alias(\"last_balance\"),\n",
    "            pl.col(\"virtual_sol_balance_after\").first().alias(\"first_sol\"),\n",
    "            pl.col(\"virtual_sol_balance_after\").last().alias(\"last_sol\")\n",
    "        )\n",
    "        .with_columns((pl.col(\"last_sol\") - pl.col(\"first_sol\")).alias(\"sol_change\"))\n",
    "    )\n",
    "    balance_vol = base.agg(\n",
    "        pl.col(\"virtual_token_balance_after\").std().alias(\"balance_volatility\"),\n",
    "        pl.col(\"virtual_token_balance_after\").max().alias(\"max_balance\"),\n",
    "        pl.col(\"virtual_token_balance_after\").min().alias(\"min_balance\")\n",
    "    )\n",
    "    print(f\"[Balance Features] {time.time() - t9:.3f}s\")\n",
    "    \n",
    "    # 7. Gas & Liquidity\n",
    "    t10 = time.time()\n",
    "    gas = base.agg(\n",
    "        pl.col(\"provided_gas_fee\").sum().alias(\"total_gas_fees\"),\n",
    "        pl.col(\"provided_gas_fee\").mean().alias(\"avg_gas_fee\"),\n",
    "        pl.col(\"provided_gas_fee\").std().alias(\"std_gas_fee\"),\n",
    "        pl.col(\"provided_gas_fee\").max().alias(\"max_gas_fee\"),\n",
    "        pl.col(\"provided_gas_fee\").min().alias(\"min_gas_fee\"),\n",
    "        pl.col(\"provided_gas_limit\").mean().alias(\"avg_gas_limit\"),\n",
    "        pl.col(\"consumed_gas\").mean().alias(\"avg_consumed_gas\"),\n",
    "        pl.col(\"consumed_gas\").sum().alias(\"total_consumed_gas\"),\n",
    "        pl.col(\"fee\").sum().alias(\"total_fee\"),\n",
    "        ((pl.col(\"provided_gas_limit\") - pl.col(\"consumed_gas\")).mean()).alias(\"limit_utilization\")\n",
    "    )\n",
    "    liq = base.agg(\n",
    "        pl.col(\"virtual_sol_balance_after\").mean().alias(\"liq_virtual_sol_balance_after_mean\"),\n",
    "        pl.col(\"quote_coin_amount\").sum().alias(\"liq_quote_coin_amount\")\n",
    "    )\n",
    "    gas_metrics = gas.join(quote_stats, on=\"base_coin\").with_columns([\n",
    "        (pl.col(\"total_fee\") / (pl.col(\"quote_sum\") + 1e-6)).alias(\"fee_per_sol\"),\n",
    "        pl.col(\"avg_consumed_gas\").alias(\"gas_per_tx\"),\n",
    "        (pl.col(\"quote_sum\") / (pl.col(\"total_consumed_gas\") + 1e-6)).alias(\"gas_efficiency\")\n",
    "    ])\n",
    "    print(f\"[Gas & Liquidity] {time.time() - t10:.3f}s\")\n",
    "    \n",
    "    # 8. Flow Imbalance\n",
    "    t11 = time.time()\n",
    "    flow = base.agg(\n",
    "        pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"buy\").sum().alias(\"buy_quote_amount\"),\n",
    "        pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"sell\").sum().alias(\"sell_quote_amount\")\n",
    "    ).with_columns(\n",
    "        ((pl.col(\"buy_quote_amount\") - pl.col(\"sell_quote_amount\")) /\n",
    "         (pl.col(\"buy_quote_amount\") + pl.col(\"sell_quote_amount\") + 1e-6)).alias(\"flow_imbalance\")\n",
    "    )\n",
    "    print(f\"[Flow Imbalance] {time.time() - t11:.3f}s\")\n",
    "    \n",
    "    # 9. Wallet Concentration\n",
    "    t12 = time.time()\n",
    "    # wallet_conc = (\n",
    "    #     lf.group_by([\"base_coin\", \"signing_wallet\"])\n",
    "    #       .agg(pl.col(\"base_coin_amount\").sum().alias(\"volume\"))\n",
    "    #       .sort([\"base_coin\", \"volume\"])\n",
    "    #       .with_columns([\n",
    "    #           pl.col(\"volume\").cum_sum().over(\"base_coin\").alias(\"cum_vol\"),\n",
    "    #           pl.col(\"volume\").sum().over(\"base_coin\").alias(\"total_vol\"),\n",
    "    #           pl.col(\"volume\").rank(\"ordinal\").over(\"base_coin\").alias(\"rank\")\n",
    "    #       ])\n",
    "    #       .group_by(\"base_coin\").agg([\n",
    "    #           ((pl.len() + 1 - 2 * (pl.col(\"cum_vol\") / pl.col(\"total_vol\")).sum()) / pl.len()).alias(\"gini_coeff\"),\n",
    "    #           (pl.col(\"volume\").top_k(5).sum() / pl.col(\"total_vol\")).alias(\"top5_wallet_share\")\n",
    "    #       ])\n",
    "    # )\n",
    "    wallet_conc = (\n",
    "        lf.group_by([\"base_coin\", \"signing_wallet\"])\n",
    "        .agg(pl.sum(\"base_coin_amount\").alias(\"wallet_volume\"))\n",
    "        .sort([\"base_coin\", \"wallet_volume\"])\n",
    "        .group_by(\"base_coin\")\n",
    "        .agg(\n",
    "            (1 - 2 * (pl.cum_sum(\"wallet_volume\") / pl.sum(\"wallet_volume\")).sum() / pl.len() + 1/pl.len()).alias(\"gini_coeff\"),\n",
    "            (pl.col(\"wallet_volume\").top_k(5).sum() / pl.sum(\"wallet_volume\")).alias(\"top5_wallet_share\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # print(wallet_conc)\n",
    "    print(f\"[Wallet Concentration] {time.time() - t12:.3f}s\")\n",
    "    \n",
    "    # 10. Hour Entropy\n",
    "    t13 = time.time()\n",
    "    hour_ent = (\n",
    "        lf.select([pl.col(\"base_coin\"), pl.col(\"block_time\").dt.hour().alias(\"hour\")])\n",
    "          .group_by([\"base_coin\", \"hour\"]).agg(pl.len().alias(\"cnt\"))\n",
    "          .with_columns((pl.col(\"cnt\") / pl.sum(\"cnt\").over(\"base_coin\")).alias(\"p\"))\n",
    "          .filter(pl.col(\"p\") > 0)\n",
    "          .group_by(\"base_coin\").agg((-(pl.col(\"p\") * pl.col(\"p\").log(2))).sum().alias(\"hour_entropy\"))\n",
    "    )\n",
    "    print(f\"[Hour Entropy] {time.time() - t13:.3f}s\")\n",
    "    \n",
    "    # 11. Price Volatility\n",
    "    t14 = time.time()\n",
    "    price_vol = (\n",
    "        lf.with_columns((pl.col(\"quote_coin_amount\") / pl.col(\"base_coin_amount\")).alias(\"price\"))\n",
    "          .filter(pl.col(\"price\").is_finite())\n",
    "          .group_by(\"base_coin\").agg([\n",
    "              pl.col(\"price\").mean().alias(\"avg_price\"),\n",
    "              pl.col(\"price\").rolling_std(window_size=5, min_periods=1).mean().alias(\"price_vol_mean\"),\n",
    "              pl.col(\"price\").rolling_std(window_size=5, min_periods=1).max().alias(\"price_vol_max\"),\n",
    "              ((pl.col(\"price\").max() - pl.col(\"price\").min()) / (pl.col(\"price\").mean() + 1e-6)).alias(\"price_slippage\")\n",
    "          ])\n",
    "    )\n",
    "    print(f\"[Price Volatility] {time.time() - t14:.3f}s\")\n",
    "    \n",
    "    # 12. Time-Weighted Features\n",
    "    t15 = time.time()\n",
    "    max_slots = lf.group_by(\"base_coin\").agg(pl.col(\"slot\").max().alias(\"max_slot\"))\n",
    "    tw = (\n",
    "        lf.join(max_slots, on=\"base_coin\")\n",
    "          .with_columns((0.1 * (pl.col(\"slot\") - pl.col(\"max_slot\"))).exp().alias(\"w\"))\n",
    "          .group_by(\"base_coin\").agg([\n",
    "              (pl.col(\"base_coin_amount\") * pl.col(\"w\")).sum().alias(\"tw_base_coin_amount\"),\n",
    "              (pl.col(\"quote_coin_amount\") * pl.col(\"w\")).sum().alias(\"tw_quote_coin_amount\")\n",
    "          ])\n",
    "    )\n",
    "    print(f\"[Time-Weighted] {time.time() - t15:.3f}s\")\n",
    "    \n",
    "    # Final join & collect\n",
    "    t16 = time.time()\n",
    "    df = (\n",
    "        counts\n",
    "        .join(dir_stats, on=\"base_coin\", how=\"left\")\n",
    "        .join(buy_wallets, on=\"base_coin\", how=\"left\")\n",
    "        .join(sell_wallets, on=\"base_coin\", how=\"left\")\n",
    "        .join(base_stats, on=\"base_coin\", how=\"left\")\n",
    "        .join(quote_stats, on=\"base_coin\", how=\"left\")\n",
    "        .join(time_stats, on=\"base_coin\", how=\"left\")\n",
    "        .join(slot_stats, on=\"base_coin\", how=\"left\")\n",
    "        .join(early_txs, on=\"base_coin\", how=\"left\")\n",
    "        .join(balance_feats, on=\"base_coin\", how=\"left\")\n",
    "        .join(balance_vol, on=\"base_coin\", how=\"left\")\n",
    "        .join(gas_metrics, on=\"base_coin\", how=\"left\")\n",
    "        .join(liq, on=\"base_coin\", how=\"left\")\n",
    "        .join(flow, on=\"base_coin\", how=\"left\")\n",
    "        .join(wallet_conc, on=\"base_coin\", how=\"left\")\n",
    "        .join(hour_ent, on=\"base_coin\", how=\"left\")\n",
    "        .join(price_vol, on=\"base_coin\", how=\"left\")\n",
    "        .join(tw, on=\"base_coin\", how=\"left\")\n",
    "        .collect()\n",
    "    )\n",
    "    pdf = df.rename({\"base_coin\": \"mint\"}).fill_null(0).to_pandas()\n",
    "    print(f\"[Final Join & Collect] {time.time() - t16:.3f}s\")\n",
    "    print(f\"[Overall Complete] {time.time() - t0:.3f}s, rows={pdf.shape[0]}, cols={pdf.shape[1]}\")\n",
    "    return pdf\n",
    "\n",
    "mint_transactions = fast_polars_process(transactions_df, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72d86112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.095009Z",
     "iopub.status.busy": "2025-07-23T12:04:43.094388Z",
     "iopub.status.idle": "2025-07-23T12:04:43.104064Z",
     "shell.execute_reply": "2025-07-23T12:04:43.102465Z"
    },
    "papermill": {
     "duration": 0.037441,
     "end_time": "2025-07-23T12:04:43.107978",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.070537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mint                                          object\n",
      "total_txs                                     uint32\n",
      "unique_tx_indices                             uint32\n",
      "unique_wallets                                uint32\n",
      "buy_count                                      int32\n",
      "sell_count                                     int32\n",
      "total_txs_dir                                 uint32\n",
      "buy_ratio                                    float64\n",
      "unique_buy_wallets                            uint32\n",
      "unique_sell_wallets                           uint32\n",
      "base_sum                                       int64\n",
      "base_mean                                    float64\n",
      "base_max                                       int64\n",
      "base_min                                       int64\n",
      "base_std                                     float64\n",
      "quote_sum                                      int64\n",
      "quote_mean                                   float64\n",
      "quote_max                                      int64\n",
      "quote_min                                      int64\n",
      "quote_std                                    float64\n",
      "first_time                            datetime64[us]\n",
      "last_time                             datetime64[us]\n",
      "activity_duration_sec                          int64\n",
      "total_txs_time                                uint32\n",
      "unique_tx_indices_right                       uint32\n",
      "unique_wallets_right                          uint32\n",
      "tx_per_sec                                   float64\n",
      "first_slot                                     int64\n",
      "last_slot                                      int64\n",
      "slot_span                                      int64\n",
      "early_txs                                      int32\n",
      "first_balance                                  int64\n",
      "last_balance                                   int64\n",
      "first_sol                                      int64\n",
      "last_sol                                       int64\n",
      "sol_change                                     int64\n",
      "balance_volatility                           float64\n",
      "max_balance                                    int64\n",
      "min_balance                                    int64\n",
      "total_gas_fees                                 int64\n",
      "avg_gas_fee                                  float64\n",
      "std_gas_fee                                  float64\n",
      "max_gas_fee                                    int64\n",
      "min_gas_fee                                    int64\n",
      "avg_gas_limit                                float64\n",
      "avg_consumed_gas                             float64\n",
      "total_consumed_gas                             int64\n",
      "total_fee                                      int64\n",
      "limit_utilization                            float64\n",
      "quote_sum_right                                int64\n",
      "quote_mean_right                             float64\n",
      "quote_max_right                                int64\n",
      "quote_min_right                                int64\n",
      "quote_std_right                              float64\n",
      "fee_per_sol                                  float64\n",
      "gas_per_tx                                   float64\n",
      "gas_efficiency                               float64\n",
      "liq_virtual_sol_balance_after_mean           float64\n",
      "liq_quote_coin_amount                          int64\n",
      "buy_quote_amount                               int64\n",
      "sell_quote_amount                              int64\n",
      "flow_imbalance                               float64\n",
      "gini_coeff                                   float64\n",
      "top5_wallet_share                            float64\n",
      "hour_entropy                                 float64\n",
      "avg_price                                    float64\n",
      "price_vol_mean                               float64\n",
      "price_vol_max                                float64\n",
      "price_slippage                               float64\n",
      "tw_base_coin_amount                          float64\n",
      "tw_quote_coin_amount                         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(mint_transactions.dtypes)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1477b1dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.152692Z",
     "iopub.status.busy": "2025-07-23T12:04:43.152290Z",
     "iopub.status.idle": "2025-07-23T12:04:43.158079Z",
     "shell.execute_reply": "2025-07-23T12:04:43.156503Z"
    },
    "papermill": {
     "duration": 0.030312,
     "end_time": "2025-07-23T12:04:43.160366",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.130054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mint_transactions_3[mint_transactions_3['mint'] == 'DdMHaNkPHYAYM7MxAnLtF7nCdnxz3XdCCkqSjiJSpump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c6af80b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.208794Z",
     "iopub.status.busy": "2025-07-23T12:04:43.208267Z",
     "iopub.status.idle": "2025-07-23T12:04:43.214344Z",
     "shell.execute_reply": "2025-07-23T12:04:43.212437Z"
    },
    "papermill": {
     "duration": 0.03168,
     "end_time": "2025-07-23T12:04:43.216498",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.184818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mint_transactions[mint_transactions['mint'] == 'DdMHaNkPHYAYM7MxAnLtF7nCdnxz3XdCCkqSjiJSpump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9305b219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.258108Z",
     "iopub.status.busy": "2025-07-23T12:04:43.257411Z",
     "iopub.status.idle": "2025-07-23T12:04:43.262509Z",
     "shell.execute_reply": "2025-07-23T12:04:43.261038Z"
    },
    "papermill": {
     "duration": 0.029505,
     "end_time": "2025-07-23T12:04:43.265032",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.235527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# mint_transactions.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c565ba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.313859Z",
     "iopub.status.busy": "2025-07-23T12:04:43.313447Z",
     "iopub.status.idle": "2025-07-23T12:04:43.318122Z",
     "shell.execute_reply": "2025-07-23T12:04:43.316806Z"
    },
    "papermill": {
     "duration": 0.029002,
     "end_time": "2025-07-23T12:04:43.320190",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.291188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# mint_transactions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e9c8efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.365246Z",
     "iopub.status.busy": "2025-07-23T12:04:43.364819Z",
     "iopub.status.idle": "2025-07-23T12:04:43.374798Z",
     "shell.execute_reply": "2025-07-23T12:04:43.373489Z"
    },
    "papermill": {
     "duration": 0.034692,
     "end_time": "2025-07-23T12:04:43.376663",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.341971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import concurrent.futures\n",
    "\n",
    "# def fast_polars_process(transactions_df, n_jobs=4):\n",
    "#     \"\"\"Optimized version of mint processing using Polars operations\"\"\"\n",
    "#     print(f\"Starting Polars transaction processing at {datetime.now()}\")\n",
    "    \n",
    "#     # Convert pandas dataframe to polars if needed\n",
    "#     if not isinstance(transactions_df, pl.DataFrame):\n",
    "#         transactions_df = pl.from_pandas(transactions_df)\n",
    "    \n",
    "#     # Ensure datetime conversion\n",
    "#     if transactions_df[\"block_time\"].dtype != pl.Datetime:\n",
    "#         transactions_df = transactions_df.with_columns(\n",
    "#             pl.col(\"block_time\").str.to_datetime()\n",
    "#         )\n",
    "    \n",
    "#     print(\"Calculating aggregated features...\")\n",
    "    \n",
    "#     # Basic counts (vectorized)\n",
    "#     counts = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.count().alias(\"total_txs\"),\n",
    "#             pl.n_unique(\"tx_idx\").alias(\"unique_tx_indices\"),\n",
    "#             pl.n_unique(\"signing_wallet\").alias(\"unique_wallets\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Direction-based metrics\n",
    "#     direction_counts = (\n",
    "#         transactions_df\n",
    "#         .group_by([\"base_coin\", \"direction\"])\n",
    "#         .agg(pl.count().alias(\"count\"))\n",
    "#         .pivot(index=\"base_coin\", columns=\"direction\", values=\"count\")\n",
    "#         .fill_null(0)\n",
    "#     )\n",
    "    \n",
    "#     # Ensure buy and sell columns exist\n",
    "#     if \"buy\" not in direction_counts.columns:\n",
    "#         direction_counts = direction_counts.with_columns(pl.lit(0).alias(\"buy\"))\n",
    "#     if \"sell\" not in direction_counts.columns:\n",
    "#         direction_counts = direction_counts.with_columns(pl.lit(0).alias(\"sell\"))\n",
    "    \n",
    "#     # Rename columns\n",
    "#     direction_counts = direction_counts.rename({\"buy\": \"buy_count\", \"sell\": \"sell_count\"})\n",
    "    \n",
    "#     # Wallet counts by direction\n",
    "#     buy_wallets = (\n",
    "#         transactions_df\n",
    "#         .filter(pl.col(\"direction\") == \"buy\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_buy_wallets\"))\n",
    "#     )\n",
    "    \n",
    "#     sell_wallets = (\n",
    "#         transactions_df\n",
    "#         .filter(pl.col(\"direction\") == \"sell\")\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(pl.n_unique(\"signing_wallet\").alias(\"unique_sell_wallets\"))\n",
    "#     )\n",
    "    \n",
    "#     # Calculate base and quote statistics\n",
    "#     base_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"base_coin_amount\").sum().alias(\"base_sum\"),\n",
    "#             pl.col(\"base_coin_amount\").mean().alias(\"base_mean\"),\n",
    "#             pl.col(\"base_coin_amount\").max().alias(\"base_max\"),\n",
    "#             pl.col(\"base_coin_amount\").min().alias(\"base_min\"),\n",
    "#             pl.col(\"base_coin_amount\").std().alias(\"base_std\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     quote_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"quote_coin_amount\").sum().alias(\"quote_sum\"),\n",
    "#             pl.col(\"quote_coin_amount\").mean().alias(\"quote_mean\"),\n",
    "#             pl.col(\"quote_coin_amount\").max().alias(\"quote_max\"),\n",
    "#             pl.col(\"quote_coin_amount\").min().alias(\"quote_min\"),\n",
    "#             pl.col(\"quote_coin_amount\").std().alias(\"quote_std\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Time-based features\n",
    "#     time_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"block_time\").min().alias(\"first_time\"),\n",
    "#             pl.col(\"block_time\").max().alias(\"last_time\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_time\") - pl.col(\"first_time\")).dt.total_seconds().alias(\"activity_duration_sec\")\n",
    "#         )\n",
    "#     )\n",
    "#     # Transaction velocity calculation (NEW FEATURE)\n",
    "#     time_stats = time_stats.join(counts.select(\"base_coin\", \"total_txs\"), on=\"base_coin\").with_columns(\n",
    "#         (pl.col(\"total_txs\") / (pl.col(\"activity_duration_sec\") + 1e-6)).alias(\"tx_per_sec\")\n",
    "#     )\n",
    "    \n",
    "#     # Slot features\n",
    "#     slot_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"slot\").min().alias(\"first_slot\"),\n",
    "#             pl.col(\"slot\").max().alias(\"last_slot\")\n",
    "#         )\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"last_slot\") - pl.col(\"first_slot\")).alias(\"slot_span\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Early transactions calculation (within first 100 slots)\n",
    "#     early_txs = transactions_df.group_by(\"base_coin\").agg(\n",
    "#         pl.col(\"tx_idx\")\n",
    "#         .filter(pl.col(\"slot\") <= pl.col(\"slot\").min() + 100)\n",
    "#         .count()\n",
    "#         .alias(\"early_txs\")\n",
    "#     )\n",
    "    \n",
    "#     # Balance features - get first and last records efficiently\n",
    "#     sorted_df = transactions_df.sort([\"base_coin\", \"slot\"])\n",
    "    \n",
    "#     # First values\n",
    "#     first_values = (\n",
    "#         sorted_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").first().alias(\"first_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").first().alias(\"first_sol\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Last values\n",
    "#     last_values = (\n",
    "#         sorted_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").last().alias(\"last_balance\"),\n",
    "#             pl.col(\"virtual_sol_balance_after\").last().alias(\"last_sol\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Combine balance features\n",
    "#     balance_features = first_values.join(last_values, on=\"base_coin\").with_columns(\n",
    "#         (pl.col(\"last_sol\") - pl.col(\"first_sol\")).alias(\"sol_change\")\n",
    "#     )\n",
    "    \n",
    "#     # Balance volatility\n",
    "#     balance_vol = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"virtual_token_balance_after\").std().alias(\"balance_volatility\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").max().alias(\"max_balance\"),\n",
    "#             pl.col(\"virtual_token_balance_after\").min().alias(\"min_balance\")\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # Gas features\n",
    "#     gas_stats = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg(\n",
    "#             pl.col(\"provided_gas_fee\").sum().alias(\"total_gas_fees\"),\n",
    "#             pl.col(\"provided_gas_fee\").mean().alias(\"avg_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").std().alias(\"std_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").max().alias(\"max_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_fee\").min().alias(\"min_gas_fee\"),\n",
    "#             pl.col(\"provided_gas_limit\").mean().alias(\"avg_gas_limit\"),\n",
    "#             pl.col(\"consumed_gas\").mean().alias(\"avg_consumed_gas\"),\n",
    "#             pl.col(\"consumed_gas\").sum().alias(\"total_consumed_gas\"),\n",
    "#             pl.col(\"fee\").sum().alias(\"total_fee\"),\n",
    "#             (pl.col(\"provided_gas_limit\") - pl.col(\"consumed_gas\")).mean().alias(\"limit_utilization\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     liq_stats = (\n",
    "#     transactions_df\n",
    "#       .group_by(\"base_coin\")\n",
    "#       .agg([\n",
    "#         # average SOL‐balance after each transaction (pool depth proxy)\n",
    "#         pl.col(\"virtual_sol_balance_after\")\n",
    "#           .mean()\n",
    "#           .alias(\"liq_virtual_sol_balance_after_mean\"),\n",
    "#         # total quote‐volume (another liquidity proxy)\n",
    "#         pl.col(\"quote_coin_amount\")\n",
    "#           .sum()\n",
    "#           .alias(\"liq_quote_coin_amount\"),\n",
    "#       ])\n",
    "#     )\n",
    "#     # Gas metrics with quote\n",
    "#     gas_metrics = gas_stats.join(quote_stats.select([\"base_coin\", \"quote_sum\"]), on=\"base_coin\").with_columns([\n",
    "#         (pl.col(\"total_fee\") / (pl.col(\"quote_sum\") + 1e-6)).alias(\"fee_per_sol\"),\n",
    "#         pl.col(\"avg_consumed_gas\").alias(\"gas_per_tx\"),\n",
    "#         (pl.col(\"quote_sum\") / (pl.col(\"total_consumed_gas\") + 1e-6)).alias(\"gas_efficiency\")\n",
    "#     ])\n",
    "    \n",
    "#     # Wallet concentration (Gini and top wallet share) - using chunks to avoid memory issues\n",
    "#     def calculate_concentration_metrics(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "#         wallet_volumes = mint_df.group_by(\"signing_wallet\").agg(pl.col(\"base_coin_amount\").sum())\n",
    "        \n",
    "#         if wallet_volumes.height <= 1:\n",
    "#             return {\"base_coin\": mint, \"gini_coeff\": 0.0, \"top5_wallet_share\": 0.0}\n",
    "        \n",
    "#         sorted_values = wallet_volumes.sort(\"base_coin_amount\").to_pandas()[\"base_coin_amount\"].values\n",
    "#         n = len(sorted_values)\n",
    "        \n",
    "#         if n > 1:\n",
    "#             gini_coeff = np.sum((2 * np.arange(1, n+1) - n - 1) * sorted_values) / (n * np.sum(sorted_values) + 1e-6)\n",
    "#         else:\n",
    "#             gini_coeff = 0.0\n",
    "        \n",
    "#         total_volume = sorted_values.sum()\n",
    "#         if total_volume > 0:\n",
    "#             top_k = min(5, n)\n",
    "#             top_k_volume = np.sum(sorted_values[-top_k:])\n",
    "#             top5_wallet_share = top_k_volume / total_volume\n",
    "#         else:\n",
    "#             top5_wallet_share = 0.0\n",
    "            \n",
    "#         return {\"base_coin\": mint, \"gini_coeff\": gini_coeff, \"top5_wallet_share\": top5_wallet_share}\n",
    "    \n",
    "#     print(\"Calculating wallet concentration metrics...\")\n",
    "#     unique_mints = counts[\"base_coin\"].to_list()\n",
    "    \n",
    "#     # Use ThreadPoolExecutor for parallel processing\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         concentration_results = list(executor.map(calculate_concentration_metrics, unique_mints))\n",
    "    \n",
    "#     concentration_df = pl.DataFrame(concentration_results)\n",
    "    \n",
    "#     # Hour distribution entropy\n",
    "#     def calculate_hour_entropy(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "#         hours = mint_df[\"block_time\"].dt.hour()\n",
    "#         hour_counts = hours.value_counts().sort(\"count\", descending=True)\n",
    "        \n",
    "#         if hour_counts.height > 0:\n",
    "#             total = hour_counts[\"count\"].sum()\n",
    "#             probs = hour_counts[\"count\"] / total\n",
    "#             non_zero_probs = probs.filter(probs > 0)\n",
    "            \n",
    "#             if non_zero_probs.len() > 0:\n",
    "#                 entropy = -np.sum(non_zero_probs.to_numpy() * np.log2(non_zero_probs.to_numpy()))\n",
    "#                 return {\"base_coin\": mint, \"hour_entropy\": entropy}\n",
    "        \n",
    "#         return {\"base_coin\": mint, \"hour_entropy\": 0.0}\n",
    "    \n",
    "#     print(\"Calculating hour entropy...\")\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         hour_entropy_results = list(executor.map(calculate_hour_entropy, unique_mints))\n",
    "    \n",
    "#     hour_entropy_df = pl.DataFrame(hour_entropy_results)\n",
    "    \n",
    "#     # Flow imbalance calculation\n",
    "#     flow_imbalance = (\n",
    "#         transactions_df\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg([\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"buy\").sum().alias(\"buy_quote_amount\"),\n",
    "#             pl.col(\"quote_coin_amount\").filter(pl.col(\"direction\") == \"sell\").sum().alias(\"sell_quote_amount\")\n",
    "#         ])\n",
    "#         .with_columns([\n",
    "#             (pl.col(\"buy_quote_amount\") + pl.col(\"sell_quote_amount\")).alias(\"total_quote_amount\"),\n",
    "#             ((pl.col(\"buy_quote_amount\") - pl.col(\"sell_quote_amount\")) / \n",
    "#              (pl.col(\"buy_quote_amount\") + pl.col(\"sell_quote_amount\") + 1e-6)).alias(\"flow_imbalance\")\n",
    "#         ])\n",
    "#         .select([\"base_coin\", \"flow_imbalance\"])\n",
    "#     )\n",
    "    \n",
    "#     # Price and volatility calculations\n",
    "#     def calculate_price_metrics(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "        \n",
    "#         # Calculate price\n",
    "#         mint_df = mint_df.with_columns(\n",
    "#             (pl.col(\"quote_coin_amount\") / pl.col(\"base_coin_amount\")\n",
    "#              .replace(0, None))\n",
    "#             .alias(\"price\")\n",
    "#         )\n",
    "        \n",
    "#         # Replace infinite values with nulls\n",
    "#         mint_df = mint_df.with_columns(\n",
    "#             pl.when(pl.col(\"price\").is_infinite())\n",
    "#             .then(None)\n",
    "#             .otherwise(pl.col(\"price\"))\n",
    "#             .alias(\"price\")\n",
    "#         )\n",
    "        \n",
    "#         avg_price = mint_df[\"price\"].mean()\n",
    "        \n",
    "#         if mint_df.height >= 5:\n",
    "#             # For rolling window calculations, we need to sort and convert to pandas temporarily\n",
    "#             # This is a performance bottleneck, but necessary for the rolling calculation\n",
    "#             sorted_df = mint_df.sort(\"slot\").to_pandas()\n",
    "#             sorted_df['rolling_price_std'] = sorted_df['price'].rolling(window=5, min_periods=1).std().fillna(0)\n",
    "#             price_vol_mean = sorted_df['rolling_price_std'].mean()\n",
    "#             price_vol_max = sorted_df['rolling_price_std'].max()\n",
    "#         else:\n",
    "#             price_vol_mean = mint_df[\"price\"].std() if mint_df.height > 1 else 0\n",
    "#             price_vol_max = price_vol_mean\n",
    "        \n",
    "#         price_min = mint_df[\"price\"].min()\n",
    "#         price_max = mint_df[\"price\"].max()\n",
    "#         price_mean = mint_df[\"price\"].mean()\n",
    "        \n",
    "#         price_slippage = (price_max - price_min) / (price_mean + 1e-6) if price_mean is not None else 0\n",
    "        \n",
    "#         return {\n",
    "#             \"base_coin\": mint,\n",
    "#             \"avg_price\": float(avg_price) if avg_price is not None else 0.0,\n",
    "#             \"price_vol_mean\": float(price_vol_mean) if price_vol_mean is not None else 0.0,\n",
    "#             \"price_vol_max\": float(price_vol_max) if price_vol_max is not None else 0.0,\n",
    "#             \"price_slippage\": float(price_slippage)\n",
    "#         }\n",
    "    \n",
    "#     print(\"Calculating price and volatility metrics...\")\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         price_results = list(executor.map(calculate_price_metrics, unique_mints))\n",
    "    \n",
    "#     price_df = pl.DataFrame(price_results)\n",
    "    \n",
    "#     # Time-weighted features\n",
    "#     def calculate_tw_features(mint):\n",
    "#         mint_df = transactions_df.filter(pl.col(\"base_coin\") == mint)\n",
    "#         max_slot = mint_df[\"slot\"].max()\n",
    "        \n",
    "#         # Calculate time weights using numpy instead of pl.exp\n",
    "#         mint_df = mint_df.with_columns(\n",
    "#             pl.col(\"slot\").map_elements(lambda s: np.exp(-0.1 * (max_slot - s))).alias(\"time_weight\")\n",
    "#         )\n",
    "        \n",
    "#         tw_base = (mint_df[\"base_coin_amount\"] * mint_df[\"time_weight\"]).sum()\n",
    "#         tw_quote = (mint_df[\"quote_coin_amount\"] * mint_df[\"time_weight\"]).sum()\n",
    "        \n",
    "#         return {\n",
    "#             \"base_coin\": mint,\n",
    "#             \"tw_base_coin_amount\": float(tw_base),\n",
    "#             \"tw_quote_coin_amount\": float(tw_quote)\n",
    "#         }\n",
    "    \n",
    "#     print(\"Calculating time-weighted features...\")\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "#         tw_results = list(executor.map(calculate_tw_features, unique_mints))\n",
    "    \n",
    "#     tw_df = pl.DataFrame(tw_results)\n",
    "    \n",
    "#     # Now merge all feature DataFrames\n",
    "#     print(\"Merging all features...\")\n",
    "    \n",
    "#     # Start with counts as the base\n",
    "#     result = counts\n",
    "    \n",
    "#     # Add buy ratio\n",
    "#     result = result.join(direction_counts, on=\"base_coin\", how=\"left\")\n",
    "#     result = result.with_columns(\n",
    "#         (pl.col(\"buy_count\") / pl.col(\"total_txs\")).alias(\"buy_ratio\")\n",
    "#     )\n",
    "    \n",
    "#     # Join remaining dataframes\n",
    "#     dfs_to_join = [\n",
    "#         buy_wallets, sell_wallets, base_stats, quote_stats, \n",
    "#         time_stats.select([\"base_coin\", \"activity_duration_sec\", \"first_time\", \"last_time\", \"tx_per_sec\"]), \n",
    "#         slot_stats, early_txs, balance_features, balance_vol, \n",
    "#         gas_metrics, concentration_df, hour_entropy_df, flow_imbalance, price_df, tw_df, liq_stats\n",
    "#     ]\n",
    "    \n",
    "#     for df in dfs_to_join:\n",
    "#         result = result.join(df, on=\"base_coin\", how=\"left\")\n",
    "    \n",
    "#     # Rename base_coin to mint for consistency with original\n",
    "#     result = result.rename({\"base_coin\": \"mint\"})\n",
    "    \n",
    "#     # Fill null values\n",
    "#     result = result.fill_null(0)\n",
    "    \n",
    "#     print(f\"Polars processing complete with {result.height} rows and {len(result.columns)} columns\")\n",
    "    \n",
    "#     # Convert back to pandas if needed\n",
    "#     return result.to_pandas()\n",
    "\n",
    "# # Usage:\n",
    "# mint_transactions = fast_polars_process(transactions_df.iloc[:100000], n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c545773d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.417591Z",
     "iopub.status.busy": "2025-07-23T12:04:43.417212Z",
     "iopub.status.idle": "2025-07-23T12:04:43.421527Z",
     "shell.execute_reply": "2025-07-23T12:04:43.420279Z"
    },
    "papermill": {
     "duration": 0.027151,
     "end_time": "2025-07-23T12:04:43.423474",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.396323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# print(pl.__version__)  # Should show 0.20.3 or later\n",
    "# print(pl.engine)  # Should show True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d4bb781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.464361Z",
     "iopub.status.busy": "2025-07-23T12:04:43.463976Z",
     "iopub.status.idle": "2025-07-23T12:04:43.470286Z",
     "shell.execute_reply": "2025-07-23T12:04:43.469197Z"
    },
    "papermill": {
     "duration": 0.030243,
     "end_time": "2025-07-23T12:04:43.472485",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.442242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "\n",
    "# def optimized_polars_pipeline(transactions_df):\n",
    "#     # Convert to Polars if needed\n",
    "#     if not isinstance(transactions_df, pl.DataFrame):\n",
    "#         transactions_df = pl.from_pandas(transactions_df)\n",
    "    \n",
    "#     # Ensure datetime conversion\n",
    "#     if transactions_df[\"block_time\"].dtype != pl.Datetime:\n",
    "#         transactions_df = transactions_df.with_columns(\n",
    "#             pl.col(\"block_time\").str.to_datetime()\n",
    "#         )\n",
    "    \n",
    "#     # Main processing pipeline\n",
    "#     return (\n",
    "#         transactions_df.lazy()\n",
    "#         # Pre-sort for efficiency\n",
    "#         .sort([\"base_coin\", \"slot\"])\n",
    "        \n",
    "#         # Base aggregations\n",
    "#         .group_by(\"base_coin\")\n",
    "#         .agg([\n",
    "#             # Basic counts\n",
    "#             pl.count().alias(\"total_txs\"),\n",
    "#             pl.n_unique(\"tx_idx\").alias(\"unique_tx_indices\"),\n",
    "#             pl.n_unique(\"signing_wallet\").alias(\"unique_wallets\"),\n",
    "            \n",
    "#             # Direction metrics\n",
    "#             pl.col(\"direction\").filter(pl.col(\"direction\") == \"buy\").count().alias(\"buy_count\"),\n",
    "#             pl.col(\"direction\").filter(pl.col(\"direction\") == \"sell\").count().alias(\"sell_count\"),\n",
    "#             pl.col(\"signing_wallet\").filter(pl.col(\"direction\") == \"buy\").n_unique().alias(\"unique_buy_wallets\"),\n",
    "#             pl.col(\"signing_wallet\").filter(pl.col(\"direction\") == \"sell\").n_unique().alias(\"unique_sell_wallets\"),\n",
    "            \n",
    "#             # Base/quote statistics\n",
    "#             pl.sum(\"base_coin_amount\").alias(\"base_sum\"),\n",
    "#             pl.mean(\"base_coin_amount\").alias(\"base_mean\"),\n",
    "#             pl.max(\"base_coin_amount\").alias(\"base_max\"),\n",
    "#             pl.min(\"base_coin_amount\").alias(\"base_min\"),\n",
    "#             pl.std(\"base_coin_amount\").alias(\"base_std\"),\n",
    "            \n",
    "#             pl.sum(\"quote_coin_amount\").alias(\"quote_sum\"),\n",
    "#             pl.mean(\"quote_coin_amount\").alias(\"quote_mean\"),\n",
    "#             pl.max(\"quote_coin_amount\").alias(\"quote_max\"),\n",
    "#             pl.min(\"quote_coin_amount\").alias(\"quote_min\"),\n",
    "#             pl.std(\"quote_coin_amount\").alias(\"quote_std\"),\n",
    "            \n",
    "#             # Time features\n",
    "#             pl.min(\"block_time\").alias(\"first_time\"),\n",
    "#             pl.max(\"block_time\").alias(\"last_time\"),\n",
    "#             (pl.max(\"block_time\") - pl.min(\"block_time\")).dt.seconds().alias(\"activity_duration_sec\"),\n",
    "#             pl.min(\"slot\").alias(\"first_slot\"),\n",
    "#             pl.max(\"slot\").alias(\"last_slot\"),\n",
    "#             (pl.max(\"slot\") - pl.min(\"slot\")).alias(\"slot_span\"),\n",
    "            \n",
    "#             # Early transactions\n",
    "#             pl.col(\"tx_idx\").filter(pl.col(\"slot\") <= pl.min(\"slot\") + 100).count().alias(\"early_txs\"),\n",
    "            \n",
    "#             # Balance features\n",
    "#             pl.first(\"virtual_token_balance_after\").alias(\"first_balance\"),\n",
    "#             pl.first(\"virtual_sol_balance_after\").alias(\"first_sol\"),\n",
    "#             pl.last(\"virtual_token_balance_after\").alias(\"last_balance\"),\n",
    "#             pl.last(\"virtual_sol_balance_after\").alias(\"last_sol\"),\n",
    "#             pl.std(\"virtual_token_balance_after\").alias(\"balance_volatility\"),\n",
    "#             pl.max(\"virtual_token_balance_after\").alias(\"max_balance\"),\n",
    "#             pl.min(\"virtual_token_balance_after\").alias(\"min_balance\"),\n",
    "            \n",
    "#             # Gas features\n",
    "#             pl.sum(\"provided_gas_fee\").alias(\"total_gas_fees\"),\n",
    "#             pl.mean(\"provided_gas_fee\").alias(\"avg_gas_fee\"),\n",
    "#             pl.std(\"provided_gas_fee\").alias(\"std_gas_fee\"),\n",
    "#             pl.max(\"provided_gas_fee\").alias(\"max_gas_fee\"),\n",
    "#             pl.min(\"provided_gas_fee\").alias(\"min_gas_fee\"),\n",
    "#             pl.mean(\"provided_gas_limit\").alias(\"avg_gas_limit\"),\n",
    "#             pl.mean(\"consumed_gas\").alias(\"avg_consumed_gas\"),\n",
    "#             pl.sum(\"consumed_gas\").alias(\"total_consumed_gas\"),\n",
    "#             pl.sum(\"fee\").alias(\"total_fee\"),\n",
    "#             (pl.mean(\"provided_gas_limit\") - pl.mean(\"consumed_gas\")).alias(\"limit_utilization\"),\n",
    "            \n",
    "#             # Liquidity proxies\n",
    "#             pl.mean(\"virtual_sol_balance_after\").alias(\"liq_virtual_sol_balance_after_mean\"),\n",
    "#         ])\n",
    "        \n",
    "#         # Add computed features\n",
    "#         .with_columns([\n",
    "#             # Transaction velocity\n",
    "#             (pl.col(\"total_txs\") / (pl.col(\"activity_duration_sec\") + 1e-8)).alias(\"tx_per_sec\"),\n",
    "            \n",
    "#             # SOL change\n",
    "#             (pl.col(\"last_sol\") - pl.col(\"first_sol\")).alias(\"sol_change\"),\n",
    "            \n",
    "#             # Buy ratio\n",
    "#             (pl.col(\"buy_count\") / pl.col(\"total_txs\")).alias(\"buy_ratio\"),\n",
    "            \n",
    "#             # Gas metrics\n",
    "#             (pl.col(\"total_fee\") / (pl.col(\"quote_sum\") + 1e-8)).alias(\"fee_per_sol\"),\n",
    "#             (pl.col(\"quote_sum\") / (pl.col(\"total_consumed_gas\") + 1e-8)).alias(\"gas_efficiency\"),\n",
    "            \n",
    "#             # Flow imbalance\n",
    "#             ((pl.col(\"buy_count\") - pl.col(\"sell_count\")) / (pl.col(\"total_txs\") + 1e-8)).alias(\"flow_imbalance\"),\n",
    "#         ])\n",
    "        \n",
    "#         # Add complex features through window functions\n",
    "#         .with_columns([\n",
    "#             # Time-weighted features\n",
    "#             (pl.col(\"base_coin_amount\") * \n",
    "#              (-0.1 * (pl.max(\"slot\").over(\"base_coin\") - pl.col(\"slot\"))).exp()).sum().over(\"base_coin\").alias(\"tw_base_coin_amount\"),\n",
    "            \n",
    "#             (pl.col(\"quote_coin_amount\") * \n",
    "#              (-0.1 * (pl.max(\"slot\").over(\"base_coin\") - pl.col(\"slot\"))).exp()).sum().over(\"base_coin\").alias(\"tw_quote_coin_amount\"),\n",
    "            \n",
    "#             # Price features\n",
    "#             (pl.col(\"quote_coin_amount\") / pl.col(\"base_coin_amount\"))\n",
    "#             .fill_nan(0)\n",
    "#             .alias(\"price\")\n",
    "#             .pipe(lambda s: s.rolling_std(5).over(\"base_coin\"))\n",
    "#             .mean().over(\"base_coin\").alias(\"price_vol_mean\"),\n",
    "            \n",
    "#             (pl.col(\"price\")\n",
    "#              .rolling_std(5)\n",
    "#              .over(\"base_coin\")\n",
    "#              .max()\n",
    "#              .alias(\"price_vol_max\")),\n",
    "            \n",
    "#             ((pl.col(\"price\").max().over(\"base_coin\") - \n",
    "#               pl.col(\"price\").min().over(\"base_coin\")) / \n",
    "#              (pl.col(\"price\").mean().over(\"base_coin\") + 1e-8)).alias(\"price_slippage\"),\n",
    "#         ])\n",
    "        \n",
    "#         # Add entropy calculation\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"block_time\").dt.hour()\n",
    "#              .value_counts()\n",
    "#              .struct.field(\"count\")\n",
    "#              .entropy()\n",
    "#              .over(\"base_coin\")).alias(\"hour_entropy\")\n",
    "#         )\n",
    "        \n",
    "#         # Add Gini coefficient\n",
    "#         .with_columns(\n",
    "#             (pl.col(\"base_coin_amount\")\n",
    "#              .group_by(\"signing_wallet\")\n",
    "#              .sum()\n",
    "#              .sort()\n",
    "#              .pipe(lambda s: (2 * pl.arange(1, s.len() + 1) - s.len() - 1) * s)\n",
    "#              .sum() / (s.sum() * s.len()))\n",
    "#              .over(\"base_coin\")\n",
    "#              .alias(\"gini_coeff\")\n",
    "#         )\n",
    "        \n",
    "#         # Final cleanup\n",
    "#         .fill_null(0)\n",
    "#         .rename({\"base_coin\": \"mint\"})\n",
    "#         .collect()\n",
    "#         .to_pandas()\n",
    "#     )\n",
    "\n",
    "# # # Usage\n",
    "# transactions_pl = pl.from_pandas(your_pandas_df)\n",
    "# mint_transactions = optimized_polars_pipeline(transactions_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2610e2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:04:43.514289Z",
     "iopub.status.busy": "2025-07-23T12:04:43.513684Z",
     "iopub.status.idle": "2025-07-23T12:05:59.250286Z",
     "shell.execute_reply": "2025-07-23T12:05:59.248953Z"
    },
    "papermill": {
     "duration": 75.758963,
     "end_time": "2025-07-23T12:05:59.252432",
     "exception": false,
     "start_time": "2025-07-23T12:04:43.493469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mint_transactions.to_csv('mint_transactions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eab565d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:05:59.291469Z",
     "iopub.status.busy": "2025-07-23T12:05:59.291074Z",
     "iopub.status.idle": "2025-07-23T12:05:59.297740Z",
     "shell.execute_reply": "2025-07-23T12:05:59.296603Z"
    },
    "papermill": {
     "duration": 0.028111,
     "end_time": "2025-07-23T12:05:59.299511",
     "exception": false,
     "start_time": "2025-07-23T12:05:59.271400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# def fast_vectorized_process(transactions_df, n_jobs=4):\n",
    "#     \"\"\"Vectorized version of mint processing using pandas operations\"\"\"\n",
    "#     print(f\"Starting vectorized transaction processing at {pd.Timestamp.now()}\")\n",
    "    \n",
    "#     # Ensure datetime conversion\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(transactions_df['block_time']):\n",
    "#         transactions_df['block_time'] = pd.to_datetime(transactions_df['block_time'])\n",
    "    \n",
    "#     # Process all mints at once with groupby operations\n",
    "#     print(\"Calculating aggregated features...\")\n",
    "    \n",
    "#     # Group by mint (base_coin) for bulk processing\n",
    "#     grouped = transactions_df.groupby('base_coin')\n",
    "    \n",
    "#     # Basic counts (vectorized)\n",
    "#     counts = grouped.size().reset_index(name='total_txs')\n",
    "    \n",
    "#     # Unique counts\n",
    "#     unique_tx = grouped['tx_idx'].nunique().reset_index(name='unique_tx_indices')\n",
    "#     unique_wallets = grouped['signing_wallet'].nunique().reset_index(name='unique_wallets')\n",
    "    \n",
    "#     # Direction-based metrics (vectorized)\n",
    "#     direction_counts = transactions_df.groupby(['base_coin', 'direction']).size().unstack(fill_value=0)\n",
    "#     direction_counts.columns = [f'{col}_count' for col in direction_counts.columns]\n",
    "    \n",
    "#     # Ensure buy and sell columns exist\n",
    "#     if 'buy_count' not in direction_counts.columns:\n",
    "#         direction_counts['buy_count'] = 0\n",
    "#     if 'sell_count' not in direction_counts.columns:\n",
    "#         direction_counts['sell_count'] = 0\n",
    "    \n",
    "#     # Wallet counts by direction\n",
    "#     buy_wallets = transactions_df[transactions_df['direction'] == 'buy'].groupby('base_coin')['signing_wallet'].nunique()\n",
    "#     sell_wallets = transactions_df[transactions_df['direction'] == 'sell'].groupby('base_coin')['signing_wallet'].nunique()\n",
    "    \n",
    "#     # Calculate base and quote statistics in one pass\n",
    "#     base_stats = grouped['base_coin_amount'].agg(['sum', 'mean', 'max', 'min', 'std']).reset_index()\n",
    "#     base_stats.columns = ['base_coin', 'base_sum', 'base_mean', 'base_max', 'base_min', 'base_std']\n",
    "    \n",
    "#     quote_stats = grouped['quote_coin_amount'].agg(['sum', 'mean', 'max', 'min', 'std']).reset_index()\n",
    "#     quote_stats.columns = ['base_coin', 'quote_sum', 'quote_mean', 'quote_max', 'quote_min', 'quote_std']\n",
    "    \n",
    "#     # Time-based features\n",
    "#     time_stats = grouped['block_time'].agg(['min', 'max']).reset_index()\n",
    "#     time_stats.columns = ['base_coin', 'first_time', 'last_time']\n",
    "#     time_stats['activity_duration_sec'] = (time_stats['last_time'] - time_stats['first_time']).dt.total_seconds()\n",
    "    \n",
    "#     # Slot features\n",
    "#     slot_stats = grouped['slot'].agg(['min', 'max']).reset_index()\n",
    "#     slot_stats.columns = ['base_coin', 'first_slot', 'last_slot'] \n",
    "#     slot_stats['slot_span'] = slot_stats['last_slot'] - slot_stats['first_slot']\n",
    "    \n",
    "#     # Calculate tx_per_sec safely\n",
    "#     counts_with_duration = counts.merge(time_stats[['base_coin', 'activity_duration_sec']], on='base_coin')\n",
    "#     counts_with_duration['tx_per_sec'] = counts_with_duration['total_txs'] / (counts_with_duration['activity_duration_sec'] + 1e-6)\n",
    "    \n",
    "#     # Early transactions calculation (within first 100 slots)\n",
    "#     def count_early_txs(group):\n",
    "#         min_slot = group['slot'].min()\n",
    "#         return group[group['slot'] <= min_slot + 100]['tx_idx'].count()\n",
    "    \n",
    "#     early_txs = grouped.apply(count_early_txs).reset_index(name='early_txs')\n",
    "    \n",
    "#     # Balance features - get first and last records efficiently\n",
    "#     transactions_df = transactions_df.sort_values(['base_coin', 'slot'])\n",
    "#     first_idx = transactions_df.groupby('base_coin')['slot'].idxmin()\n",
    "#     last_idx = transactions_df.groupby('base_coin')['slot'].idxmax()\n",
    "#     first_records = transactions_df.loc[first_idx, ['base_coin', 'virtual_token_balance_after', 'virtual_sol_balance_after']]\n",
    "#     last_records = transactions_df.loc[last_idx, ['base_coin', 'virtual_token_balance_after', 'virtual_sol_balance_after']]\n",
    "#     first_records.columns = ['base_coin', 'first_balance', 'first_sol']\n",
    "#     last_records.columns = ['base_coin', 'last_balance', 'last_sol']\n",
    "#     balance_features = first_records.merge(last_records, on='base_coin')\n",
    "#     balance_features['sol_change'] = balance_features['last_sol'] - balance_features['first_sol']\n",
    "    \n",
    "#     # Balance volatility\n",
    "#     balance_vol = grouped['virtual_token_balance_after'].agg(['std', 'max', 'min']).reset_index()\n",
    "#     balance_vol.columns = ['base_coin', 'balance_volatility', 'max_balance', 'min_balance']\n",
    "    \n",
    "#     # Gas features\n",
    "#     gas_stats = grouped['provided_gas_fee'].agg(['sum', 'mean', 'std', 'max', 'min']).reset_index()\n",
    "#     gas_stats.columns = ['base_coin', 'total_gas_fees', 'avg_gas_fee', 'std_gas_fee', 'max_gas_fee', 'min_gas_fee']\n",
    "    \n",
    "#     # Gas metrics\n",
    "#     gas_metrics = pd.DataFrame({\n",
    "#         'base_coin': grouped.first().index,\n",
    "#         'avg_gas_limit': grouped['provided_gas_limit'].mean(),\n",
    "#         'avg_consumed_gas': grouped['consumed_gas'].mean(),\n",
    "#         'total_consumed_gas': grouped['consumed_gas'].sum(),\n",
    "#         'total_fee': grouped['fee'].sum()\n",
    "#     }).reset_index(drop=True)\n",
    "#     gas_metrics['limit_utilization'] = grouped.apply(lambda x: (x['provided_gas_limit'] - x['consumed_gas']).mean()).reset_index(name='limit_utilization')['limit_utilization']\n",
    "#     gas_with_quote = gas_metrics.merge(quote_stats[['base_coin', 'quote_sum']], on='base_coin')\n",
    "#     gas_with_quote['fee_per_sol'] = gas_with_quote['total_fee'] / (gas_with_quote['quote_sum'] + 1e-6)\n",
    "#     gas_with_quote['gas_per_tx'] = gas_with_quote['avg_consumed_gas']\n",
    "#     gas_with_quote['gas_efficiency'] = gas_with_quote['quote_sum'] / (gas_with_quote['total_consumed_gas'] + 1e-6)\n",
    "    \n",
    "#     # Wallet concentration (Gini and top wallet share)\n",
    "#     def calculate_concentration(group):\n",
    "#         wallet_volumes = group.groupby('signing_wallet')['base_coin_amount'].sum()\n",
    "#         if len(wallet_volumes) > 1:\n",
    "#             sorted_volumes = np.sort(wallet_volumes.values)\n",
    "#             n = len(sorted_volumes)\n",
    "#             gini_coeff = (np.sum((2 * np.arange(1, n+1) - n - 1) * sorted_volumes) / (n * np.sum(sorted_volumes) + 1e-6))\n",
    "#         else:\n",
    "#             gini_coeff = 0\n",
    "#         if len(wallet_volumes) > 0 and wallet_volumes.sum() > 0:\n",
    "#             top5_volumes = wallet_volumes.nlargest(min(5, len(wallet_volumes)))\n",
    "#             top5_wallet_share = top5_volumes.sum() / wallet_volumes.sum()\n",
    "#         else:\n",
    "#             top5_wallet_share = 0\n",
    "#         return pd.Series({'gini_coeff': gini_coeff, 'top5_wallet_share': top5_wallet_share})\n",
    "#     print(\"Calculating wallet concentration metrics...\")\n",
    "#     concentration_metrics = Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(lambda m: (m, calculate_concentration(transactions_df[transactions_df['base_coin'] == m])))(mint)\n",
    "#         for mint in counts['base_coin'].unique()\n",
    "#     )\n",
    "#     concentration_df = pd.DataFrame([{'base_coin': mint, **metrics} for mint, metrics in concentration_metrics])\n",
    "    \n",
    "#     # Hour distribution entropy\n",
    "#     def calculate_hour_entropy(group):\n",
    "#         hour_counts = group['block_time'].dt.hour.value_counts()\n",
    "#         hour_dist = hour_counts / hour_counts.sum() if hour_counts.sum() > 0 else hour_counts\n",
    "#         non_zero_probs = hour_dist[hour_dist > 0]\n",
    "#         return -np.sum(non_zero_probs * np.log2(non_zero_probs)) if len(non_zero_probs) > 0 else 0\n",
    "#     hour_entropy = grouped.apply(calculate_hour_entropy).reset_index(name='hour_entropy')\n",
    "    \n",
    "#     # Flow imbalance calculation\n",
    "#     def calculate_flow_imbalance(group):\n",
    "#         buy_quote_amount = group[group['direction'] == 'buy']['quote_coin_amount'].sum()\n",
    "#         sell_quote_amount = group[group['direction'] == 'sell']['quote_coin_amount'].sum()\n",
    "#         total_quote_amount = buy_quote_amount + sell_quote_amount\n",
    "#         return (buy_quote_amount - sell_quote_amount) / (total_quote_amount + 1e-6)\n",
    "#     flow_imbalance = grouped.apply(calculate_flow_imbalance).reset_index(name='flow_imbalance')\n",
    "    \n",
    "#     # Price and volatility calculations\n",
    "#     def calculate_price_metrics(group):\n",
    "#         group = group.copy()\n",
    "#         group['price'] = group['quote_coin_amount'] / group['base_coin_amount'].replace(0, np.nan)\n",
    "#         group['price'] = group['price'].replace([np.inf, -np.inf], np.nan)\n",
    "#         avg_price = group['price'].mean()\n",
    "#         if len(group) >= 5:\n",
    "#             group['rolling_price_std'] = group['price'].rolling(window=5, min_periods=1).std().fillna(0)\n",
    "#             price_vol_mean = group['rolling_price_std'].mean()\n",
    "#             price_vol_max = group['rolling_price_std'].max()\n",
    "#         else:\n",
    "#             price_vol_mean = group['price'].std()\n",
    "#             price_vol_max = price_vol_mean\n",
    "#         price_slippage = (group['price'].max() - group['price'].min()) / (group['price'].mean() + 1e-6)\n",
    "#         return pd.Series({'avg_price': avg_price, 'price_vol_mean': price_vol_mean, 'price_vol_max': price_vol_max, 'price_slippage': price_slippage})\n",
    "#     print(\"Calculating price and volatility metrics...\")\n",
    "#     price_metrics = Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(lambda m: (m, calculate_price_metrics(transactions_df[transactions_df['base_coin'] == m])))(mint)\n",
    "#         for mint in counts['base_coin'].unique()\n",
    "#     )\n",
    "#     price_df = pd.DataFrame([{'base_coin': mint, **metrics} for mint, metrics in price_metrics])\n",
    "    \n",
    "#     # Time-weighted features\n",
    "#     def calculate_tw_features(group):\n",
    "#         max_slot = group['slot'].max()\n",
    "#         tw_base = (group['base_coin_amount'] * np.exp(-0.1 * (max_slot - group['slot']))).sum()\n",
    "#         tw_quote = (group['quote_coin_amount'] * np.exp(-0.1 * (max_slot - group['slot']))).sum()\n",
    "#         return pd.Series({'tw_base_coin_amount': tw_base, 'tw_quote_coin_amount': tw_quote})\n",
    "#     print(\"Calculating time-weighted features...\")\n",
    "#     tw_features = Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(lambda m: (m, calculate_tw_features(transactions_df[transactions_df['base_coin'] == m])))(mint)\n",
    "#         for mint in counts['base_coin'].unique()\n",
    "#     )\n",
    "#     tw_df = pd.DataFrame([{'base_coin': mint, **metrics} for mint, metrics in tw_features])\n",
    "    \n",
    "#     # Now merge all feature DataFrames\n",
    "#     print(\"Merging all features...\")\n",
    "#     result = counts.merge(unique_tx, on='base_coin')\n",
    "#     result = result.merge(unique_wallets, on='base_coin')\n",
    "#     result = result.merge(direction_counts.reset_index(), on='base_coin')\n",
    "#     result['buy_ratio'] = result['buy_count'] / result['total_txs']\n",
    "#     result['unique_buy_wallets'] = result['base_coin'].map(buy_wallets).fillna(0)\n",
    "#     result['unique_sell_wallets'] = result['base_coin'].map(sell_wallets).fillna(0)\n",
    "#     dfs_to_merge = [base_stats, quote_stats, time_stats[['base_coin', 'activity_duration_sec']], slot_stats, early_txs, balance_features, balance_vol, gas_stats, gas_with_quote, concentration_df, hour_entropy, flow_imbalance, price_df, tw_df]\n",
    "#     for df in dfs_to_merge:\n",
    "#         result = result.merge(df, on='base_coin', how='left')\n",
    "#     result = result.rename(columns={'base_coin': 'mint'})\n",
    "#     print(f\"Vectorized processing complete with {len(result)} rows and {len(result.columns)} columns\")\n",
    "#     return result\n",
    "\n",
    "\n",
    "# mint_transactions = fast_vectorized_process(transactions_df, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e32cbf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:05:59.339642Z",
     "iopub.status.busy": "2025-07-23T12:05:59.339290Z",
     "iopub.status.idle": "2025-07-23T12:05:59.347571Z",
     "shell.execute_reply": "2025-07-23T12:05:59.346142Z"
    },
    "papermill": {
     "duration": 0.030228,
     "end_time": "2025-07-23T12:05:59.349466",
     "exception": false,
     "start_time": "2025-07-23T12:05:59.319238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from joblib import Parallel, delayed\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def process_mint_chunk(chunk_data, chunk_id):\n",
    "#     \"\"\"Process a chunk of transaction data to extract features for each mint.\"\"\"\n",
    "#     chunk_start = time.time()\n",
    "#     unique_mints = chunk_data['base_coin'].unique()\n",
    "#     print(f\"[Chunk {chunk_id}] Processing {len(unique_mints)} unique mints...\")\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for mint in unique_mints:\n",
    "#         # Get transactions for this mint\n",
    "#         mint_txs = chunk_data[chunk_data['base_coin'] == mint]\n",
    "        \n",
    "#         # Skip if too few transactions\n",
    "#         if len(mint_txs) < 3:\n",
    "#             continue\n",
    "        \n",
    "#         # Calculate features for this mint\n",
    "#         try:\n",
    "#             features = calculate_mint_features(mint_txs, mint)\n",
    "#             results.append(features)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[Chunk {chunk_id}] Error processing mint {mint}: {str(e)}\")\n",
    "    \n",
    "#     chunk_duration = time.time() - chunk_start\n",
    "#     print(f\"[Chunk {chunk_id}] Completed in {chunk_duration:.2f} seconds, processed {len(results)}/{len(unique_mints)} mints\")\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# def calculate_mint_features(mint_txs, mint_id):\n",
    "#     \"\"\"Extract all features for a single mint.\"\"\"\n",
    "#     # Basic counts\n",
    "#     total_txs = len(mint_txs)\n",
    "#     unique_tx_indices = mint_txs['tx_idx'].nunique()\n",
    "#     unique_wallets = mint_txs['signing_wallet'].nunique()\n",
    "    \n",
    "#     # Buy/sell analysis\n",
    "#     buy_txs = mint_txs[mint_txs['direction'] == 'buy']\n",
    "#     sell_txs = mint_txs[mint_txs['direction'] == 'sell']\n",
    "#     buy_ratio = len(buy_txs) / total_txs if total_txs > 0 else 0\n",
    "#     unique_buy_wallets = buy_txs['signing_wallet'].nunique()\n",
    "#     unique_sell_wallets = sell_txs['signing_wallet'].nunique()\n",
    "    \n",
    "#     # Base/quote amounts\n",
    "#     base_sum = mint_txs['base_coin_amount'].sum()\n",
    "#     base_mean = mint_txs['base_coin_amount'].mean()\n",
    "#     base_max = mint_txs['base_coin_amount'].max()\n",
    "#     base_min = mint_txs['base_coin_amount'].min()\n",
    "#     base_std = mint_txs['base_coin_amount'].std()\n",
    "    \n",
    "#     quote_sum = mint_txs['quote_coin_amount'].sum()\n",
    "#     quote_mean = mint_txs['quote_coin_amount'].mean()\n",
    "#     quote_max = mint_txs['quote_coin_amount'].max()\n",
    "#     quote_min = mint_txs['quote_coin_amount'].min()\n",
    "#     quote_std = mint_txs['quote_coin_amount'].std()\n",
    "    \n",
    "#     # Time features\n",
    "#     first_time = mint_txs['block_time'].min()\n",
    "#     last_time = mint_txs['block_time'].max()\n",
    "#     activity_duration_sec = (last_time - first_time).total_seconds()\n",
    "    \n",
    "#     first_slot = mint_txs['slot'].min()\n",
    "#     last_slot = mint_txs['slot'].max()\n",
    "#     slot_span = last_slot - first_slot\n",
    "    \n",
    "#     tx_per_sec = total_txs / (activity_duration_sec + 1e-6)\n",
    "#     early_txs = mint_txs[mint_txs['slot'] <= first_slot + 100]['tx_idx'].count()\n",
    "    \n",
    "#     # Balance features\n",
    "#     first_balance = mint_txs.iloc[0]['virtual_token_balance_after']\n",
    "#     first_sol = mint_txs.iloc[0]['virtual_sol_balance_after']\n",
    "#     last_balance = mint_txs.iloc[-1]['virtual_token_balance_after']\n",
    "#     last_sol = mint_txs.iloc[-1]['virtual_sol_balance_after']\n",
    "    \n",
    "#     sol_change = last_sol - first_sol\n",
    "#     balance_volatility = mint_txs['virtual_token_balance_after'].std()\n",
    "#     max_balance = mint_txs['virtual_token_balance_after'].max()\n",
    "#     min_balance = mint_txs['virtual_token_balance_after'].min()\n",
    "    \n",
    "#     # Gas features\n",
    "#     total_gas_fees = mint_txs['provided_gas_fee'].sum()\n",
    "#     avg_gas_fee = mint_txs['provided_gas_fee'].mean()\n",
    "#     std_gas_fee = mint_txs['provided_gas_fee'].std()\n",
    "#     max_gas_fee = mint_txs['provided_gas_fee'].max()\n",
    "#     min_gas_fee = mint_txs['provided_gas_fee'].min()\n",
    "    \n",
    "#     avg_gas_limit = mint_txs['provided_gas_limit'].mean()\n",
    "#     avg_consumed_gas = mint_txs['consumed_gas'].mean()\n",
    "#     total_consumed_gas = mint_txs['consumed_gas'].sum()\n",
    "#     total_fee = mint_txs['fee'].sum()\n",
    "    \n",
    "#     limit_utilization = (mint_txs['provided_gas_limit'] - mint_txs['consumed_gas']).mean()\n",
    "#     fee_per_sol = total_fee / (quote_sum + 1e-6)\n",
    "#     gas_per_tx = avg_consumed_gas\n",
    "#     gas_efficiency = quote_sum / (total_consumed_gas + 1e-6)\n",
    "    \n",
    "#     # Wallet concentration\n",
    "#     wallet_volumes = mint_txs.groupby('signing_wallet')['base_coin_amount'].sum()\n",
    "    \n",
    "#     # Gini coefficient\n",
    "#     if len(wallet_volumes) > 1:\n",
    "#         sorted_volumes = np.sort(wallet_volumes.values)\n",
    "#         n = len(sorted_volumes)\n",
    "#         gini_coeff = (np.sum((2 * np.arange(1, n+1) - n - 1) * sorted_volumes) / \n",
    "#                      (n * np.sum(sorted_volumes) + 1e-6))\n",
    "#     else:\n",
    "#         gini_coeff = 0\n",
    "    \n",
    "#     # Top wallet share\n",
    "#     if len(wallet_volumes) > 0 and wallet_volumes.sum() > 0:\n",
    "#         top5_volumes = wallet_volumes.nlargest(min(5, len(wallet_volumes)))\n",
    "#         top5_wallet_share = top5_volumes.sum() / wallet_volumes.sum()\n",
    "#     else:\n",
    "#         top5_wallet_share = 0\n",
    "    \n",
    "#     # Flow imbalance\n",
    "#     buy_quote_amount = buy_txs['quote_coin_amount'].sum()\n",
    "#     sell_quote_amount = sell_txs['quote_coin_amount'].sum()\n",
    "#     total_quote_amount = buy_quote_amount + sell_quote_amount\n",
    "#     flow_imbalance = (buy_quote_amount - sell_quote_amount) / (total_quote_amount + 1e-6)\n",
    "    \n",
    "#     # Price and volatility\n",
    "#     mint_txs['price'] = mint_txs['quote_coin_amount'] / mint_txs['base_coin_amount'].replace(0, np.nan)\n",
    "#     mint_txs['price'] = mint_txs['price'].replace([np.inf, -np.inf], np.nan)\n",
    "#     avg_price = mint_txs['price'].mean()\n",
    "    \n",
    "#     # Price volatility\n",
    "#     if len(mint_txs) >= 5:\n",
    "#         mint_txs['rolling_price_std'] = mint_txs['price'].rolling(window=5, min_periods=1).std().fillna(0)\n",
    "#         price_vol_mean = mint_txs['rolling_price_std'].mean()\n",
    "#         price_vol_max = mint_txs['rolling_price_std'].max()\n",
    "#     else:\n",
    "#         price_vol_mean = mint_txs['price'].std()\n",
    "#         price_vol_max = price_vol_mean\n",
    "    \n",
    "#     # Slippage as liquidity indicator\n",
    "#     price_slippage = (mint_txs['price'].max() - mint_txs['price'].min()) / (mint_txs['price'].mean() + 1e-6)\n",
    "    \n",
    "#     # Hour distribution\n",
    "#     mint_txs['tx_hour'] = mint_txs['block_time'].dt.hour\n",
    "#     hour_counts = mint_txs.groupby('tx_hour')['tx_idx'].count().values\n",
    "#     hour_dist = hour_counts / hour_counts.sum() if hour_counts.sum() > 0 else np.zeros(len(hour_counts))\n",
    "    \n",
    "#     # Shannon entropy for hour distribution\n",
    "#     non_zero_probs = hour_dist[hour_dist > 0]\n",
    "#     hour_entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs)) if len(non_zero_probs) > 0 else 0\n",
    "    \n",
    "#     # Time-weighted features using exponential decay\n",
    "#     max_slot = mint_txs['slot'].max()\n",
    "#     tw_base = (mint_txs['base_coin_amount'] * np.exp(-0.1 * (max_slot - mint_txs['slot']))).sum()\n",
    "#     tw_quote = (mint_txs['quote_coin_amount'] * np.exp(-0.1 * (max_slot - mint_txs['slot']))).sum()\n",
    "    \n",
    "#     return {\n",
    "#         'mint': mint_id,\n",
    "#         'total_txs': total_txs,\n",
    "#         'unique_tx_indices': unique_tx_indices,\n",
    "#         'unique_wallets': unique_wallets,\n",
    "#         'unique_buy_wallets': unique_buy_wallets,\n",
    "#         'unique_sell_wallets': unique_sell_wallets,\n",
    "#         'buy_ratio': buy_ratio,\n",
    "#         'total_base': base_sum,\n",
    "#         'mean_base': base_mean,\n",
    "#         'max_base': base_max,\n",
    "#         'min_base': base_min,\n",
    "#         'std_base': base_std,\n",
    "#         'total_quote': quote_sum,\n",
    "#         'mean_quote': quote_mean,\n",
    "#         'max_quote': quote_max,\n",
    "#         'min_quote': quote_min,\n",
    "#         'std_quote': quote_std,\n",
    "#         'avg_price': avg_price,\n",
    "#         'activity_duration_sec': activity_duration_sec,\n",
    "#         'first_slot': first_slot,\n",
    "#         'last_slot': last_slot,\n",
    "#         'slot_span': slot_span,\n",
    "#         'tx_per_sec': tx_per_sec,\n",
    "#         'early_txs': early_txs,\n",
    "#         'initial_balance': first_balance,\n",
    "#         'initial_sol': first_sol,\n",
    "#         'final_balance': last_balance,\n",
    "#         'final_sol': last_sol,\n",
    "#         'sol_change': sol_change,\n",
    "#         'balance_volatility': balance_volatility,\n",
    "#         'max_balance': max_balance,\n",
    "#         'min_balance': min_balance,\n",
    "#         'total_gas_fees': total_gas_fees,\n",
    "#         'avg_gas_fee': avg_gas_fee,\n",
    "#         'std_gas_fee': std_gas_fee,\n",
    "#         'max_gas_fee': max_gas_fee,\n",
    "#         'min_gas_fee': min_gas_fee,\n",
    "#         'limit_utilization': limit_utilization,\n",
    "#         'fee_per_sol': fee_per_sol,\n",
    "#         'gas_per_tx': gas_per_tx,\n",
    "#         'gas_efficiency': gas_efficiency,\n",
    "#         'gini_coeff': gini_coeff,\n",
    "#         'top5_wallet_share': top5_wallet_share,\n",
    "#         'hour_entropy': hour_entropy,\n",
    "#         'flow_imbalance': flow_imbalance,\n",
    "#         'price_vol_mean': price_vol_mean,\n",
    "#         'price_vol_max': price_vol_max,\n",
    "#         'price_slippage': price_slippage,\n",
    "#         'tw_base_coin_amount': tw_base,\n",
    "#         'tw_quote_coin_amount': tw_quote\n",
    "#     }\n",
    "\n",
    "# def fast_process_mint(transactions_df, n_jobs=4, chunk_size=None):\n",
    "#     \"\"\"Process mint transactions in parallel chunks.\"\"\"\n",
    "#     total_start_time = time.time()\n",
    "#     print(f\"Starting transaction processing at {pd.Timestamp.now()}\")\n",
    "#     print(f\"Total transactions: {len(transactions_df)}\")\n",
    "    \n",
    "#     # Ensure datetime conversion is done\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(transactions_df['block_time']):\n",
    "#         print(\"Converting block_time to datetime...\")\n",
    "#         transactions_df['block_time'] = pd.to_datetime(transactions_df['block_time'])\n",
    "#         print(\"Datetime conversion complete\")\n",
    "    \n",
    "#     # Sort by base_coin and slot for more efficient processing\n",
    "#     print(\"Sorting transactions by base_coin and slot...\")\n",
    "#     sort_start = time.time()\n",
    "#     transactions_df = transactions_df.sort_values(['base_coin', 'slot']).reset_index(drop=True)\n",
    "#     print(f\"Sorting complete in {time.time() - sort_start:.2f} seconds\")\n",
    "    \n",
    "#     # Count unique mints\n",
    "#     unique_mints = transactions_df['base_coin'].unique()\n",
    "#     print(f\"Found {len(unique_mints)} unique mints to process\")\n",
    "    \n",
    "#     # Determine chunk size if not provided (aim for n_jobs * 5 chunks)\n",
    "#     if chunk_size is None:\n",
    "#         chunk_size = max(1, len(unique_mints) // (n_jobs * 5))\n",
    "#     print(f\"Using chunk size: {chunk_size} mints per chunk\")\n",
    "    \n",
    "#     # Create chunks based on unique mints to avoid splitting transactions for the same mint\n",
    "#     print(\"Creating processing chunks...\")\n",
    "#     chunk_start = time.time()\n",
    "    \n",
    "#     # Group by base_coin\n",
    "#     print(\"Grouping by base_coin...\")\n",
    "#     grouped = transactions_df.groupby('base_coin')\n",
    "    \n",
    "#     # Create chunks of mints\n",
    "#     chunks = []\n",
    "#     current_chunk = []\n",
    "#     current_chunk_size = 0\n",
    "    \n",
    "#     for mint in unique_mints:\n",
    "#         current_chunk.append(mint)\n",
    "#         current_chunk_size += 1\n",
    "        \n",
    "#         if current_chunk_size >= chunk_size:\n",
    "#             chunks.append(current_chunk)\n",
    "#             current_chunk = []\n",
    "#             current_chunk_size = 0\n",
    "    \n",
    "#     # Add remaining mints as a final chunk\n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk)\n",
    "    \n",
    "#     print(f\"Created {len(chunks)} chunks in {time.time() - chunk_start:.2f} seconds\")\n",
    "    \n",
    "#     # Process chunks in parallel\n",
    "#     print(f\"Starting parallel processing with {n_jobs} jobs\")\n",
    "    \n",
    "#     # Create a sequential version to process each chunk\n",
    "#     results = []\n",
    "#     # for chunk_id, chunk_mints in enumerate(chunks):\n",
    "#     #     print(f\"Processing chunk {chunk_id+1}/{len(chunks)} with {len(chunk_mints)} mints...\")\n",
    "#     #     chunk_start = time.time()\n",
    "        \n",
    "#     #     # Extract data for this chunk\n",
    "#     #     print(f\"[Chunk {chunk_id+1}] Extracting transaction data...\")\n",
    "#     #     extract_start = time.time()\n",
    "#     #     chunk_data = pd.concat([grouped.get_group(mint) for mint in chunk_mints])\n",
    "#     #     print(f\"[Chunk {chunk_id+1}] Data extraction complete in {time.time() - extract_start:.2f} seconds\")\n",
    "        \n",
    "#     #     # Process the chunk\n",
    "#     #     print(f\"[Chunk {chunk_id+1}] Processing features...\")\n",
    "#     #     process_start = time.time()\n",
    "#     #     chunk_result = process_mint_chunk(chunk_data, chunk_id+1)\n",
    "#     #     print(f\"[Chunk {chunk_id+1}] Feature processing complete in {time.time() - process_start:.2f} seconds\")\n",
    "        \n",
    "#     #     results.append(chunk_result)\n",
    "#     #     print(f\"[Chunk {chunk_id+1}] Complete in {time.time() - chunk_start:.2f} seconds, {len(chunk_result)} mints processed\")\n",
    "    \n",
    "    \n",
    "#     # Parallel version - uncomment to use joblib for parallel processing\n",
    "#     print(\"Preparing chunk data for parallel processing...\")\n",
    "#     chunk_datasets = []\n",
    "#     for chunk_id, chunk_mints in enumerate(chunks):\n",
    "#         print(f\"Preparing data for chunk {chunk_id+1}/{len(chunks)}...\")\n",
    "#         chunk_data = pd.concat([grouped.get_group(mint) for mint in chunk_mints])\n",
    "#         chunk_datasets.append((chunk_data, chunk_id+1))\n",
    "    \n",
    "#     print(f\"Starting parallel processing with {n_jobs} jobs...\")\n",
    "#     results = Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(process_mint_chunk)(chunk_data, chunk_id)\n",
    "#         for chunk_data, chunk_id in chunk_datasets\n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     # Combine results\n",
    "#     print(\"Combining results from all chunks...\")\n",
    "#     combine_start = time.time()\n",
    "#     if results:\n",
    "#         result_df = pd.concat(results, ignore_index=True)\n",
    "#         print(f\"Combined {len(result_df)} mint records with {len(result_df.columns)} features in {time.time() - combine_start:.2f} seconds\")\n",
    "#     else:\n",
    "#         print(\"Warning: No results to combine!\")\n",
    "#         result_df = pd.DataFrame()\n",
    "    \n",
    "#     total_duration = time.time() - total_start_time\n",
    "#     print(f\"Total processing completed in {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n",
    "#     print(f\"Final result: {len(result_df)} rows, {len(result_df.columns)} columns\")\n",
    "#     return result_df\n",
    "\n",
    "# # Usage example - just set n_jobs and chunk_size when calling the function\n",
    "# print(\"Starting mint processing...\")\n",
    "# mint_transactions = fast_process_mint(\n",
    "#     transactions_df,\n",
    "#     n_jobs=4,  # Adjust based on your CPU cores\n",
    "#     chunk_size=100000  # Adjust based on your data size\n",
    "# )\n",
    "\n",
    "# print(f\"Processing complete! Output shape: {mint_transactions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f14fe67e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:05:59.387947Z",
     "iopub.status.busy": "2025-07-23T12:05:59.387576Z",
     "iopub.status.idle": "2025-07-23T12:05:59.391786Z",
     "shell.execute_reply": "2025-07-23T12:05:59.390758Z"
    },
    "papermill": {
     "duration": 0.025661,
     "end_time": "2025-07-23T12:05:59.393706",
     "exception": false,
     "start_time": "2025-07-23T12:05:59.368045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mint_transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "456eefcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:05:59.435216Z",
     "iopub.status.busy": "2025-07-23T12:05:59.434813Z",
     "iopub.status.idle": "2025-07-23T12:05:59.439415Z",
     "shell.execute_reply": "2025-07-23T12:05:59.438445Z"
    },
    "papermill": {
     "duration": 0.026534,
     "end_time": "2025-07-23T12:05:59.441231",
     "exception": false,
     "start_time": "2025-07-23T12:05:59.414697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# # transactions['block_time'] = pd.to_datetime(transactions['block_time'])\n",
    "\n",
    "# def process_group(group):\n",
    "#     return pd.Series({\n",
    "#         # Basic counts\n",
    "#         'total_txs': group['tx_idx'].count(),\n",
    "#         'unique_tx_indices': group['tx_idx'].nunique(),\n",
    "#         'unique_wallets': group['signing_wallet'].nunique(),\n",
    "        \n",
    "#         # Base/quote amounts\n",
    "#         'total_base': group['base_coin_amount'].sum(),\n",
    "#         'avg_base': group['base_coin_amount'].mean(),\n",
    "#         'max_base': group['base_coin_amount'].max(),\n",
    "#         'min_base': group['base_coin_amount'].min(),\n",
    "#         'std_base': group['base_coin_amount'].std(),\n",
    "        \n",
    "#         'total_quote': group['quote_coin_amount'].sum(),\n",
    "#         'avg_quote': group['quote_coin_amount'].mean(),\n",
    "#         'max_quote': group['quote_coin_amount'].max(),\n",
    "#         'min_quote': group['quote_coin_amount'].min(),\n",
    "#         'std_quote': group['quote_coin_amount'].std(),\n",
    "        \n",
    "#         # Direction analysis\n",
    "#         'buy_ratio': (group['direction'] == 'buy').mean(),\n",
    "        \n",
    "#         # Time features\n",
    "#         'activity_duration_sec': (\n",
    "#             group['block_time'].max() - group['block_time'].min()\n",
    "#         ).total_seconds(),\n",
    "#         'first_slot': group['slot'].min(),\n",
    "#         'last_slot': group['slot'].max(),\n",
    "#         'slot_span': group['slot'].max() - group['slot'].min(),\n",
    "        \n",
    "#         # Balance features\n",
    "#         'initial_balance': group['virtual_token_balance_after'].iloc[0] \n",
    "#             if len(group) > 0 else 0,\n",
    "#         'final_balance': group['virtual_token_balance_after'].iloc[-1] \n",
    "#             if len(group) > 0 else 0,\n",
    "#         'balance_volatility': group['virtual_token_balance_after'].std(),\n",
    "        \n",
    "#         'sol_change': (\n",
    "#             group['virtual_sol_balance_after'].iloc[-1] - \n",
    "#             group['virtual_sol_balance_after'].iloc[0]\n",
    "#         ) if len(group) > 0 else 0,\n",
    "        \n",
    "#         # Gas features\n",
    "#         'total_gas_fees': group['provided_gas_fee'].sum(),\n",
    "#         'avg_gas_fee': group['provided_gas_fee'].mean(),\n",
    "#         'limit_utilization': (\n",
    "#             group['provided_gas_limit'] - group['consumed_gas']\n",
    "#         ).mean(),\n",
    "#         'fee_per_sol': (\n",
    "#             group['fee'].sum() / \n",
    "#             (group['quote_coin_amount'].sum() + 1e-6)\n",
    "#         ),\n",
    "#         'gas_per_tx': group['consumed_gas'].mean(),\n",
    "#         'gas_efficiency': (\n",
    "#             group['quote_coin_amount'].sum() / \n",
    "#             (group['consumed_gas'].sum() + 1e-6)\n",
    "#         )\n",
    "#     })\n",
    "\n",
    "# mint_transactions = transactions.groupby('base_coin').apply(process_group).reset_index()\n",
    "# mint_transactions = mint_transactions.rename(columns={'base_coin': 'mint'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73d491c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:05:59.479980Z",
     "iopub.status.busy": "2025-07-23T12:05:59.479591Z",
     "iopub.status.idle": "2025-07-23T12:05:59.485553Z",
     "shell.execute_reply": "2025-07-23T12:05:59.484409Z"
    },
    "papermill": {
     "duration": 0.027809,
     "end_time": "2025-07-23T12:05:59.487557",
     "exception": false,
     "start_time": "2025-07-23T12:05:59.459748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from joblib import Parallel, delayed\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Make sure datetime conversion is done\n",
    "# transactions['block_time'] = pd.to_datetime(transactions['block_time'])\n",
    "\n",
    "# # Group by base_coin but don't use apply - use agg instead\n",
    "# def fast_process_mint(transactions):\n",
    "#     # Get unique mints\n",
    "#     unique_mints = transactions['base_coin'].unique()\n",
    "#     results = []\n",
    "    \n",
    "#     # For each computation, group once and calculate all metrics together\n",
    "#     grouped = transactions.groupby('base_coin')\n",
    "    \n",
    "#     # Basic counts\n",
    "#     counts = grouped.agg({\n",
    "#         'tx_idx': ['count', 'nunique'],\n",
    "#         'signing_wallet': 'nunique'\n",
    "#     })\n",
    "    \n",
    "#     # Base/quote amounts\n",
    "#     base_stats = grouped['base_coin_amount'].agg(['sum', 'mean', 'max', 'min', 'std'])\n",
    "#     quote_stats = grouped['quote_coin_amount'].agg(['sum', 'mean', 'max', 'min', 'std'])\n",
    "    \n",
    "#     # Direction analysis\n",
    "#     buy_ratio = grouped['direction'].apply(lambda x: (x == 'buy').mean())\n",
    "    \n",
    "#     # Time features\n",
    "#     time_features = grouped.agg({\n",
    "#         'block_time': [lambda x: (x.max() - x.min()).total_seconds()],\n",
    "#         'slot': ['min', 'max']\n",
    "#     })\n",
    "#     slot_span = grouped['slot'].max() - grouped['slot'].min()\n",
    "    \n",
    "#     # Get first and last records for each group\n",
    "#     # This is faster than using .apply on each group\n",
    "#     first_idx = grouped['virtual_token_balance_after'].transform('first')\n",
    "#     last_idx = grouped['virtual_token_balance_after'].transform('last')\n",
    "    \n",
    "#     # Create a mapping of base_coin to first/last values\n",
    "#     balance_df = transactions[['base_coin', 'virtual_token_balance_after', 'virtual_sol_balance_after']].copy()\n",
    "#     balance_df['is_first'] = (balance_df['virtual_token_balance_after'] == \n",
    "#                              transactions.groupby('base_coin')['virtual_token_balance_after'].transform('first'))\n",
    "#     balance_df['is_last'] = (balance_df['virtual_token_balance_after'] == \n",
    "#                             transactions.groupby('base_coin')['virtual_token_balance_after'].transform('last'))\n",
    "    \n",
    "#     # Extract first values\n",
    "#     first_balances = balance_df[balance_df['is_first']].drop_duplicates('base_coin')[['base_coin', 'virtual_token_balance_after', 'virtual_sol_balance_after']]\n",
    "#     first_balances = first_balances.rename(columns={\n",
    "#         'virtual_token_balance_after': 'initial_balance',\n",
    "#         'virtual_sol_balance_after': 'initial_sol'\n",
    "#     })\n",
    "    \n",
    "#     # Extract last values\n",
    "#     last_balances = balance_df[balance_df['is_last']].drop_duplicates('base_coin')[['base_coin', 'virtual_token_balance_after', 'virtual_sol_balance_after']]\n",
    "#     last_balances = last_balances.rename(columns={\n",
    "#         'virtual_token_balance_after': 'final_balance',\n",
    "#         'virtual_sol_balance_after': 'final_sol'\n",
    "#     })\n",
    "    \n",
    "#     # Balance volatility\n",
    "#     balance_vol = grouped['virtual_token_balance_after'].agg('std')\n",
    "    \n",
    "#     # Gas features\n",
    "#     gas_features = grouped.agg({\n",
    "#         'provided_gas_fee': ['sum', 'mean'],\n",
    "#         'provided_gas_limit': 'mean',\n",
    "#         'consumed_gas': ['mean', 'sum'],\n",
    "#         'fee': 'sum'\n",
    "#     })\n",
    "    \n",
    "#     # Calculate limit utilization\n",
    "#     limit_util = grouped.apply(lambda x: (x['provided_gas_limit'] - x['consumed_gas']).mean())\n",
    "    \n",
    "#     # Create the final dataframe by combining all the above\n",
    "#     result_df = pd.DataFrame({'mint': unique_mints})\n",
    "    \n",
    "#     # Add counts\n",
    "#     result_df['total_txs'] = result_df['mint'].map(dict(zip(counts.index, counts[('tx_idx', 'count')])))\n",
    "#     result_df['unique_tx_indices'] = result_df['mint'].map(dict(zip(counts.index, counts[('tx_idx', 'nunique')])))\n",
    "#     result_df['unique_wallets'] = result_df['mint'].map(dict(zip(counts.index, counts[('signing_wallet', 'nunique')])))\n",
    "    \n",
    "#     # Add base/quote amounts\n",
    "#     for op in ['sum', 'mean', 'max', 'min', 'std']:\n",
    "#         result_df[f'total_base' if op == 'sum' else f'{op}_base'] = result_df['mint'].map(dict(zip(base_stats.index, base_stats[op])))\n",
    "#         result_df[f'total_quote' if op == 'sum' else f'{op}_quote'] = result_df['mint'].map(dict(zip(quote_stats.index, quote_stats[op])))\n",
    "    \n",
    "#     # Add direction analysis\n",
    "#     result_df['buy_ratio'] = result_df['mint'].map(dict(zip(buy_ratio.index, buy_ratio)))\n",
    "    \n",
    "#     # Add time features\n",
    "#     result_df['activity_duration_sec'] = result_df['mint'].map(dict(zip(time_features.index, time_features[('block_time', '<lambda>')])))\n",
    "#     result_df['first_slot'] = result_df['mint'].map(dict(zip(time_features.index, time_features[('slot', 'min')])))\n",
    "#     result_df['last_slot'] = result_df['mint'].map(dict(zip(time_features.index, time_features[('slot', 'max')])))\n",
    "#     result_df['slot_span'] = result_df['mint'].map(dict(zip(slot_span.index, slot_span)))\n",
    "    \n",
    "#     # Add balance features by merging\n",
    "#     result_df = result_df.merge(first_balances, left_on='mint', right_on='base_coin', how='left')\n",
    "#     result_df = result_df.merge(last_balances, left_on='mint', right_on='base_coin', how='left')\n",
    "#     result_df = result_df.drop(['base_coin_x', 'base_coin_y'], axis=1, errors='ignore')\n",
    "    \n",
    "#     # Calculate sol_change\n",
    "#     result_df['sol_change'] = result_df['final_sol'] - result_df['initial_sol']\n",
    "#     result_df = result_df.drop(['final_sol', 'initial_sol'], axis=1)\n",
    "    \n",
    "#     # Add balance volatility\n",
    "#     result_df['balance_volatility'] = result_df['mint'].map(dict(zip(balance_vol.index, balance_vol)))\n",
    "    \n",
    "#     # Add gas features\n",
    "#     result_df['total_gas_fees'] = result_df['mint'].map(dict(zip(gas_features.index, gas_features[('provided_gas_fee', 'sum')])))\n",
    "#     result_df['avg_gas_fee'] = result_df['mint'].map(dict(zip(gas_features.index, gas_features[('provided_gas_fee', 'mean')])))\n",
    "#     result_df['limit_utilization'] = result_df['mint'].map(dict(zip(limit_util.index, limit_util)))\n",
    "    \n",
    "#     # Calculate fee_per_sol\n",
    "#     result_df['fee_per_sol'] = result_df['mint'].map(\n",
    "#         dict(zip(gas_features.index, \n",
    "#                 gas_features[('fee', 'sum')] / (quote_stats['sum'] + 1e-6)))\n",
    "#     )\n",
    "    \n",
    "#     result_df['gas_per_tx'] = result_df['mint'].map(dict(zip(gas_features.index, gas_features[('consumed_gas', 'mean')])))\n",
    "    \n",
    "#     # Calculate gas_efficiency\n",
    "#     result_df['gas_efficiency'] = result_df['mint'].map(\n",
    "#         dict(zip(gas_features.index, \n",
    "#                 quote_stats['sum'] / (gas_features[('consumed_gas', 'sum')] + 1e-6)))\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Processed {len(unique_mints)} mints in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "#     return result_df\n",
    "\n",
    "# mint_transactions = fast_process_mint(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2094b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:05:59.625426Z",
     "iopub.status.busy": "2025-07-23T12:05:59.625031Z",
     "iopub.status.idle": "2025-07-23T12:06:03.262755Z",
     "shell.execute_reply": "2025-07-23T12:06:03.261456Z"
    },
    "papermill": {
     "duration": 3.660069,
     "end_time": "2025-07-23T12:06:03.264736",
     "exception": false,
     "start_time": "2025-07-23T12:05:59.604667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mint                    1118389\n",
       "total_txs                   661\n",
       "unique_tx_indices           583\n",
       "unique_wallets              425\n",
       "buy_count                   493\n",
       "                         ...   \n",
       "price_vol_mean           924297\n",
       "price_vol_max            685309\n",
       "price_slippage           924257\n",
       "tw_base_coin_amount     1042774\n",
       "tw_quote_coin_amount    1034613\n",
       "Length: 71, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mint_transactions.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd632999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:06:03.308839Z",
     "iopub.status.busy": "2025-07-23T12:06:03.308500Z",
     "iopub.status.idle": "2025-07-23T12:06:28.201607Z",
     "shell.execute_reply": "2025-07-23T12:06:28.200405Z"
    },
    "papermill": {
     "duration": 24.918043,
     "end_time": "2025-07-23T12:06:28.203837",
     "exception": false,
     "start_time": "2025-07-23T12:06:03.285794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_dune_features(df, outer_df):\n",
    "    outer_df = outer_df.rename(columns={'token_mint_address': 'mint'})\n",
    "    outer_df = outer_df.drop_duplicates(subset='mint', keep='first')\n",
    "    features_to_add = ['decimals', 'name', 'symbol', 'token_uri', 'created_at']\n",
    "    \n",
    "    df_merged = df.merge(\n",
    "        outer_df[['mint'] + features_to_add],\n",
    "        on='mint',\n",
    "        how='left'\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "def merge_onchain_features(df, onchain_df):\n",
    "    onchain_df = onchain_df.drop_duplicates(subset='mint', keep='first')\n",
    "    df_merged = df.merge(\n",
    "        onchain_df,\n",
    "        on='mint',\n",
    "        how='left'\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "def merge_transaction_features(df, transactions):\n",
    "    transactions = transactions.drop_duplicates(subset='mint', keep='first')\n",
    "    # print(f'new transactions shape : {transactions.shape}')\n",
    "    df_merged = df.merge(\n",
    "        transactions,\n",
    "        on='mint',\n",
    "        how='left'\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "train_merged = merge_dune_features(train, dune_token_df_combined)\n",
    "test_merged = merge_dune_features(test, dune_token_df_combined)\n",
    "\n",
    "train_merged_enh = merge_onchain_features(train_merged, onchain_df_2)\n",
    "test_merged_enh = merge_onchain_features(test_merged, onchain_df_2)\n",
    "\n",
    "train_merged_enh = merge_transaction_features(train_merged_enh, mint_transactions)\n",
    "test_merged_enh = merge_transaction_features(test_merged_enh, mint_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7083b962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:06:28.243069Z",
     "iopub.status.busy": "2025-07-23T12:06:28.242679Z",
     "iopub.status.idle": "2025-07-23T12:06:28.249375Z",
     "shell.execute_reply": "2025-07-23T12:06:28.248256Z"
    },
    "papermill": {
     "duration": 0.028073,
     "end_time": "2025-07-23T12:06:28.251246",
     "exception": false,
     "start_time": "2025-07-23T12:06:28.223173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_merged_enh['slot_min'] = train_merged_enh['slot_min'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a2e5bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:06:28.294303Z",
     "iopub.status.busy": "2025-07-23T12:06:28.293821Z",
     "iopub.status.idle": "2025-07-23T12:11:00.496072Z",
     "shell.execute_reply": "2025-07-23T12:11:00.494491Z"
    },
    "papermill": {
     "duration": 272.227619,
     "end_time": "2025-07-23T12:11:00.498171",
     "exception": false,
     "start_time": "2025-07-23T12:06:28.270552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "<ipython-input-35-f104f34cd583>:157: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df['has_meme_keyword'] = df['name'].str.contains(r'\\b(doge|shiba|floki|elon|moon)\\b', case=False, na=False).astype(int)\n",
      "<ipython-input-35-f104f34cd583>:238: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(regex, case=False, na=False).astype(int)\n",
      "<ipython-input-35-f104f34cd583>:238: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(regex, case=False, na=False).astype(int)\n",
      "<ipython-input-35-f104f34cd583>:238: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(regex, case=False, na=False).astype(int)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return op(a, b)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n",
      "  return op(a, b)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sin\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in cos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "<ipython-input-35-f104f34cd583>:157: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df['has_meme_keyword'] = df['name'].str.contains(r'\\b(doge|shiba|floki|elon|moon)\\b', case=False, na=False).astype(int)\n",
      "<ipython-input-35-f104f34cd583>:238: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(regex, case=False, na=False).astype(int)\n",
      "<ipython-input-35-f104f34cd583>:238: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(regex, case=False, na=False).astype(int)\n",
      "<ipython-input-35-f104f34cd583>:238: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(regex, case=False, na=False).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed both train and test set in: 270.64 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class BlockchainPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.hash_dims = {\n",
    "            'name_x': 500,\n",
    "            'symbol_x': 200,\n",
    "            'token_uri': 800,\n",
    "            'creator': 1000,\n",
    "            'curve_address': 1500\n",
    "        }\n",
    "        self.numeric_clips = {}\n",
    "        self.temporal_stats = {}\n",
    "        self.frequency_maps = {}\n",
    "        self.target_encoding = {}\n",
    "        self.momentum_features = {}\n",
    "        self.token_patterns = {\n",
    "            'meme': r'\\b(doge|shiba|floki|elon|moon|pepe|wojak|cat|dog|inu|based|frog)\\b',\n",
    "            'tech': r'\\b(ai|token|chain|eth|sol|chain|defi|nft|swap|dao|yield|meta|web3)\\b',\n",
    "            'pump': r'\\b(moon|pump|diamond|rocket|lambo|rich|millionaire|gold|gem)\\b'\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate temporal statistics\n",
    "        temp_df = self._preprocess_time(X.copy())\n",
    "        \n",
    "        # Create hash features before any groupby operations\n",
    "        temp_df = self._create_hash_features(temp_df)\n",
    "        \n",
    "        # Calculate temporal stats\n",
    "        self.temporal_stats = {\n",
    "            'gas_mean': temp_df['gas_used'].mean(),\n",
    "            'gas_std': temp_df['gas_used'].std(),\n",
    "            'gas_90th': temp_df['gas_used'].quantile(0.9),\n",
    "            'dev_balance_mean': temp_df['dev_balance'].mean(),\n",
    "            'dev_balance_std': temp_df['dev_balance'].std()\n",
    "        }\n",
    "        \n",
    "        # Calculate target encoding and momentum features\n",
    "        self.target_encoding['creator_grad_rate'] = temp_df.groupby('creator_hash')['has_graduated'].mean()\n",
    "        graduated = temp_df[temp_df['has_graduated'] == 1]\n",
    "        self.momentum_features = {\n",
    "            'median_tx_velocity': graduated['tx_per_sec'].median(),\n",
    "            'median_unique_wallets': graduated['unique_wallets'].median(),\n",
    "            'grad_flow_imbalance': graduated['flow_imbalance'].mean()\n",
    "        }\n",
    "        \n",
    "        # Calculate frequency maps\n",
    "        self.frequency_maps['name_x_hash'] = temp_df['name_x_hash'].value_counts(normalize=True)\n",
    "        self.frequency_maps['symbol_x_hash'] = temp_df['symbol_x_hash'].value_counts(normalize=True)\n",
    "        \n",
    "        # Network stats\n",
    "        self.network_stats = {\n",
    "            'flow_mean': X['flow_imbalance'].mean(),\n",
    "            'flow_std': X['flow_imbalance'].std(),\n",
    "            'flow_quantiles': {q: X['flow_imbalance'].quantile(q) for q in [0.25, 0.5, 0.75, 0.9]}\n",
    "        }\n",
    "        \n",
    "        # Volatility stats\n",
    "        self.volatility_stats = {\n",
    "            'vol_mean': X['price_vol_mean'].mean(),\n",
    "            'vol_std': X['price_vol_mean'].std(),\n",
    "            'volatility_quantiles': {q: X['price_vol_mean'].quantile(q) for q in [0.25, 0.5, 0.75, 0.9]}\n",
    "        }\n",
    "        \n",
    "        # Calculate IQR clipping ranges\n",
    "        for col in ['slot_min', 'gas_used', 'dev_balance']:\n",
    "            q1 = X[col].quantile(0.25)\n",
    "            q3 = X[col].quantile(0.75)\n",
    "            self.numeric_clips[col] = (q1 - 1.5*(q3-q1), q3 + 1.5*(q3-q1))\n",
    "        \n",
    "        # Creator reputation stats\n",
    "        creator_stats = temp_df.groupby('creator_hash').agg({\n",
    "            'has_graduated': ['mean', 'count']\n",
    "        })\n",
    "        creator_stats.columns = ['_'.join(col).strip() for col in creator_stats.columns.values]\n",
    "        self.creator_stats = creator_stats\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        \n",
    "        df = self._preprocess_time(df)\n",
    "        df = self._handle_names(df)\n",
    "        df = self._parse_bundle(df)\n",
    "        df = self._create_hash_features(df)\n",
    "        df = self._process_numerics(df)\n",
    "        df = self._create_frequency_features(df)\n",
    "        \n",
    "        df = self._create_temporal_features(df)\n",
    "        df = self._create_interaction_features(df)\n",
    "        df = self._create_anomaly_features(df)\n",
    "        df = self._create_text_features(df)\n",
    "        df = self._create_target_encoding(df)\n",
    "        df = self._process_network_features(df)\n",
    "        df = self._process_volatility_features(df)\n",
    "        df = self._create_liquidity_indicators(df)\n",
    "        df = self._create_momentum_features(df)\n",
    "        df = self._create_token_type_features(df)\n",
    "        df = self._create_creator_reputation_features(df)\n",
    "        df = self._create_advanced_liquidity_features(df)\n",
    "        df = self._create_whale_activity_features(df)\n",
    "        df = self._create_datetime_features_pandas(df)\n",
    "        \n",
    "        return self._final_cleanup(df)\n",
    "\n",
    "    def _preprocess_time(self, df):\n",
    "        time_cols = ['created_at', 'block_time']\n",
    "        for col in time_cols:\n",
    "            df[col] = pd.to_datetime(df[col], utc=True).dt.tz_convert(None)\n",
    "\n",
    "        df['coin_age'] = (df['block_time'] - df['created_at']).dt.total_seconds()\n",
    "        df['peak_launch'] = df['created_at'].dt.hour.between(9, 17).astype(int)\n",
    "        \n",
    "        for prefix in ['created', 'block']:\n",
    "            dt_col = f'{prefix}_at' if prefix == 'created' else f'{prefix}_time'\n",
    "            for unit in ['day', 'hour', 'month']:\n",
    "                col = f'{prefix}_{unit}'\n",
    "                df[col] = getattr(df[dt_col].dt, unit)\n",
    "                df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / self._get_period(unit))\n",
    "                df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / self._get_period(unit))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _get_period(self, unit):\n",
    "        return {'day': 31, 'hour': 24, 'month': 12}.get(unit, 1)\n",
    "\n",
    "    def _create_temporal_features(self, df):\n",
    "        df['tx_velocity'] = df['tx_idx'] / (df['coin_age'] + 1e-6)\n",
    "        df['block_saturation'] = df.groupby('slot')['tx_idx'].transform(lambda x: x / x.max())\n",
    "        return df\n",
    "\n",
    "    def _create_interaction_features(self, df):\n",
    "        action_cols = ['nft_actions', 'swap_actions', 'other_actions']\n",
    "        for col in action_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        \n",
    "        df['total_actions'] = df[action_cols].sum(axis=1)\n",
    "        df['swap_action_ratio'] = df['swap_actions'] / (df['total_actions'].replace(0, 1))\n",
    "        df['gas_per_instruction'] = df['gas_used'] / (df['amount_of_instructions'].replace(0, 1))\n",
    "        return df\n",
    "\n",
    "    def _create_anomaly_features(self, df):\n",
    "        df['dev_balance_z'] = (df['dev_balance'] - self.temporal_stats['dev_balance_mean']) / self.temporal_stats['dev_balance_std']\n",
    "        df['gas_z'] = (df['gas_used'] - self.temporal_stats['gas_mean']) / self.temporal_stats['gas_std']\n",
    "        df['gas_outlier'] = (df['gas_used'] > self.temporal_stats['gas_90th']).astype(int)\n",
    "        return df\n",
    "\n",
    "    def _create_text_features(self, df):\n",
    "        df['name'] = df['name_x'].fillna(df.get('name_y', ''))\n",
    "        df['name_length'] = df['name'].str.len().fillna(0)\n",
    "        df['has_meme_keyword'] = df['name'].str.contains(r'\\b(doge|shiba|floki|elon|moon)\\b', case=False, na=False).astype(int)\n",
    "        return df\n",
    "\n",
    "    def _create_target_encoding(self, df):\n",
    "        df['creator_grad_rate'] = df['creator_hash'].map(self.target_encoding['creator_grad_rate']).fillna(df['has_graduated'].mean())\n",
    "        return df\n",
    "\n",
    "    def _handle_names(self, df):\n",
    "        df['name'] = df['name_x'].fillna(df.get('name_y', ''))\n",
    "        df['symbol'] = df['symbol_x'].fillna(df.get('symbol_y', ''))\n",
    "        return df\n",
    "\n",
    "    def _parse_bundle(self, df):\n",
    "        def parse_bundle(s):\n",
    "            try:\n",
    "                return {\n",
    "                    'nft_actions': s.count('NFT_MINT'),\n",
    "                    'swap_actions': s.count('TOKEN_SWAP'),\n",
    "                    'other_actions': s.count('UNRECOGNIZED_ACTION')\n",
    "                }\n",
    "            except:\n",
    "                return {'nft_actions': 0, 'swap_actions': 0, 'other_actions': 0}\n",
    "        \n",
    "        bundle_data = df['bundle_structure'].fillna('').apply(parse_bundle)\n",
    "        action_counts = pd.json_normalize(bundle_data)\n",
    "        df = pd.concat([df, action_counts], axis=1)\n",
    "        \n",
    "        for col in ['nft_actions', 'swap_actions', 'other_actions']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        return df\n",
    "\n",
    "    def _create_hash_features(self, df):\n",
    "        for col, dim in self.hash_dims.items():\n",
    "            df[f'{col}_hash'] = (df[col].fillna('missing').astype(str).apply(lambda x: (hash(x) % dim) + 1))\n",
    "        return df\n",
    "\n",
    "    def _process_numerics(self, df):\n",
    "        for col in ['gas_used', 'dev_balance']:\n",
    "            clip_min, clip_max = self.numeric_clips[col]\n",
    "            df[col] = (df[col].fillna(df[col].median()).clip(lower=clip_min, upper=clip_max).pipe(np.log1p))\n",
    "        \n",
    "        df['complexity_ratio'] = df['amount_of_instructions'] / df['bundle_size'].replace(0, 1e-6)\n",
    "        return df\n",
    "\n",
    "    def _create_frequency_features(self, df):\n",
    "        for col in ['pf_program_index', 'creation_ix_index']:\n",
    "            freq = df[col].value_counts(normalize=True).add(1e-6).to_dict()\n",
    "            df[f'{col}_freq'] = df[col].map(freq).fillna(1e-6)\n",
    "        return df\n",
    "\n",
    "    def _process_network_features(self, df):\n",
    "        df['flow_zscore'] = (df['flow_imbalance'] - self.network_stats['flow_mean']) / self.network_stats['flow_std']\n",
    "        return df\n",
    "\n",
    "    def _process_volatility_features(self, df):\n",
    "        df['volatility_ratio'] = df['price_vol_mean'] / (df['avg_price'] + 1e-6)\n",
    "        df['vol_zscore'] = (df['price_vol_mean'] - self.volatility_stats['vol_mean']) / self.volatility_stats['vol_std']\n",
    "        return df\n",
    "\n",
    "    def _create_liquidity_indicators(self, df):\n",
    "        df['depth_ratio'] = df['quote_sum'] / (df['liq_virtual_sol_balance_after_mean'] + 1e-6)\n",
    "        df['slippage_ratio'] = df['liq_quote_coin_amount'] / (df['quote_sum'] + 1e-6)\n",
    "        return df\n",
    "\n",
    "    def _process_transaction_features(self, df):\n",
    "        df['whale_impact'] = df['top5_wallet_share'] * df['total_base']\n",
    "        df['volatility_intensity'] = df['tx_per_sec'] * df['price_vol_mean']\n",
    "        df['concentration_risk'] = df['gini_coeff'] * np.log1p(df['unique_wallets'])\n",
    "        return df\n",
    "\n",
    "    def _create_momentum_features(self, df):\n",
    "        df['velocity_vs_grads'] = df['tx_per_sec'] / (self.momentum_features['median_tx_velocity'] + 1e-6)\n",
    "        df['wallet_adoption_vs_grads'] = df['unique_wallets'] / (self.momentum_features['median_unique_wallets'] + 1e-6)\n",
    "        df['wallet_growth_rate'] = df['unique_wallets'] / (df['activity_duration_sec']/3600 + 1e-6)\n",
    "        df['flow_vs_grads'] = df['flow_imbalance'] / (self.momentum_features['grad_flow_imbalance'] + 1e-6)\n",
    "        df['early_adoption_ratio'] = df['early_txs'] / (df['total_txs'] + 1e-6)\n",
    "        return df\n",
    "\n",
    "    def _create_token_type_features(self, df):\n",
    "        for pattern_name, regex in self.token_patterns.items():\n",
    "            df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(regex, case=False, na=False).astype(int)\n",
    "        df['name_uniqueness'] = df['name_length'] * (1 - df['name_x_hash'].map(self.frequency_maps.get('name_x_hash', pd.Series())).fillna(0.5))\n",
    "        return df\n",
    "\n",
    "    def _create_creator_reputation_features(self, df):\n",
    "        for feature in ['has_graduated_mean', 'has_graduated_count']:\n",
    "            df[feature] = df['creator_hash'].map(self.creator_stats[feature])\n",
    "        prior_mean = df['has_graduated'].mean()\n",
    "        prior_count = 2\n",
    "        df['creator_reputation'] = (\n",
    "            (df['has_graduated_mean'] * df['has_graduated_count'] + prior_mean * prior_count) / \n",
    "            (df['has_graduated_count'] + prior_count)\n",
    "        )\n",
    "        df['creator_experience'] = np.log1p(df['has_graduated_count'])\n",
    "        return df\n",
    "\n",
    "    def _create_advanced_liquidity_features(self, df):\n",
    "        df['liquidity_strength'] = df['quote_sum'] / (df['price_slippage'] + 1e-6)\n",
    "        df['liquidity_stability'] = 1 / (df['price_vol_mean'] * df['top5_wallet_share'] + 1e-6)\n",
    "        df['buy_pressure'] = df['buy_ratio'] * df['flow_imbalance']\n",
    "        df['liquidity_per_wallet'] = df['quote_sum'] / (df['unique_wallets'] + 1e-6)\n",
    "        return df\n",
    "\n",
    "    def _create_whale_activity_features(self, df):\n",
    "        df['whale_dominance'] = df['top5_wallet_share'] * np.sqrt(df['unique_wallets'])\n",
    "        df['whale_manipulation'] = df['gini_coeff'] * df['price_vol_mean']\n",
    "        df['whale_balance_ratio'] = df['max_balance'] / (df['base_sum'] + 1e-6)\n",
    "        return df\n",
    "\n",
    "    def _create_datetime_features_pandas(self, df):\n",
    "\n",
    "        df = df.copy()\n",
    "        df['first_time'] = pd.to_datetime(df['first_time'])\n",
    "        df['last_time'] = pd.to_datetime(df['last_time'])\n",
    "    \n",
    "    # 1. Duration Features\n",
    "        df['duration_sec'] = (df['last_time'] - df['first_time']).dt.total_seconds()\n",
    "    \n",
    "    # # Binned durations - use right=False to make intervals left-inclusive\n",
    "    #     bins = [0, 1, 5, 30, 60, 300, np.inf]\n",
    "    #     labels = ['<1s', '1-5s', '5-30s', '30-60s', '1-5m', '5m+']\n",
    "    #     df['duration_bin'] = pd.cut(df['duration_sec'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # 2. Temporal Intensity\n",
    "        df['tx_intensity'] = df['total_txs'] / (df['duration_sec'] + 1e-6)  # Transactions per second\n",
    "    \n",
    "    # 3. Time-of-Day Features (Cyclical Encoding)\n",
    "        df['hour_sin_f'] = np.sin(2 * np.pi * df['first_time'].dt.hour / 24)\n",
    "        df['hour_cos_f'] = np.cos(2 * np.pi * df['first_time'].dt.hour / 24)\n",
    "        df['minute_sin_f'] = np.sin(2 * np.pi * df['first_time'].dt.minute / 60)\n",
    "        df['hour_sin_f'] = np.sin(2 * np.pi * df['last_time'].dt.hour / 24)\n",
    "        df['hour_cos_f'] = np.cos(2 * np.pi * df['last_time'].dt.hour / 24)\n",
    "        df['minute_sin_f'] = np.sin(2 * np.pi * df['last_time'].dt.minute / 60)\n",
    "    \n",
    "    # 6. Burst Detection\n",
    "        df['burstiness'] = 1 - (df['duration_sec'] / \n",
    "                          (df['last_time'].max() - df['first_time'].min()).total_seconds())\n",
    "\n",
    "        # df['time_since_last'] = df.groupby('mint')['first_time'].diff().dt.total_seconds()\n",
    "    \n",
    "    # 8. Is Opening Activity (first transaction of the day)\n",
    "        # df['is_opening_activity'] = (\n",
    "        #     df['first_time'].dt.normalize() == \n",
    "        #     df.groupby('mint')['first_time'].transform('min').dt.normalize()\n",
    "        # ).astype(int)\n",
    "    \n",
    "        return df\n",
    "\n",
    "    def _final_cleanup(self, df):\n",
    "        cols_to_drop = [\n",
    "            'mint', 'slot_graduated', 'has_graduated',\n",
    "            'name_x', 'symbol_x', 'token_uri', 'creator',\n",
    "            'curve_address', 'bundle_structure', 'url', 'version',\n",
    "            'first_time', 'last_time'\n",
    "        ]\n",
    "        existing_cols = [col for col in cols_to_drop if col in df.columns]\n",
    "        return df.drop(columns=existing_cols, errors='ignore')\n",
    "\n",
    "# Example usage\n",
    "test_merged_enh['slot_graduated'] = np.nan\n",
    "test_merged_enh['has_graduated'] = np.nan\n",
    "\n",
    "start_time = time.time()\n",
    "preprocessor = BlockchainPreprocessor()\n",
    "train_clean = preprocessor.fit_transform(train_merged_enh)\n",
    "test_clean = preprocessor.transform(test_merged_enh)\n",
    "print(f'processed both train and test set in: {time.time() - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77ec60fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:00.540420Z",
     "iopub.status.busy": "2025-07-23T12:11:00.539796Z",
     "iopub.status.idle": "2025-07-23T12:11:00.544368Z",
     "shell.execute_reply": "2025-07-23T12:11:00.543162Z"
    },
    "papermill": {
     "duration": 0.028433,
     "end_time": "2025-07-23T12:11:00.546438",
     "exception": false,
     "start_time": "2025-07-23T12:11:00.518005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f6a5526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:00.588373Z",
     "iopub.status.busy": "2025-07-23T12:11:00.587829Z",
     "iopub.status.idle": "2025-07-23T12:11:00.609481Z",
     "shell.execute_reply": "2025-07-23T12:11:00.608346Z"
    },
    "papermill": {
     "duration": 0.045187,
     "end_time": "2025-07-23T12:11:00.611266",
     "exception": false,
     "start_time": "2025-07-23T12:11:00.566079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.base import TransformerMixin\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# class BlockchainPreprocessor(TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         self.hash_dims = {\n",
    "#             'name_x': 500,\n",
    "#             'symbol_x': 200,\n",
    "#             'token_uri': 800,\n",
    "#             'creator': 1000,\n",
    "#             'curve_address': 1500\n",
    "#         }\n",
    "#         self.numeric_clips = {}\n",
    "#         self.temporal_stats = {}\n",
    "#         self.frequency_maps = {}\n",
    "#         self.target_encoding = {}\n",
    "#         self.momentum_features = {}\n",
    "#         self.token_patterns = {\n",
    "#             'meme': r'\\b(doge|shiba|floki|elon|moon|pepe|wojak|cat|dog|inu|based|frog)\\b',\n",
    "#             'tech': r'\\b(ai|token|chain|eth|sol|chain|defi|nft|swap|dao|yield|meta|web3)\\b',\n",
    "#             'pump': r'\\b(moon|pump|diamond|rocket|lambo|rich|millionaire|gold|gem)\\b'\n",
    "#         }\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         # Calculate temporal statistics\n",
    "#         temp_df = self._preprocess_time(X.copy())\n",
    "        \n",
    "#         # Create hash features before any groupby operations\n",
    "#         temp_df = self._create_hash_features(temp_df)\n",
    "        \n",
    "#         # Calculate temporal stats\n",
    "#         self.temporal_stats = {\n",
    "#             'gas_mean': temp_df['gas_used'].mean(),\n",
    "#             'gas_std': temp_df['gas_used'].std(),\n",
    "#             'gas_90th': temp_df['gas_used'].quantile(0.9),\n",
    "#             'dev_balance_mean': temp_df['dev_balance'].mean(),\n",
    "#             'dev_balance_std': temp_df['dev_balance'].std()\n",
    "#         }\n",
    "        \n",
    "#         # Calculate target encoding after creating hashes\n",
    "#         if 'has_graduated' in temp_df.columns:\n",
    "#             self.target_encoding['creator_grad_rate'] = temp_df.groupby('creator_hash')['has_graduated'].mean()\n",
    "            \n",
    "#             # NEW: Extract momentum features if target exists\n",
    "#             graduated = temp_df[temp_df['has_graduated'] == 1]\n",
    "#             self.momentum_features = {\n",
    "#                 'median_tx_velocity': graduated['tx_per_sec'].median(),\n",
    "#                 'median_unique_wallets': graduated['unique_wallets'].median(),\n",
    "#                 'grad_flow_imbalance': graduated['flow_imbalance'].mean() if 'flow_imbalance' in graduated.columns else 0\n",
    "#             }\n",
    "        \n",
    "#         # Calculate frequency maps after creating hashes\n",
    "#         self.frequency_maps['name_x_hash'] = temp_df['name_x_hash'].value_counts(normalize=True)\n",
    "#         self.frequency_maps['symbol_x_hash'] = temp_df['symbol_x_hash'].value_counts(normalize=True)\n",
    "        \n",
    "#         if 'flow_imbalance' in X.columns:\n",
    "#             self.network_stats = {\n",
    "#                 'flow_mean': X['flow_imbalance'].mean(),\n",
    "#                 'flow_std': X['flow_imbalance'].std(),\n",
    "#                 # NEW: Temporal flow patterns\n",
    "#                 'flow_quantiles': {q: X['flow_imbalance'].quantile(q) for q in [0.25, 0.5, 0.75, 0.9]}\n",
    "#             }\n",
    "            \n",
    "#         # Volatility normalization\n",
    "#         if 'price_vol_mean' in X.columns:\n",
    "#             self.volatility_stats = {\n",
    "#                 'vol_mean': X['price_vol_mean'].mean(),\n",
    "#                 'vol_std': X['price_vol_mean'].std(),\n",
    "#                 # NEW: Market sentiment indicators\n",
    "#                 'volatility_quantiles': {q: X['price_vol_mean'].quantile(q) for q in [0.25, 0.5, 0.75, 0.9]}\n",
    "#             }\n",
    "            \n",
    "#         # Calculate IQR clipping ranges\n",
    "#         for col in ['slot_min', 'gas_used', 'dev_balance']:\n",
    "#             if col in X.columns:\n",
    "#                 q1 = X[col].quantile(0.25)\n",
    "#                 q3 = X[col].quantile(0.75)\n",
    "#                 self.numeric_clips[col] = (q1 - 1.5*(q3-q1), q3 + 1.5*(q3-q1))\n",
    "        \n",
    "#         # NEW: Creator reputation stats\n",
    "#         if 'creator' in X.columns:\n",
    "#             creator_stats = temp_df.groupby('creator_hash').agg({\n",
    "#                 'has_graduated': ['mean', 'count']\n",
    "#             })\n",
    "#             creator_stats.columns = ['_'.join(col).strip() for col in creator_stats.columns.values]\n",
    "#             self.creator_stats = creator_stats\n",
    "        \n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         df = X.copy()\n",
    "        \n",
    "#         df = self._preprocess_time(df)\n",
    "#         df = self._handle_names(df)\n",
    "#         df = self._parse_bundle(df)\n",
    "#         df = self._create_hash_features(df)\n",
    "#         df = self._process_numerics(df)\n",
    "#         df = self._create_frequency_features(df)\n",
    "        \n",
    "#         df = self._create_temporal_features(df)\n",
    "#         df = self._create_interaction_features(df)\n",
    "#         df = self._create_anomaly_features(df)\n",
    "#         df = self._create_text_features(df)\n",
    "#         df = self._create_target_encoding(df)\n",
    "#         df = self._process_network_features(df)\n",
    "#         df = self._process_volatility_features(df)\n",
    "#         df = self._create_liquidity_indicators(df)\n",
    "        \n",
    "#         # NEW FEATURES\n",
    "#         df = self._create_momentum_features(df)\n",
    "#         df = self._create_token_type_features(df)\n",
    "#         df = self._create_creator_reputation_features(df)\n",
    "#         df = self._create_advanced_liquidity_features(df)\n",
    "#         df = self._create_whale_activity_features(df)\n",
    "        \n",
    "#         return self._final_cleanup(df)\n",
    "\n",
    "#     def _preprocess_time(self, df):\n",
    "#         time_cols = ['created_at', 'block_time']\n",
    "#         for col in time_cols:\n",
    "#             if col in df.columns:\n",
    "#                 df[col] = pd.to_datetime(df[col], utc=True).dt.tz_convert(None)\n",
    "\n",
    "#         df['coin_age'] = (df['block_time'] - df['created_at']).dt.total_seconds()\n",
    "#         df['peak_launch'] = df['created_at'].dt.hour.between(9, 17).astype(int)\n",
    "        \n",
    "#         for prefix in ['created', 'block']:\n",
    "#             dt_col = f'{prefix}_at' if prefix == 'created' else f'{prefix}_time'\n",
    "#             if dt_col in df.columns:\n",
    "#                 for unit in ['day', 'hour', 'month']:\n",
    "#                     col = f'{prefix}_{unit}'\n",
    "#                     df[col] = getattr(df[dt_col].dt, unit)\n",
    "#                     df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / self._get_period(unit))\n",
    "#                     df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / self._get_period(unit))\n",
    "        \n",
    "#         return df\n",
    "\n",
    "#     def _get_period(self, unit):\n",
    "#         return {\n",
    "#             'day': 31,\n",
    "#             'hour': 24,\n",
    "#             'month': 12\n",
    "#         }.get(unit, 1)\n",
    "\n",
    "#     def _create_temporal_features(self, df):\n",
    "#         # Transaction velocity\n",
    "#         df['tx_velocity'] = df['tx_idx'] / (df['coin_age'] + 1e-6)\n",
    "        \n",
    "#         # Block saturation\n",
    "#         df['block_saturation'] = df.groupby('slot')['tx_idx'].transform(\n",
    "#             lambda x: x / x.max()\n",
    "#         )\n",
    "#         return df\n",
    "\n",
    "#     def _create_interaction_features(self, df):\n",
    "#         \"\"\"Create interaction features with safety checks\"\"\"\n",
    "#         # Verify action columns exist\n",
    "#         action_cols = ['nft_actions', 'swap_actions', 'other_actions']\n",
    "#         for col in action_cols:\n",
    "#             if col not in df.columns:\n",
    "#                 df[col] = 0\n",
    "    \n",
    "#         # Create interaction features\n",
    "#         df['total_actions'] = df[action_cols].sum(axis=1)\n",
    "#         df['swap_action_ratio'] = df['swap_actions'] / (df['total_actions'].replace(0, 1))  # Prevent div/0\n",
    "    \n",
    "#         # Gas efficiency metrics\n",
    "#         if 'amount_of_instructions' in df.columns:\n",
    "#             df['gas_per_instruction'] = df['gas_used'] / (df['amount_of_instructions'].replace(0, 1))\n",
    "    \n",
    "#         return df\n",
    "\n",
    "#     def _create_anomaly_features(self, df):\n",
    "#         # Z-scores\n",
    "#         df['dev_balance_z'] = (df['dev_balance'] - self.temporal_stats['dev_balance_mean']) / self.temporal_stats['dev_balance_std']\n",
    "#         df['gas_z'] = (df['gas_used'] - self.temporal_stats['gas_mean']) / self.temporal_stats['gas_std']\n",
    "        \n",
    "#         # Outlier flags\n",
    "#         df['gas_outlier'] = (df['gas_used'] > self.temporal_stats['gas_90th']).astype(int)\n",
    "#         return df\n",
    "\n",
    "#     def _create_text_features(self, df):\n",
    "#         # Name analysis\n",
    "#         df['name'] = df['name_x'].fillna(df.get('name_y', ''))\n",
    "#         df['name_length'] = df['name'].str.len().fillna(0)\n",
    "#         df['has_meme_keyword'] = df['name'].str.contains(\n",
    "#             r'\\b(doge|shiba|floki|elon|moon)\\b', \n",
    "#             case=False, \n",
    "#             na=False\n",
    "#         ).astype(int)\n",
    "#         return df\n",
    "\n",
    "#     def _create_target_encoding(self, df):\n",
    "#         if self.target_encoding.get('creator_grad_rate') is not None:\n",
    "#             df['creator_grad_rate'] = df['creator_hash'].map(\n",
    "#                 self.target_encoding['creator_grad_rate']\n",
    "#             ).fillna(df['has_graduated'].mean())\n",
    "#         return df\n",
    "\n",
    "#     # Existing helper methods (updated with safety checks)\n",
    "#     def _handle_names(self, df):\n",
    "#         df['name'] = df['name_x'].fillna(df.get('name_y', ''))\n",
    "#         df['symbol'] = df['symbol_x'].fillna(df.get('symbol_y', ''))\n",
    "#         return df\n",
    "\n",
    "#     def _parse_bundle(self, df):\n",
    "#         if 'bundle_structure' in df.columns:\n",
    "#         # Parse bundle structure and create raw action columns\n",
    "#             def parse_bundle(s):\n",
    "#                 try:\n",
    "#                     return {\n",
    "#                         'nft_actions': s.count('NFT_MINT'),\n",
    "#                         'swap_actions': s.count('TOKEN_SWAP'),\n",
    "#                         'other_actions': s.count('UNRECOGNIZED_ACTION')\n",
    "#                         }\n",
    "#                 except:\n",
    "#                     return {'nft_actions': 0, 'swap_actions': 0, 'other_actions': 0}\n",
    "        \n",
    "#             bundle_data = df['bundle_structure'].fillna('').apply(parse_bundle)\n",
    "#             action_counts = pd.json_normalize(bundle_data)\n",
    "#             df = pd.concat([df, action_counts], axis=1)\n",
    "        \n",
    "#         # Ensure action columns exist even if parsing fails\n",
    "#         for col in ['nft_actions', 'swap_actions', 'other_actions']:\n",
    "#             if col not in df.columns:\n",
    "#                 df[col] = 0\n",
    "            \n",
    "#         return df\n",
    "\n",
    "#     def _create_hash_features(self, df):\n",
    "#     # Create hash-based features with safety checks\n",
    "#         for col, dim in self.hash_dims.items():\n",
    "#             if col in df.columns:\n",
    "#             # Handle missing values and ensure string type\n",
    "#                 df[f'{col}_hash'] = (\n",
    "#                     df[col]\n",
    "#                     .fillna('missing')\n",
    "#                     .astype(str)\n",
    "#                     .apply(lambda x: (hash(x) % dim) + 1)  # +1 to avoid 0\n",
    "#                 )\n",
    "#             else:\n",
    "#             # Create default hash if column missing\n",
    "#                 df[f'{col}_hash'] = 1\n",
    "#         return df\n",
    "\n",
    "#     def _process_numerics(self, df):\n",
    "#         # Process numerical features with robust error handling\n",
    "#         for col in ['gas_used', 'dev_balance']:\n",
    "#             if col in df.columns and col in self.numeric_clips:\n",
    "#                 # Apply safe clipping with fallbacks\n",
    "#                 clip_min, clip_max = self.numeric_clips[col]\n",
    "#                 df[col] = (\n",
    "#                     df[col]\n",
    "#                     .fillna(df[col].median())\n",
    "#                     .clip(lower=clip_min, upper=clip_max)\n",
    "#                     .pipe(np.log1p)\n",
    "#                 )\n",
    "        \n",
    "#         # Add complexity ratio with division guard\n",
    "#         if 'amount_of_instructions' in df.columns and 'bundle_size' in df.columns:\n",
    "#             df['complexity_ratio'] = (\n",
    "#                 df['amount_of_instructions'] / \n",
    "#                 df['bundle_size'].replace(0, 1e-6)\n",
    "#             )\n",
    "#         return df\n",
    "\n",
    "#     def _create_frequency_features(self, df):\n",
    "#         # Create frequency-encoded features with smoothing\n",
    "#         for col in ['pf_program_index', 'creation_ix_index']:\n",
    "#             if col in df.columns:\n",
    "#                 # Calculate frequencies with additive smoothing\n",
    "#                 freq = (\n",
    "#                     df[col].value_counts(normalize=True)\n",
    "#                     .add(1e-6)  # Prevent zero probabilities\n",
    "#                     .to_dict()\n",
    "#                 )\n",
    "#                 df[f'{col}_freq'] = df[col].map(freq).fillna(1e-6)\n",
    "#         return df\n",
    "\n",
    "#     def _process_network_features(self, df):\n",
    "#         if 'flow_imbalance' in df.columns:\n",
    "#             df['flow_zscore'] = (df['flow_imbalance'] - self.network_stats['flow_mean']) / \\\n",
    "#                                self.network_stats['flow_std']\n",
    "#         return df\n",
    "\n",
    "#     def _process_volatility_features(self, df):\n",
    "#         if 'price_vol_mean' in df.columns:\n",
    "#             df['volatility_ratio'] = df['price_vol_mean'] / (df['avg_price'] + 1e-6)\n",
    "#             df['vol_zscore'] = (df['price_vol_mean'] - self.volatility_stats['vol_mean']) / \\\n",
    "#                              self.volatility_stats['vol_std']\n",
    "#         return df\n",
    "\n",
    "#     def _create_liquidity_indicators(self, df):\n",
    "#         if 'liq_virtual_sol_balance_after_mean' in df.columns:\n",
    "#             df['depth_ratio'] = df['total_quote'] / (df['liq_virtual_sol_balance_after_mean'] + 1e-6)\n",
    "#             df['slippage_ratio'] = df['liq_quote_coin_amount'] / (df['total_quote'] + 1e-6)\n",
    "#         return df\n",
    "\n",
    "#     def _process_transaction_features(self, df):\n",
    "#         # Enhanced version with new ratios\n",
    "#         if 'total_txs' in df.columns and 'unique_wallets' in df.columns:\n",
    "#             df['whale_impact'] = df['top5_wallet_share'] * df['total_base']\n",
    "            \n",
    "#         if 'tx_per_sec' in df.columns and 'price_vol_mean' in df.columns:\n",
    "#             df['volatility_intensity'] = df['tx_per_sec'] * df['price_vol_mean']\n",
    "            \n",
    "#         # Add interaction terms\n",
    "#         if 'gini_coeff' in df.columns and 'unique_wallets' in df.columns:\n",
    "#             df['concentration_risk'] = df['gini_coeff'] * np.log1p(df['unique_wallets'])\n",
    "            \n",
    "#         return df\n",
    "\n",
    "#     def _create_momentum_features(self, df):\n",
    "#         \"\"\"Create momentum and velocity-based features that indicate token adoption rate\"\"\"\n",
    "#         if not self.momentum_features:\n",
    "#             return df\n",
    "            \n",
    "#         # Velocity ratio compared to graduated tokens\n",
    "#         if 'tx_per_sec' in df.columns:\n",
    "#             df['velocity_vs_grads'] = df['tx_per_sec'] / (self.momentum_features['median_tx_velocity'] + 1e-6)\n",
    "            \n",
    "#         # Wallet adoption compared to graduated tokens\n",
    "#         if 'unique_wallets' in df.columns:\n",
    "#             df['wallet_adoption_vs_grads'] = df['unique_wallets'] / (self.momentum_features['median_unique_wallets'] + 1e-6)\n",
    "            \n",
    "#         # Calculate adoption acceleration (wallet growth over time)\n",
    "#         if 'unique_wallets' in df.columns and 'activity_duration_sec' in df.columns:\n",
    "#             df['wallet_growth_rate'] = df['unique_wallets'] / (df['activity_duration_sec']/3600 + 1e-6)  # per hour\n",
    "            \n",
    "#         # Flow momentum vs graduated tokens\n",
    "#         if 'flow_imbalance' in df.columns:\n",
    "#             df['flow_vs_grads'] = df['flow_imbalance'] / (self.momentum_features['grad_flow_imbalance'] + 1e-6)\n",
    "            \n",
    "#         # Early adoption momentum\n",
    "#         if 'early_txs' in df.columns and 'total_txs' in df.columns:\n",
    "#             df['early_adoption_ratio'] = df['early_txs'] / (df['total_txs'] + 1e-6)\n",
    "            \n",
    "#         return df\n",
    "        \n",
    "#     def _create_token_type_features(self, df):\n",
    "#         \"\"\"Classify tokens based on naming patterns that correlate with graduation\"\"\"\n",
    "#         if 'name' not in df.columns:\n",
    "#             return df\n",
    "            \n",
    "#         # Token type detection based on name patterns\n",
    "#         for pattern_name, regex in self.token_patterns.items():\n",
    "#             df[f'is_{pattern_name}_token'] = df['name'].str.lower().str.contains(\n",
    "#                 regex, case=False, na=False\n",
    "#             ).astype(int)\n",
    "            \n",
    "#         # Calculate name uniqueness score\n",
    "#         if 'name_length' in df.columns:\n",
    "#             # Unique tokens tend to have longer, more distinct names\n",
    "#             df['name_uniqueness'] = df['name_length'] * (1 - df['name_x_hash'].map(\n",
    "#                 self.frequency_maps.get('name_x_hash', pd.Series())\n",
    "#             ).fillna(0.5))\n",
    "            \n",
    "#         return df\n",
    "        \n",
    "#     def _create_creator_reputation_features(self, df):\n",
    "#         \"\"\"Add features capturing creator's historical success with tokens\"\"\"\n",
    "#         if not hasattr(self, 'creator_stats') or 'creator_hash' not in df.columns:\n",
    "#             return df\n",
    "            \n",
    "#         # Join creator historical performance\n",
    "#         creator_features = ['has_graduated_mean', 'has_graduated_count']\n",
    "        \n",
    "#         for feature in creator_features:\n",
    "#             if feature in self.creator_stats.columns:\n",
    "#                 df[feature] = df['creator_hash'].map(self.creator_stats[feature])\n",
    "                \n",
    "#         # Calculate reputation scores\n",
    "#         if 'has_graduated_mean' in df.columns and 'has_graduated_count' in df.columns:\n",
    "#             # Bayesian average with pseudocounts for reliability\n",
    "#             prior_mean = df['has_graduated'].mean() if 'has_graduated' in df.columns else 0.5\n",
    "#             prior_count = 2  # Prior strength\n",
    "            \n",
    "#             df['creator_reputation'] = (\n",
    "#                 (df['has_graduated_mean'] * df['has_graduated_count'] + prior_mean * prior_count) / \n",
    "#                 (df['has_graduated_count'] + prior_count)\n",
    "#             )\n",
    "            \n",
    "#             # Experience factor - creators with more tokens have more experience\n",
    "#             df['creator_experience'] = np.log1p(df['has_graduated_count'])\n",
    "            \n",
    "#         return df\n",
    "        \n",
    "#     def _create_advanced_liquidity_features(self, df):\n",
    "#         \"\"\"Create features that better characterize liquidity patterns\"\"\"\n",
    "#         liquidity_cols = [col for col in df.columns if 'liq_' in col or 'slippage' in col]\n",
    "#         if not liquidity_cols:\n",
    "#             return df\n",
    "            \n",
    "#         # Liquidity depth measures\n",
    "#         # if 'quote_sum' in df.columns and 'price_slippage' in df.columns:\n",
    "#             df['liquidity_strength'] = df['quote_sum'] / (df['price_slippage'] + 1e-6)\n",
    "            \n",
    "#         # Liquidity stability - how consistent the pools are\n",
    "#         # if 'price_vol_mean' in df.columns and 'top5_wallet_share' in df.columns:\n",
    "#             df['liquidity_stability'] = 1 / (df['price_vol_mean'] * df['top5_wallet_share'] + 1e-6)\n",
    "            \n",
    "#         # Liquidity imbalance - buy vs sell pressure\n",
    "#         # if 'buy_ratio' in df.columns and 'flow_imbalance' in df.columns:\n",
    "#             df['buy_pressure'] = df['buy_ratio'] * df['flow_imbalance']\n",
    "            \n",
    "#         # Liquidity saturation - how much the market is able to absorb\n",
    "#         # if 'total_quote' in df.columns and 'unique_wallets' in df.columns:\n",
    "#             df['liquidity_per_wallet'] = df['total_quote'] / (df['unique_wallets'] + 1e-6)\n",
    "            \n",
    "#         return df\n",
    "        \n",
    "#     def _create_whale_activity_features(self, df):\n",
    "#         # \"\"\"Create features to capture large holder (whale) behavior\"\"\"\n",
    "#         # if 'top5_wallet_share' not in df.columns:\n",
    "#             # return df\n",
    "            \n",
    "#         # Concentration risk factor \n",
    "#         # if 'unique_wallets' in df.columns:\n",
    "#             df['whale_dominance'] = df['top5_wallet_share'] * np.sqrt(df['unique_wallets'])\n",
    "            \n",
    "#         # Whale activity detection\n",
    "#         # if 'gini_coeff' in df.columns and 'price_vol_mean' in df.columns:\n",
    "#             df['whale_manipulation'] = df['gini_coeff'] * df['price_vol_mean']\n",
    "            \n",
    "#         # Whale balance indicators\n",
    "#         # if 'max_balance' in df.columns and 'total_base' in df.columns:\n",
    "#             df['whale_balance_ratio'] = df['max_balance'] / (df['base_sum'] + 1e-6)\n",
    "            \n",
    "#             return df\n",
    "\n",
    "#     def _final_cleanup(self, df):\n",
    "#         # Safe column removal with existence checks\n",
    "#         cols_to_drop = [\n",
    "#             'mint', 'slot_graduated', 'has_graduated',\n",
    "#             'name_x', 'symbol_x', 'token_uri', 'creator',\n",
    "#             'curve_address', 'bundle_structure', 'url', 'version'\n",
    "#         ]\n",
    "#         existing_cols = [col for col in cols_to_drop if col in df.columns]\n",
    "#         return df.drop(columns=existing_cols, errors='ignore')\n",
    "\n",
    "# test_merged_enh['slot_graduated'] = np.nan\n",
    "# test_merged_enh['has_graduated'] = np.nan\n",
    "\n",
    "# start_time = time.time()\n",
    "# preprocessor = BlockchainPreprocessor()\n",
    "# train_clean = preprocessor.fit_transform(train_merged_enh.iloc[:1000])\n",
    "# test_clean = preprocessor.transform(test_merged_enh.iloc[:1000])\n",
    "# print(f'processed both train and test set in: {time.time() - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1873d8cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:00.651819Z",
     "iopub.status.busy": "2025-07-23T12:11:00.651416Z",
     "iopub.status.idle": "2025-07-23T12:11:00.655582Z",
     "shell.execute_reply": "2025-07-23T12:11:00.654470Z"
    },
    "papermill": {
     "duration": 0.026711,
     "end_time": "2025-07-23T12:11:00.657394",
     "exception": false,
     "start_time": "2025-07-23T12:11:00.630683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39c125e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:00.699137Z",
     "iopub.status.busy": "2025-07-23T12:11:00.698716Z",
     "iopub.status.idle": "2025-07-23T12:11:00.704246Z",
     "shell.execute_reply": "2025-07-23T12:11:00.703201Z"
    },
    "papermill": {
     "duration": 0.028504,
     "end_time": "2025-07-23T12:11:00.706148",
     "exception": false,
     "start_time": "2025-07-23T12:11:00.677644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# def preprocess_time(df):\n",
    "#     df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "#     df['block_time'] = pd.to_datetime(df['block_time'])\n",
    "#     df['created_year'] = df['created_at'].dt.year\n",
    "#     df['created_month'] = df['created_at'].dt.month\n",
    "#     df['created_day'] = df['created_at'].dt.day\n",
    "#     df['created_hour'] = df['created_at'].dt.hour\n",
    "#     df['created_day_sin'] = np.sin((2 * np.pi * df['created_day'] ) / 31)\n",
    "#     df['created_day_cos'] = np.cos((2 * np.pi * df['created_day']) / 31)\n",
    "#     df['created_hour_sin'] = np.sin((2 * np.pi * df['created_hour'])  / 24)\n",
    "#     df['created_hour_cos'] = np.cos((2 * np.pi * df['created_hour']) / 24)\n",
    "#     df['created_month_sin'] =  np.sin((2 * np.pi * df['created_month']) / 12)\n",
    "#     df['created_month_cos'] = np.cos((2 * np.pi * df['created_month']) / 12)\n",
    "#     df['created_at_unix'] = df['created_at'].astype(int) // 10**9  # Unix timestamp\n",
    "\n",
    "#     df['block_year'] = df['block_time'].dt.year\n",
    "#     df['block_month'] = df['block_time'].dt.month\n",
    "#     df['block_day'] = df['block_time'].dt.day\n",
    "#     df['block_hour'] = df['block_time'].dt.hour\n",
    "#     df['block_day_sin'] = np.sin((2 * np.pi * df['block_day'] ) / 31)\n",
    "#     df['block_day_cos'] = np.cos((2 * np.pi * df['block_day']) / 31)\n",
    "#     df['block_hour_sin'] = np.sin((2 * np.pi * df['block_hour'])  / 24)\n",
    "#     df['block_hour_cos'] = np.cos((2 * np.pi * df['block_hour']) / 24)\n",
    "#     df['block_month_sin'] =  np.sin((2 * np.pi * df['block_month']) / 12)\n",
    "#     df['block_month_cos'] = np.cos((2 * np.pi * df['block_month']) / 12)\n",
    "#     df['block_time_unix'] = df['block_time'].astype(int) // 10**9  # Unix timestamp\n",
    "#     return df.drop(columns=['created_at', 'block_time'])\n",
    "\n",
    "# # def create_hash_encoding(df, hash_dict):\n",
    "# #     for col, n_components in hash_dict.items():\n",
    "# #         df[col] = df[col].astype('str')\n",
    "# #         df[f'{col}_hash'] = df[col].apply(lambda x: hash(x) % n_components)\n",
    "# #     return df.drop(columns=['name', 'symbol', 'token_uri'])\n",
    "\n",
    "# # hash_dict = {\n",
    "# #     'name':500,\n",
    "# #     'symbol':300,\n",
    "# #     'token_uri':1000,\n",
    "# # }\n",
    "# # preprocessor = ColumnTransformer(\n",
    "# #     transformers = [\n",
    "# #         ('datetime', FunctionTransformer(preprocess_time), ['created_at']),\n",
    "# #         ('categorical', FunctionTransformer(create_hash_encoding, kw_args = {'hash_dict':hash_dict}), ['name', 'symbol', 'token_uri'])\n",
    "        \n",
    "# #     ],\n",
    "# #     remainder='passthrough'\n",
    "# # )\n",
    "# # preprocessor.set_output(transform='pandas')\n",
    "\n",
    "# from sklearn.base import TransformerMixin\n",
    "\n",
    "# class BlockchainPreprocessor(TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         self.hash_dims = {\n",
    "#             'name_x': 500,\n",
    "#             'symbol_x': 200,\n",
    "#             'token_uri': 800,\n",
    "#             'creator': 1000,\n",
    "#             'curve_address': 1500\n",
    "#         }\n",
    "#         self.numeric_clips = {}\n",
    "#     def fit(self, X, y=None):\n",
    "#         for col in ['slot_min', 'gas_used', 'dev_balance']:\n",
    "#             q1 = X[col].quantile(0.25)\n",
    "#             q3 = X[col].quantile(0.75)\n",
    "\n",
    "#             self.numeric_clips[col] = (q1 - 1.5 * (q3 - q1), q3 + 1.5 * (q3 - q1))\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         df = X.copy()\n",
    "#         df['coin_age'] = df['block_time_unix'] - df['created_at_unix']\n",
    "#         df['peak_launch'] = df['created_hour'].between(9, 17).astype(int)\n",
    "#         df = preprocess_time(df)\n",
    "#         df['name'] = df['name_x'].fillna(df.get('name_y', ''))\n",
    "#         df['symbol'] = df['symbol_x'].fillna(df.get('symbol_y', ''))\n",
    "#         def parse_bundle_structure(s):\n",
    "#             try:\n",
    "#                 return {\n",
    "#             'nft_actions': s.count('NFT_MINT'),\n",
    "#             'swap_actions': s.count('TOKEN_SWAP'),\n",
    "#             'other_actions': s.count('UNRECOGNIZED_ACTION')\n",
    "#                     }\n",
    "#             except:\n",
    "#                 return {'nft_actions':0, 'swap_actions':0, 'other_actions':0}\n",
    "\n",
    "#         bundle_data = df['bundle_structure'].apply(parse_bundle_structure)\n",
    "#         df = pd.concat([df, bundle_data.apply(pd.Series)], axis=1)\n",
    "        \n",
    "#         # df['version'] = df['version'].fillna(0).astype(int)\n",
    "        \n",
    "#         for col, dim in self.hash_dims.items():\n",
    "#             df[f'{col}_hash'] = df[col].astype(str).apply(lambda x: hash(x) % dim)\n",
    "#         df['complexity_ratio'] = df['amount_of_instructions'] / (df['bundle_size'] + 1e-6)\n",
    "\n",
    "#         for col in ['gas_used', 'dev_balance']:\n",
    "#             df[col] = np.log1p(df[col].clip(lower=1))\n",
    "#             df[col]  = df[col].clip(*self.numeric_clips[col])\n",
    "\n",
    "#         for col in ['pf_program_index', 'creation_ix_index']:\n",
    "#             freq = df[col].value_counts(normalize=True)\n",
    "#             df[f'{col}_freq'] = df[col].map(freq)\n",
    "#         to_drop = ['mint', 'slot_graduated', 'has_graduated',\n",
    "#                   'name_x', 'symbol_x', 'token_uri', 'creator', \n",
    "#                   'curve_address', 'bundle_structure', 'url', 'version']\n",
    "#         df = df.drop(columns=to_drop)\n",
    "#         return df\n",
    "    \n",
    "\n",
    "\n",
    "# preprocessor = BlockchainPreprocessor()\n",
    "# train_clean = preprocessor.fit_transform(train_merged_enh)\n",
    "# test_clean = preprocessor.transform(test_merged_enh)\n",
    "# # train_merged_2 = preprocessor.fit_transform(train_merged)\n",
    "# # test_merged_2 = preprocessor.transform(test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8519eded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:00.747001Z",
     "iopub.status.busy": "2025-07-23T12:11:00.746591Z",
     "iopub.status.idle": "2025-07-23T12:11:00.750631Z",
     "shell.execute_reply": "2025-07-23T12:11:00.749425Z"
    },
    "papermill": {
     "duration": 0.026307,
     "end_time": "2025-07-23T12:11:00.752432",
     "exception": false,
     "start_time": "2025-07-23T12:11:00.726125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_clean.to_csv('train_clean.csv', index=False)\n",
    "# test_clean.to_csv('test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f7e8325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:00.793278Z",
     "iopub.status.busy": "2025-07-23T12:11:00.792862Z",
     "iopub.status.idle": "2025-07-23T12:11:01.446629Z",
     "shell.execute_reply": "2025-07-23T12:11:01.445438Z"
    },
    "papermill": {
     "duration": 0.676612,
     "end_time": "2025-07-23T12:11:01.448838",
     "exception": false,
     "start_time": "2025-07-23T12:11:00.772226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['created_at', 'block_time']\n",
    "train_clean.drop(columns=cols_to_drop, inplace=True)\n",
    "test_clean.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "344e9ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:01.490948Z",
     "iopub.status.busy": "2025-07-23T12:11:01.490553Z",
     "iopub.status.idle": "2025-07-23T12:11:01.711541Z",
     "shell.execute_reply": "2025-07-23T12:11:01.710173Z"
    },
    "papermill": {
     "duration": 0.244675,
     "end_time": "2025-07-23T12:11:01.713680",
     "exception": false,
     "start_time": "2025-07-23T12:11:01.469005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.base import clone\n",
    "seed=1\n",
    "def cv_score(X, y, model, folds=10, seed=seed):\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "    scores = []\n",
    "    for i , (train_idx, valid_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        clone_clf = clone(model)\n",
    "        clone_clf.fit(X_train, y_train)\n",
    "        preds = clone_clf.predict_proba(X_valid)[:,1]\n",
    "        score = log_loss(y_valid, preds)\n",
    "        scores.append(score)\n",
    "        print(f'score at fold {i} :', score)\n",
    "\n",
    "    print(f'Mean score across folds {i} :', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78043abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:01.754841Z",
     "iopub.status.busy": "2025-07-23T12:11:01.754470Z",
     "iopub.status.idle": "2025-07-23T12:11:01.758809Z",
     "shell.execute_reply": "2025-07-23T12:11:01.757556Z"
    },
    "papermill": {
     "duration": 0.027121,
     "end_time": "2025-07-23T12:11:01.760733",
     "exception": false,
     "start_time": "2025-07-23T12:11:01.733612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# train_merged_scaled = scaler.fit_transform(train_merged_2[input_cols])\n",
    "# test_merged_scaled = scaler.transform(test_merged_2[input_cols])\n",
    "\n",
    "# train_merged_scaled_df = pd.DataFrame(train_merged_scaled, columns=input_cols)\n",
    "# test_merged_scaled_df = pd.DataFrame(test_merged_scaled, columns=input_cols)\n",
    "\n",
    "# train_merged_scaled_df = train_merged_scaled_df.fillna(0)\n",
    "# test_merged_scaled_df = test_merged_scaled_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbce7e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:01.801415Z",
     "iopub.status.busy": "2025-07-23T12:11:01.801068Z",
     "iopub.status.idle": "2025-07-23T12:11:01.805440Z",
     "shell.execute_reply": "2025-07-23T12:11:01.804281Z"
    },
    "papermill": {
     "duration": 0.026863,
     "end_time": "2025-07-23T12:11:01.807112",
     "exception": false,
     "start_time": "2025-07-23T12:11:01.780249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# problematic_chars = ['\"', '\\\\', '[', ']', '{', '}', ':', ',']\n",
    "# problematic_features = []\n",
    "\n",
    "# for col in test_clean.columns:\n",
    "#     if any(char in col for char in problematic_chars):\n",
    "#         problematic_features.append(col)\n",
    "\n",
    "# problematic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7158644a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:01.848729Z",
     "iopub.status.busy": "2025-07-23T12:11:01.847786Z",
     "iopub.status.idle": "2025-07-23T12:11:01.852460Z",
     "shell.execute_reply": "2025-07-23T12:11:01.851278Z"
    },
    "papermill": {
     "duration": 0.027984,
     "end_time": "2025-07-23T12:11:01.854699",
     "exception": false,
     "start_time": "2025-07-23T12:11:01.826715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_clean = train_clean.rename(columns={'Unnamed: 0': 'unnamed'})\n",
    "# test_clean = test_clean.rename(columns={'Unnamed: 0': 'unnamed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e544af13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:11:01.896976Z",
     "iopub.status.busy": "2025-07-23T12:11:01.896538Z",
     "iopub.status.idle": "2025-07-23T12:14:15.470834Z",
     "shell.execute_reply": "2025-07-23T12:14:15.469700Z"
    },
    "papermill": {
     "duration": 193.598488,
     "end_time": "2025-07-23T12:14:15.473456",
     "exception": false,
     "start_time": "2025-07-23T12:11:01.874968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_clean.to_csv('train_clean.csv', index=False)\n",
    "test_clean.to_csv('test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7c180a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:14:15.516980Z",
     "iopub.status.busy": "2025-07-23T12:14:15.516560Z",
     "iopub.status.idle": "2025-07-23T12:14:15.521041Z",
     "shell.execute_reply": "2025-07-23T12:14:15.519976Z"
    },
    "papermill": {
     "duration": 0.027681,
     "end_time": "2025-07-23T12:14:15.522785",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.495104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# # X = train_clean.drop(columns=['created_at', 'block_time'])\n",
    "# X = train_clean.copy()\n",
    "# # X = X.fillna(0)\n",
    "# y = train['has_graduated'].copy()\n",
    "# # value_counts = y.value_counts()\n",
    "# # scale_weights = value_counts[0] / value_counts[1]\n",
    "# # class_weights = np.where(y==1, 80, 20)\n",
    "# cat_dtypes = X.select_dtypes(include=['object', 'category']).columns\n",
    "# X[cat_dtypes] = X[cat_dtypes].fillna('missing').astype('category')\n",
    "# # X[cat_dtypes] = X[cat_dtypes].astype('category')\n",
    "# # numerical_dtypes = [cols for col in X.columns if col!='k']\n",
    "# num_cols = X.select_dtypes(include=np.number).columns\n",
    "# X[num_cols] = X[num_cols].fillna(0)\n",
    "# ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "# X[cat_dtypes] = ordinal_encoder.fit_transform(X[cat_dtypes])\n",
    "# lgb_clf = LogisticRegression(random_state=1)\n",
    "# # lgb_clf = xgb.XGBClassifier(objective='binary:logistic',\n",
    "# #     eval_metric='logloss', random_state=1)\n",
    "# # lgb_clf = lgb.LGBMClassifier(random_state=1)\n",
    "# # lgb_clf = ExtraTreesClassifier(random_state=1, n_jobs=-1)\n",
    "# # xgb_clf = xgb.XGBClassifier(random_state=1, n_estimators=6000, learning_rate=0.05, enable_categorical=True)\n",
    "# lgb_clf.fit(X, y)\n",
    "# # cv_score(X, y, lgb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4372aa92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:14:15.563597Z",
     "iopub.status.busy": "2025-07-23T12:14:15.563210Z",
     "iopub.status.idle": "2025-07-23T12:14:15.567189Z",
     "shell.execute_reply": "2025-07-23T12:14:15.566158Z"
    },
    "papermill": {
     "duration": 0.02618,
     "end_time": "2025-07-23T12:14:15.568655",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.542475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_clean[cat_dtypes] = train_clean[cat_dtypes].fillna('missing').astype('category').copy()\n",
    "# test_clean[cat_dtypes] = test_clean[cat_dtypes].fillna('missing').astype('category').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68f6dd79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:14:15.609602Z",
     "iopub.status.busy": "2025-07-23T12:14:15.609211Z",
     "iopub.status.idle": "2025-07-23T12:14:15.613695Z",
     "shell.execute_reply": "2025-07-23T12:14:15.612488Z"
    },
    "papermill": {
     "duration": 0.027454,
     "end_time": "2025-07-23T12:14:15.615688",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.588234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdf27081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:14:15.656000Z",
     "iopub.status.busy": "2025-07-23T12:14:15.655585Z",
     "iopub.status.idle": "2025-07-23T12:14:15.659860Z",
     "shell.execute_reply": "2025-07-23T12:14:15.658748Z"
    },
    "papermill": {
     "duration": 0.026489,
     "end_time": "2025-07-23T12:14:15.661680",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.635191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # test_new = test['slot_min'].copy()\n",
    "# test_clean[cat_dtypes] = test_clean[cat_dtypes].fillna('missing').astype('category')\n",
    "# test_clean[cat_dtypes] = ordinal_encoder.transform(test_clean[cat_dtypes])\n",
    "# test_clean[num_cols] = test_clean[num_cols].fillna(0)\n",
    "# # test_clean[cat_dtypes] = ordinal_encoder.transform(test_clean[cat_dtypes])\n",
    "# # test_clean = test_clean.fillna(0)\n",
    "# preds = lgb_clf.predict_proba(test_clean)[:,1]\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1128387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:14:15.703083Z",
     "iopub.status.busy": "2025-07-23T12:14:15.702669Z",
     "iopub.status.idle": "2025-07-23T12:14:15.706852Z",
     "shell.execute_reply": "2025-07-23T12:14:15.705791Z"
    },
    "papermill": {
     "duration": 0.027198,
     "end_time": "2025-07-23T12:14:15.708613",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.681415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dbd48ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:14:15.751806Z",
     "iopub.status.busy": "2025-07-23T12:14:15.751426Z",
     "iopub.status.idle": "2025-07-23T12:14:15.755471Z",
     "shell.execute_reply": "2025-07-23T12:14:15.754365Z"
    },
    "papermill": {
     "duration": 0.028782,
     "end_time": "2025-07-23T12:14:15.757316",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.728534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# samp_sub['has_graduated'] = preds\n",
    "# samp_sub.to_csv('submission_lgbm_clf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5ecdfda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:14:15.799964Z",
     "iopub.status.busy": "2025-07-23T12:14:15.799561Z",
     "iopub.status.idle": "2025-07-23T12:14:15.804085Z",
     "shell.execute_reply": "2025-07-23T12:14:15.802856Z"
    },
    "papermill": {
     "duration": 0.028308,
     "end_time": "2025-07-23T12:14:15.806113",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.777805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.histplot(samp_sub['has_graduated'], bins=500, kde=True, color='r')\n",
    "# # sns.histplot(y, bins=100, kde=True, color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7cbf4",
   "metadata": {
    "papermill": {
     "duration": 0.01927,
     "end_time": "2025-07-23T12:14:15.846256",
     "exception": false,
     "start_time": "2025-07-23T12:14:15.826986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11856763,
     "sourceId": 97569,
     "sourceType": "competition"
    },
    {
     "datasetId": 7012766,
     "sourceId": 11407081,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 908.894309,
   "end_time": "2025-07-23T12:14:19.793937",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-23T11:59:10.899628",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
